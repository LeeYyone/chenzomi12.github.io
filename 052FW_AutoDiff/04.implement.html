<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>2.4. 微分实现方式 &#8212; 人工智能系统（AISys） 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2.5. 动手实现自动微分" href="05.forward_mode.html" />
    <link rel="prev" title="2.3. 微分计算模式" href="03.grad_mode.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="README.html"><span class="section-number">2. </span>自动微分(DONE)</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">2.4. </span>微分实现方式</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/052FW_AutoDiff/04.implement.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/chenzomi12/DeepLearningSystem">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="人工智能系统（AISys）"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../010system/README.html">=== 一、AI系统概述 ===</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../010system/01present.html">AI现状与大模型(待更)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../010system/02drive.html">AI发展驱动力(待更)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../010system/03architecture.html">AI系统全栈架构(待更)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../010system/04sample.html">AI系统样例(待更)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../010system/05principle.html">AI系统原则(待更)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../010system/06foundation.html">大模型的到来(待更)</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../020Hardware/README.html">=== 二、AI芯片体系结构 ===</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../021HW_Foundation/README.html">1. AI 计算体系概述</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/01.introduction.html">1.1. 课程内容</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/02.arch_slim.html">1.2. AI计算模式(上)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/03.mobile_parallel.html">1.3. AI计算模式(下)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/04.metrics.html">1.4. 关键设计指标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/05.matrix.html">1.5. 核心计算之矩阵乘</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/06.bit_width.html">1.6. 计算之比特位宽</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/07.summary.html">1.7. AI计算体系总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../022HW_ChipBase/README.html">2. AI 芯片基础</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/01.cpu_base.html">2.1. CPU 基础</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/02.cpu_isa.html">2.2. CPU 指令集架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/03.cpu_data.html">2.3. CPU 计算本质</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/04.cpu_latency.html">2.4. CPU 计算时延</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/05.gpu.html">2.5. GPU 基础</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/06.npu.html">2.6. NPU 基础</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/07.future.html">2.7. 超异构计算</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../023HW_GPUBase/README.html">3. GPU 原理详解</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../023HW_GPUBase/01.works.html">3.1. GPU工作原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023HW_GPUBase/02.principle.html">3.2. 为什么 GPU 适用于 AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023HW_GPUBase/03.base_concept.html">3.3. GPU架构与CUDA关系</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023HW_GPUBase/04.fermi.html">3.4. GPU架构回顾第一篇</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023HW_GPUBase/05.turing.html">3.5. GPU架构回顾第二篇</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../024HW_NVIDIA/README.html">4. NVIDIA GPU详解</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/01.basic_tc.html">4.1. TensorCore原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/02.history_tc.html">4.2. TensorCore架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/03.deep_tc.html">4.3. TensorCore剖析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/04.basic_nvlink.html">4.4. 分布式通信与NVLink</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/05.deep_nvlink.html">4.5. NVLink原理剖析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/06.deep_nvswitch.html">4.6. NVSwitch原理剖析</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../025HW_Abroad/README.html">5. 国外 AI 芯片架构</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/01.DOJO_Arch.html">5.1. 特斯拉DOJO架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/02.DOJO_Detail.html">5.2. 特斯拉DOJO Core原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/03.DOJO_System.html">5.3. 特斯拉DOJO存算系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/04.TPU_Introl.html">5.4. 谷歌TPU历史发展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/05.TPU1.html">5.5. 谷歌TPUv1脉动阵列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/06.TPU2.html">5.6. 谷歌TPUv2训练芯片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/07.TPU3.html">5.7. 谷歌TPUv3 POD形态</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/08.TPU4.html">5.8. 谷歌TPUv4三维互联</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/09.Future.html">5.9. 国外 AI 芯片思考</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../026HW_Domestic/README.html">6. 国内 AI 芯片架构</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/01.BR100_System.html">6.1. 壁仞产品解读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/02.BR100_Detail.html">6.2. 壁仞BR100架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/03.SUIYUAN_DTU.html">6.3. 燧原产品与DTU架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/04.cambricon_Product.html">6.4. 寒武纪产品解读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/05.cambricon_Arch.html">6.5. 寒武纪MLU芯片架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/06.cambricon_Arch.html">6.6. 寒武纪MLU架构细节</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../030Compiler/README.html">==== 三、AI编译原理(更新中)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../031CM_Tradition/README.html">1. 传统编译器(DOING)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/01.introduction.html">1.1. 编译器基础介绍 OK</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/02.history.html">1.2. 传统编译器发展 OK</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/03.gcc.html">1.3. GCC编译过程和原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/04.llvm.html">1.4. LLVM架构设计和原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/05.llvm_detail01.html">1.5. LLVM IR详解</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/06.llvm_detail02.html">1.6. LLVM前端和优化层</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/07.llvm_detail03.html">1.7. LLVM后端代码生成</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../032CM_AICompiler/README.html">2. AI 编译器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/01.appear.html">2.1. 为什么需要AI编译器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/02.stage.html">2.2. AI编译器的发展阶段</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/03.architecture.html">2.3. AI编译器的通用架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/04.future.html">2.4. AI编译器挑战与思考</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../033CM_Frontend/README.html">3. 前端优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/01.introduction.html">3.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/02.graph_ir.html">3.2. 图算 IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/03.op_fusion.html">3.3. 算子融合</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../034CM_Backend/README.html">4. 后端优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/01.introduction.html">4.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/02.ops_compute.html">4.2. 算子的计算与调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/03.optimization.html">4.3. 算子手工优化方式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/04.loop_opt.html">4.4. 算子循环优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/05.other_opt.html">4.5. 指令和内存优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/06.auto_tuning.html">4.6. Auto-Tuning原理</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../036CM_PyTorch/README.html">5. PyTorch2.0 图模式</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/01.introduction.html">5.1. PyTorch2.0 特性</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/02.torchscript.html">5.2. TorchScript 静态图尝试</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/03.torchfx_lazy.html">5.3. FX 与 LazyTensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/04.torchdynamo.html">5.4. TorchDynamo 获取图</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/05.aotatuograd.html">5.5. AOTAutograd 原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/06.dispatch.html">5.6. Dispatch 机制</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../040Inference/README.html">=== 四、推理系统&amp;引擎 ===</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../041INF_Inference/README.html">1. 推理系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/01.introduction.html">1.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/02.constraints.html">1.2. 推理系统介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/03.workflow.html">1.3. 推理流程全景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/04.system.html">1.4. 推理系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/05.inference.html">1.5. 推理引擎架构（上）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/06.architecture.html">1.6. 推理引擎架构（下）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../042INF_Mobilenet/README.html">2. 模型轻量化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/01.introduction.html">2.1. 推理参数了解</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/02.cnn.html">2.2. CNN模型小型化（上）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/03.cnn.html">2.3. CNN模型小型化（下）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/04.transformer.html">2.4. Transformer小型化</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../043INF_Slim/README.html">3. 模型压缩</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/01.introduction.html">3.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/02.quant.html">3.2. 低比特量化原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/03.qat.html">3.3. 感知量化训练QAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/04.ptq.html">3.4. 训练后量化PTQ与部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/05.pruning.html">3.5. 模型剪枝原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/06.distillation.html">3.6. 知识蒸馏原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/07.distillation.html">3.7. 知识蒸馏算法</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../044INF_Converter/README.html">4. 模型转换&amp;优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/01.introduction.html">4.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/02.converter_princ.html">4.2. 架构与文件格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/03.converter_ir.html">4.3. 自定义计算图IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/04.converter_detail.html">4.4. 模型转换流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/05.optimizer.html">4.5. 计算图优化策略</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/06.basic.html">4.6. 常量折叠&amp;冗余节点消除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/07.extend.html">4.7. 算子融合/替换/前移</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../045INF_Kernel/README.html">5. Kernel优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/01.introduction.html">5.1. Kernel优化架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/02.conv.html">5.2. 卷积操作原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/03.im2col.html">5.3. Im2Col算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/04.winograd.html">5.4. Winograd算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/05.qnnpack.html">5.5. QNNPack算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/06.memory.html">5.6. 推理内存布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/07.nc4hw4.html">5.7. nc4hw4内存排布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/08.others.html">5.8. 汇编与循环优化</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../050Framework/README.html">=== 五、AI框架核心模块 ===</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../051FW_Foundation/README.html">1. AI框架基础(DONE)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../051FW_Foundation/01.introduction.html">1.1. 本章内容</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051FW_Foundation/02.fundamentals.html">1.2. AI框架作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051FW_Foundation/03.history.html">1.3. AI框架之争</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051FW_Foundation/04.programing.html">1.4. 框架编程范式</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="README.html">2. 自动微分(DONE)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01.introduction.html">2.1. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="02.base_concept.html">2.2. 什么是微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.grad_mode.html">2.3. 微分计算模式</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">2.4. 微分实现方式</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.forward_mode.html">2.5. 动手实现自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="06.reversed_mode.html">2.6. 动手实现PyTorch微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="07.challenge.html">2.7. 自动微分的挑战&amp;未来</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../053FW_DataFlow/README.html">3. 计算图(DONE)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/01.introduction.html">3.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/02.computegraph.html">3.2. 计算图原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/03.atuodiff.html">3.3. 计算图与自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/04.dispatch.html">3.4. 计算图的调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/05.control_flow.html">3.5. 计算图的控制流实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/06.static_graph.html">3.6. 动态图与静态图转换</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/07.future.html">3.7. 计算图的挑战&amp;未来</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../060Foundation/README.html">==== 六、大模型训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../061FW_AICluster/README.html">1. 分布式集群</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../061FW_AICluster/01.introduction.html">1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../061FW_AICluster/02.architecture.html">1.2. AI集群服务器架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../061FW_AICluster/03.communication.html">1.3. AI集群软硬件通信</a></li>
<li class="toctree-l2"><a class="reference internal" href="../061FW_AICluster/04.primitive.html">1.4. 集合通信原语</a></li>
<li class="toctree-l2"><a class="reference internal" href="../061FW_AICluster/05.system.html">1.5. 分布式功能</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../062FW_AIAlgo/README.html">2. 分布式算法</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../062FW_AIAlgo/01.challenge.html">2.1. 大模型训练挑战</a></li>
<li class="toctree-l2"><a class="reference internal" href="../062FW_AIAlgo/02.algorithm_arch.html">2.2. 大模型算法结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../062FW_AIAlgo/03.algorithm_sota.html">2.3. 亿级规模大模型</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../063FW_Parallel/README.html">3. 分布式并行</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/01.introduction.html">3.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/02.data_parallel.html">3.2. 数据并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/03.tensor_parallel.html">3.3. 张量并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/04.mindspore_parallel.html">3.4. MindSpore张量并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/05.pipeline_parallel.html">3.5. 流水并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/06.hybrid_parallel.html">3.6. 混合并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/07.summary.html">3.7. 分布式训练总结</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../000Others/README.html">=== 附录(DONE) ===</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../000Others/instruments.html">书写工具(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/criterion.html">书写规范(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/glossary.html">术语表(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/inference.html">参考链接(DONE)</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="人工智能系统（AISys）"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../010system/README.html">=== 一、AI系统概述 ===</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../010system/01present.html">AI现状与大模型(待更)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../010system/02drive.html">AI发展驱动力(待更)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../010system/03architecture.html">AI系统全栈架构(待更)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../010system/04sample.html">AI系统样例(待更)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../010system/05principle.html">AI系统原则(待更)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../010system/06foundation.html">大模型的到来(待更)</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../020Hardware/README.html">=== 二、AI芯片体系结构 ===</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../021HW_Foundation/README.html">1. AI 计算体系概述</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/01.introduction.html">1.1. 课程内容</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/02.arch_slim.html">1.2. AI计算模式(上)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/03.mobile_parallel.html">1.3. AI计算模式(下)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/04.metrics.html">1.4. 关键设计指标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/05.matrix.html">1.5. 核心计算之矩阵乘</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/06.bit_width.html">1.6. 计算之比特位宽</a></li>
<li class="toctree-l2"><a class="reference internal" href="../021HW_Foundation/07.summary.html">1.7. AI计算体系总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../022HW_ChipBase/README.html">2. AI 芯片基础</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/01.cpu_base.html">2.1. CPU 基础</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/02.cpu_isa.html">2.2. CPU 指令集架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/03.cpu_data.html">2.3. CPU 计算本质</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/04.cpu_latency.html">2.4. CPU 计算时延</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/05.gpu.html">2.5. GPU 基础</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/06.npu.html">2.6. NPU 基础</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022HW_ChipBase/07.future.html">2.7. 超异构计算</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../023HW_GPUBase/README.html">3. GPU 原理详解</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../023HW_GPUBase/01.works.html">3.1. GPU工作原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023HW_GPUBase/02.principle.html">3.2. 为什么 GPU 适用于 AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023HW_GPUBase/03.base_concept.html">3.3. GPU架构与CUDA关系</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023HW_GPUBase/04.fermi.html">3.4. GPU架构回顾第一篇</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023HW_GPUBase/05.turing.html">3.5. GPU架构回顾第二篇</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../024HW_NVIDIA/README.html">4. NVIDIA GPU详解</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/01.basic_tc.html">4.1. TensorCore原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/02.history_tc.html">4.2. TensorCore架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/03.deep_tc.html">4.3. TensorCore剖析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/04.basic_nvlink.html">4.4. 分布式通信与NVLink</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/05.deep_nvlink.html">4.5. NVLink原理剖析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024HW_NVIDIA/06.deep_nvswitch.html">4.6. NVSwitch原理剖析</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../025HW_Abroad/README.html">5. 国外 AI 芯片架构</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/01.DOJO_Arch.html">5.1. 特斯拉DOJO架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/02.DOJO_Detail.html">5.2. 特斯拉DOJO Core原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/03.DOJO_System.html">5.3. 特斯拉DOJO存算系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/04.TPU_Introl.html">5.4. 谷歌TPU历史发展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/05.TPU1.html">5.5. 谷歌TPUv1脉动阵列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/06.TPU2.html">5.6. 谷歌TPUv2训练芯片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/07.TPU3.html">5.7. 谷歌TPUv3 POD形态</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/08.TPU4.html">5.8. 谷歌TPUv4三维互联</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025HW_Abroad/09.Future.html">5.9. 国外 AI 芯片思考</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../026HW_Domestic/README.html">6. 国内 AI 芯片架构</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/01.BR100_System.html">6.1. 壁仞产品解读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/02.BR100_Detail.html">6.2. 壁仞BR100架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/03.SUIYUAN_DTU.html">6.3. 燧原产品与DTU架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/04.cambricon_Product.html">6.4. 寒武纪产品解读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/05.cambricon_Arch.html">6.5. 寒武纪MLU芯片架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026HW_Domestic/06.cambricon_Arch.html">6.6. 寒武纪MLU架构细节</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../030Compiler/README.html">==== 三、AI编译原理(更新中)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../031CM_Tradition/README.html">1. 传统编译器(DOING)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/01.introduction.html">1.1. 编译器基础介绍 OK</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/02.history.html">1.2. 传统编译器发展 OK</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/03.gcc.html">1.3. GCC编译过程和原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/04.llvm.html">1.4. LLVM架构设计和原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/05.llvm_detail01.html">1.5. LLVM IR详解</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/06.llvm_detail02.html">1.6. LLVM前端和优化层</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/07.llvm_detail03.html">1.7. LLVM后端代码生成</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../032CM_AICompiler/README.html">2. AI 编译器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/01.appear.html">2.1. 为什么需要AI编译器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/02.stage.html">2.2. AI编译器的发展阶段</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/03.architecture.html">2.3. AI编译器的通用架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/04.future.html">2.4. AI编译器挑战与思考</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../033CM_Frontend/README.html">3. 前端优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/01.introduction.html">3.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/02.graph_ir.html">3.2. 图算 IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/03.op_fusion.html">3.3. 算子融合</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../034CM_Backend/README.html">4. 后端优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/01.introduction.html">4.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/02.ops_compute.html">4.2. 算子的计算与调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/03.optimization.html">4.3. 算子手工优化方式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/04.loop_opt.html">4.4. 算子循环优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/05.other_opt.html">4.5. 指令和内存优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/06.auto_tuning.html">4.6. Auto-Tuning原理</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../036CM_PyTorch/README.html">5. PyTorch2.0 图模式</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/01.introduction.html">5.1. PyTorch2.0 特性</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/02.torchscript.html">5.2. TorchScript 静态图尝试</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/03.torchfx_lazy.html">5.3. FX 与 LazyTensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/04.torchdynamo.html">5.4. TorchDynamo 获取图</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/05.aotatuograd.html">5.5. AOTAutograd 原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/06.dispatch.html">5.6. Dispatch 机制</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../040Inference/README.html">=== 四、推理系统&amp;引擎 ===</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../041INF_Inference/README.html">1. 推理系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/01.introduction.html">1.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/02.constraints.html">1.2. 推理系统介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/03.workflow.html">1.3. 推理流程全景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/04.system.html">1.4. 推理系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/05.inference.html">1.5. 推理引擎架构（上）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/06.architecture.html">1.6. 推理引擎架构（下）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../042INF_Mobilenet/README.html">2. 模型轻量化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/01.introduction.html">2.1. 推理参数了解</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/02.cnn.html">2.2. CNN模型小型化（上）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/03.cnn.html">2.3. CNN模型小型化（下）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/04.transformer.html">2.4. Transformer小型化</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../043INF_Slim/README.html">3. 模型压缩</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/01.introduction.html">3.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/02.quant.html">3.2. 低比特量化原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/03.qat.html">3.3. 感知量化训练QAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/04.ptq.html">3.4. 训练后量化PTQ与部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/05.pruning.html">3.5. 模型剪枝原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/06.distillation.html">3.6. 知识蒸馏原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/07.distillation.html">3.7. 知识蒸馏算法</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../044INF_Converter/README.html">4. 模型转换&amp;优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/01.introduction.html">4.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/02.converter_princ.html">4.2. 架构与文件格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/03.converter_ir.html">4.3. 自定义计算图IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/04.converter_detail.html">4.4. 模型转换流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/05.optimizer.html">4.5. 计算图优化策略</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/06.basic.html">4.6. 常量折叠&amp;冗余节点消除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/07.extend.html">4.7. 算子融合/替换/前移</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../045INF_Kernel/README.html">5. Kernel优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/01.introduction.html">5.1. Kernel优化架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/02.conv.html">5.2. 卷积操作原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/03.im2col.html">5.3. Im2Col算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/04.winograd.html">5.4. Winograd算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/05.qnnpack.html">5.5. QNNPack算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/06.memory.html">5.6. 推理内存布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/07.nc4hw4.html">5.7. nc4hw4内存排布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/08.others.html">5.8. 汇编与循环优化</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../050Framework/README.html">=== 五、AI框架核心模块 ===</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../051FW_Foundation/README.html">1. AI框架基础(DONE)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../051FW_Foundation/01.introduction.html">1.1. 本章内容</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051FW_Foundation/02.fundamentals.html">1.2. AI框架作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051FW_Foundation/03.history.html">1.3. AI框架之争</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051FW_Foundation/04.programing.html">1.4. 框架编程范式</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="README.html">2. 自动微分(DONE)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01.introduction.html">2.1. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="02.base_concept.html">2.2. 什么是微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.grad_mode.html">2.3. 微分计算模式</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">2.4. 微分实现方式</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.forward_mode.html">2.5. 动手实现自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="06.reversed_mode.html">2.6. 动手实现PyTorch微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="07.challenge.html">2.7. 自动微分的挑战&amp;未来</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../053FW_DataFlow/README.html">3. 计算图(DONE)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/01.introduction.html">3.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/02.computegraph.html">3.2. 计算图原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/03.atuodiff.html">3.3. 计算图与自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/04.dispatch.html">3.4. 计算图的调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/05.control_flow.html">3.5. 计算图的控制流实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/06.static_graph.html">3.6. 动态图与静态图转换</a></li>
<li class="toctree-l2"><a class="reference internal" href="../053FW_DataFlow/07.future.html">3.7. 计算图的挑战&amp;未来</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../060Foundation/README.html">==== 六、大模型训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../061FW_AICluster/README.html">1. 分布式集群</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../061FW_AICluster/01.introduction.html">1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../061FW_AICluster/02.architecture.html">1.2. AI集群服务器架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../061FW_AICluster/03.communication.html">1.3. AI集群软硬件通信</a></li>
<li class="toctree-l2"><a class="reference internal" href="../061FW_AICluster/04.primitive.html">1.4. 集合通信原语</a></li>
<li class="toctree-l2"><a class="reference internal" href="../061FW_AICluster/05.system.html">1.5. 分布式功能</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../062FW_AIAlgo/README.html">2. 分布式算法</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../062FW_AIAlgo/01.challenge.html">2.1. 大模型训练挑战</a></li>
<li class="toctree-l2"><a class="reference internal" href="../062FW_AIAlgo/02.algorithm_arch.html">2.2. 大模型算法结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../062FW_AIAlgo/03.algorithm_sota.html">2.3. 亿级规模大模型</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../063FW_Parallel/README.html">3. 分布式并行</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/01.introduction.html">3.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/02.data_parallel.html">3.2. 数据并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/03.tensor_parallel.html">3.3. 张量并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/04.mindspore_parallel.html">3.4. MindSpore张量并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/05.pipeline_parallel.html">3.5. 流水并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/06.hybrid_parallel.html">3.6. 混合并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../063FW_Parallel/07.summary.html">3.7. 分布式训练总结</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../000Others/README.html">=== 附录(DONE) ===</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../000Others/instruments.html">书写工具(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/criterion.html">书写规范(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/glossary.html">术语表(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/inference.html">参考链接(DONE)</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <!--适用于[License](https://github.com/chenzomi12/DeepLearningSystem/blob/main/LICENSE)版权许可--><div class="section" id="id1">
<h1><span class="section-number">2.4. </span>微分实现方式<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<p>在上一节了解到了正反向模式只是自动微分的原理模式，在实际代码实现的过程，正方向模式只是提供一个原理性的指导，在真正编码过程会有很多细节需要打开，例如如何解析表达式，如何记录反向求导表达式的操作等等。这一节中，希望通过介绍目前比较热门的方法给大家普及一下自动微分的具体实现。</p>
<div class="section" id="id2">
<h2><span class="section-number">2.4.1. </span>微分实现关键步骤<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h2>
<p>了解自动微分的不同实现方式非常有用。在这里呢，我们将介绍主要的自动微分实现方法。在上一篇的文章中，我们介绍了自动微分的基本数学原理。可以总结自动微分的关键步骤为：</p>
<ul class="simple">
<li><p>分解程序为一系列已知微分规则的基础表达式的组合</p></li>
<li><p>根据已知微分规则给出各基础表达式的微分结果</p></li>
<li><p>根据基础表达式间的数据依赖关系，使用链式法则将微分结果组合完成程序的微分结果</p></li>
</ul>
<p>虽然自动微分的数学原理已经明确，包括正向和反向的数学逻辑和模式。但具体的实现方法则可以有很大的差异，2018
年，Siskind 等学者在其综述论文Automatic Differentiation in Machine
Learning: a Survey [1] 中对自动微分实现方案划分为三类：</p>
<p><strong>基本表达式</strong>：基本表达式或者称元素库（Elemental
Libraries），基于元素库中封装一系列基本的表达式（如：加减乘除等）及其对应的微分结果表达式，作为库函数。用户通过调用库函数构建需要被微分的程序。而封装后的库函数在运行时会记录所有的基本表达式和相应的组合关系，最后使用链式法则对上述基本表达式的微分结果进行组合完成自动微分。</p>
<p><strong>操作符重载</strong>：操作符重载或者称运算重载（Operator
Overloading，OO），利用现代语言的多态特性（例如C++/JAVA/Python等高级语言），使用操作符重载对语言中基本运算表达式的微分规则进行封装。同样，重载后的操作符在运行时会记录所有的操作符和相应的组合关系，最后使用链式法则对上述基本表达式的微分结果进行组合完成自动微分。</p>
<p><strong>源代码变换</strong>：源代码变换或者叫做源码转换（Source Code
Transformation，SCT）则是通过对语言预处理器、编译器或解释器的扩展，将其中程序表达（如：源码、AST抽象语法树
或 编译过程中的中间表达
IR）的基本表达式微分规则进行预定义，再对程序表达进行分析得到基本表达式的组合关系，最后使用链式法则对上述基本表达式的微分结果进行组合生成对应微分结果的新程序表达，完成自动微分。</p>
<p>任何 AD 实现中的一个主要考虑因素是 AD
运算时候引入的性能开销。就计算复杂性而言，AD
需要保证算术量增加不超过一个小的常数因子。另一方面，如果不小心管理 AD
算法，可能会带来很大的开销。例如，简单的分配数据结构来保存对偶数（正向运算和反向求导），将涉及每个算术运算的内存访问和分配，这通常比现代计算机上的算术运算更昂贵。同样，使用运算符重载可能会引入伴随成本的方法分派，与原始函数的原始数值计算相比，这很容易导致一个数量级的减速。</p>
<p>下面这个图是论文作者回顾了一些比较通用的 AD 实现。</p>
<div class="figure align-default">
<img alt="../_images/autodiff06.png" src="../_images/autodiff06.png" />
</div>
</div>
<div class="section" id="id3">
<h2><span class="section-number">2.4.2. </span>基本表达式<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h2>
<p>基本表达式法也叫做元素库（Elemental
Libraries），程序中实现构成自动微分中计算的最基本的类别或者表达式，并通过调用自动微分中的库，来代替数学逻辑运算来工作。然后在函数定义中使用库公开的方法，这意味着在编写代码时，手动将任何函数分解为基本操作。</p>
<p>这个方法呢从自动微分刚出现的时候就已经被广泛地使用，典型的例子是 Lawson
(1971) 的 WCOMP 和 UCOMP 库，Neidinger (1989) 的 APL 库，以及 Hinkins
(1994) 的工作。同样，Rich 和 Hill (1992) 使用基本表达式法在 MATLAB
中制定了他们的自动微分实现。</p>
<p>以公式为例子：</p>
<div class="math notranslate nohighlight" id="equation-diff-04-eq1">
<span class="eqno">(2.4.1)<a class="headerlink" href="#equation-diff-04-eq1" title="Permalink to this equation">¶</a></span>\[f(x1,x2)=ln(x1)+x1*x2−sin(x2)\]</div>
<p>用户首先需要手动将公式1中的各个操作，或者叫做子函数，分解为库函数中基本表达式组合：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">t3</span> <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span>
<span class="n">t4</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span>
<span class="n">t5</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span>
</pre></div>
</div>
<p>使用给定的库函数，完成上述函数的程序设计：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">参数为变量</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">t</span> <span class="n">和对应的导数变量</span> <span class="n">dx</span><span class="p">,</span><span class="n">dy</span><span class="p">,</span><span class="n">dt</span>
<span class="k">def</span> <span class="nf">ADAdd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>

<span class="o">//</span> <span class="n">同理对上面的公式实现对应的函数</span>
<span class="k">def</span> <span class="nf">ADSub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ADMul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ADLog</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ADSin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
</pre></div>
</div>
<p>而库函数中则定义了对应表达式的数学微分规则，和对应的链式法则:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">参数为变量</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">t</span> <span class="n">和对应的导数变量</span> <span class="n">dx</span><span class="p">,</span><span class="n">dy</span><span class="p">,</span><span class="n">dt</span>
<span class="k">def</span> <span class="nf">ADAdd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">dy</span> <span class="o">+</span> <span class="n">dx</span>

<span class="o">//</span> <span class="n">参数为变量</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">t</span> <span class="n">和对应的导数变量</span> <span class="n">dx</span><span class="p">,</span><span class="n">dy</span><span class="p">,</span><span class="n">dt</span>
<span class="k">def</span> <span class="nf">ADSub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">dy</span> <span class="o">-</span> <span class="n">dx</span>

<span class="o">//</span> <span class="o">...</span> <span class="n">以此类推</span>
</pre></div>
</div>
<p>针对公式1中基本表达式法，可以按照下面示例代码来实现正向的推理功能，反向其实也是一样，不过调用代码更复杂一点：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">xxx</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">xxx</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">ADlog</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">ADSin</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="n">t3</span> <span class="o">=</span> <span class="n">ADMul</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
<span class="n">t4</span> <span class="o">=</span> <span class="n">ADAdd</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t3</span><span class="p">)</span>
<span class="n">t5</span> <span class="o">=</span> <span class="n">ADSub</span><span class="p">(</span><span class="n">t4</span><span class="p">,</span> <span class="n">t2</span><span class="p">)</span>
</pre></div>
</div>
<p>基本表达式法的<strong>优点</strong>可以总结如下：</p>
<ul class="simple">
<li><p>实现简单，基本可在任意语言中快速地实现为库</p></li>
</ul>
<p>基本表达式法的<strong>缺点</strong>可以总结如下：</p>
<ul class="simple">
<li><p>用户必须使用库函数进行编程，而无法使用语言原生的运算表达式；</p></li>
<li><p>另外实现逻辑和代码也会冗余较长，依赖于开发人员较强的数学背景</p></li>
</ul>
<p>基本表达式法在没有操作符重载AD的80到90年代初期，仍然是计算机中实现自动微分功能最简单和快捷的策略啦。</p>
</div>
<div class="section" id="id4">
<h2><span class="section-number">2.4.3. </span>操作符重载<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h2>
<p>在具有多态特性的现代编程语言中，运算符重载提供了实现自动微分的最直接方式，利用了编程语言的第一特性（first
class feature），重新定义了微分基本操作语义的能力。</p>
<p>在 C++ 中使用运算符重载实现的流行工具是 ADOL-C（Walther 和
Griewank，2012）。 ADOL-C 要求对变量使用启用 AD 的类型，并在 Tape
数据结构中记录变量的算术运算，随后可以在反向模式 AD 计算期间“回放”。
Mxyzptlk 库 (Michelotti, 1990) 是 C++
能够通过前向传播计算任意阶偏导数的另一个例子。 FADBAD++ 库（Bendtsen 和
Stauning，1996 年）使用模板和运算符重载为 C++ 实现自动微分。对于 Python
语言来说，autograd 提供正向和反向模式自动微分，支持高阶导数。</p>
<p>在机器学习 ML 或者深度学习 DL
领域，目前AI框架中使用操作符重载的一个典型代表是
Pytroch，其中使用数据结构 Tape
来记录计算流程，在反向模式求解梯度的过程中进行replay Operator。</p>
<ol class="arabic simple">
<li><p>操作符重载来实现自动微分的功能里面，很重要的是利用高级语言的特性。下面简单看看伪代码，这里面我们定义一个特殊的数据结构
<code class="docutils literal notranslate"><span class="pre">Variable</span></code>，然后基于 <code class="docutils literal notranslate"><span class="pre">Variable</span></code> 重载一系列的操作如 <code class="docutils literal notranslate"><span class="pre">__mul__</span></code>
代替 * 操作。</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Variable</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ops_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

   <span class="c1"># 同样重载各种不同的基础操作</span>
   <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
   <span class="k">def</span> <span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
   <span class="k">def</span> <span class="nf">__div__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>实现操作符重载后的计算。</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ops_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>接着通过一个 Tape 的数据结构，来记录每次 <code class="docutils literal notranslate"><span class="pre">Variable</span></code>
执行计算的顺序，Tape
这里面主要是记录正向的计算，把输入、输出和执行运算的操作符记录下来。</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Tape</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="p">:</span> <span class="p">[]</span>
    <span class="n">outputs</span> <span class="p">:</span> <span class="p">[]</span>
    <span class="n">propagate</span> <span class="p">:</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outpus</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>因为大部分 ML 系统或者 AI 框架采用的是反向模式，因此最后会逆向遍历
Tape
里面的数据（相当于反向传播或者反向模式的过程），然后累积反向计算的梯度。</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 反向求导的过程，类似于 Pytroch 的 backward 接口</span>
<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>

    <span class="c1"># 通过 reversed 操作把带有梯度信息的 tape 逆向遍历</span>
    <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">gradient_tape</span><span class="p">):</span>
        <span class="c1"># 进行梯度累积，反向传播给上一次的操作计算</span>
        <span class="n">dl_d</span><span class="p">[</span><span class="nb">input</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dl_dinput</span>
</pre></div>
</div>
<p>当然啦，我们会在下一节当中带着大家亲自通过操作符重载实现一个前向的自动微分和后向的自动微分。下面总结一下操作符重载的一个基本流程：</p>
<ul class="simple">
<li><p>预定义了特定的数据结构，并对该数据结构重载了相应的基本运算操作符</p></li>
<li><p>程序在实际执行时会将相应表达式的操作类型和输入输出信息记录至特殊数据结构</p></li>
<li><p>得到特殊数据结构后，将对数据结构进行遍历并对其中记录的基本运算操作进行微分</p></li>
<li><p>把结果通过链式法则进行组合，完成自动微分</p></li>
</ul>
<p>操作符重载法的<strong>优点</strong>可以总结如下：</p>
<ul class="simple">
<li><p>实现简单，只要求语言提供多态的特性能力</p></li>
<li><p>易用性高，重载操作符后跟使用原生语言的编程方式类似</p></li>
</ul>
<p>操作符重载法的<strong>缺点</strong>可以总结如下：</p>
<ul class="simple">
<li><p>需要显式的构造特殊数据结构和对特殊数据结构进行大量读写、遍历操作，这些额外数据结构和操作的引入不利于高阶微分的实现</p></li>
<li><p>对于一些类似 if，while
等控制流表达式，难以通过操作符重载进行微分规则定义。对于这些操作的处理会退化成基本表达式方法中特定函数封装的方式，难以使用语言原生的控制流表达式</p></li>
</ul>
</div>
<div class="section" id="id5">
<h2><span class="section-number">2.4.4. </span>源代码转换<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h2>
<p>源码转换（Source Code
Transformation，SCT）是最复杂的，实现起来也是非常具有挑战性。</p>
<p>源码转换的实现提供了对编程语言的扩展，可自动将算法分解为支持自动微分的基本操作。通常作为预处理器执行，以将扩展语言的输入转换为原始语言。简单来说就是利用源语言来实现领域扩展语言
DSL 的操作方式。</p>
<p>源代码转换的经典实例包括 Fortran 预处理器 GRESS（Horwedel 等人，1988
年）和 PADRE2（Kubo 和 Iri，1990 年），在编译之前将启用 AD 的 Fortran
变体转换为标准 Fortran。类似地，ADIFOR 工具 (Bischof et al., 1996)
给定一个 Fortran
源代码，生成一个增强代码，其中除了原始结果之外还计算所有指定的偏导数。对于以
ANSI C 编码的过程，ADIC 工具（Bischof
等人，1997）在指定因变量和自变量之后将 AD 实现为源代码转换。
Tapenade（Pascual 和 Hasco¨et，2008 年；Hasco¨et 和 Pascual，2013
年）是过去10年终 SCT 的流行工具，它为 Fortran 和 C
程序实现正向和反向模式 AD。</p>
<p>除了通过源代码转换进行语言扩展外，还有一些实现通过专用编译器或解释器引入了具有紧密集成的
AD 功能的新语言。一些最早的 AD 工具，例如 SLANG (Adamson and Winant,
1969) 和 PROSE (Pfeiffer, 1987) 属于这一类。 NAGWare Fortran 编译器
(Naumann and Riehme, 2005) 是一个较新的示例，其中使用与 AD
相关的扩展会在编译时触发衍生代码的自动生成。</p>
<p>作为基于解释器的实现的一个例子，代数建模语言 AMPL (Fourer et al., 2002)
可以用数学符号表示目标和约束，系统从中推导出活动变量并安排必要的 AD
计算。此类别中的其他示例包括基于类似 Algol 的 DIFALG 语言的 FM/FAD 包
(Mazourik, 1991)，以及类似于 Pascal 的面向对象的 COZY 语言 (Berz et al.,
1996)。</p>
<p>而华为全场景AI框架 MindSpore 则是基于 Python 语言使用源代码转换实现 AD
的正反向模式，并采用了函数式编程的风格，该机制可以用控制流表示复杂的组合。函数被转换成函数中间表达（Intermediate
Representation，IR），中间表达构造出一个能够在不同设备上解析和执行的计算图。在执行前，计算图上应用了多种软硬件协同优化技术，以提升端、边、云等不同场景下的性能和效率。</p>
<p>其主要流程是：分析获得源程序的 AST 表达形式；然后基于 AST
完成基本表达式的分解和微分操作；再通过遍历 AST
得到基本表达式间的依赖关系，从而应用链式法则完成自动微分。</p>
<p>因为源码转换涉及到底层的抽象语法树、编译执行等细节，因此这里就不给出伪代码了（实在太难了给不出来），我们通过下面这张图来简单了解下
SCT 的一般性过程。</p>
<div class="figure align-default" id="id7">
<img alt="../_images/ast.png" src="../_images/ast.png" />
<p class="caption"><span class="caption-number">图2.4.1 </span><span class="caption-text">源码转换流程示例</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p>从图中可以看到源码转换的整体流程分为编译时间和执行时间，以 MindSpore
为例，其在运行之前的第一个 epoch
会等待一段时间，是因为需要对源码进行编译转换解析等一系列的操作。然后再
run time
运行时则会比较顺畅，直接对数据和代码不断地按照计算机指令来高速执行。</p>
<p>编译阶段呢，在 Initialization 过程中会对源码进行 Parse
转换成为抽象语法树 AST，接着转换为基于图表示的中间表达
IR，这个基于图的IR从概念上理解可以理解为计算图，神经网络层数的表示通过图表示会比较直观。</p>
<p>接着对 Graph base IR进行一些初级的类型推导，特别是针对 Tensor/List/Str
等不同的基础数据表示，然后进行宏展开，还有语言单态化，最后再对变量或者自变量进行类型推导。可以从图中看到，很多地方出现了不同形式的
IR，IR 其实是编译器中常用的一个中间表达概念，在编译的 Pass
中会有很多处理流程，每一步处理流程产生一个 IR，交给下一个Pass进行处理。</p>
<p>最后通过 LLVM 或者其他等不同的底层编译器，最后把 IR
编译成机器码，然后就可以真正地在runtime执行起来。</p>
<p>源码转换法的<strong>优点</strong>可以总结如下：</p>
<ul class="simple">
<li><p>支持更多的数据类型（原生和用户自定义的数据类型） +
原生语言操作（基本数学运算操作和控制流操作）</p></li>
<li><p>高阶微分中实现容易，不用每次使用 Tape
来记录高阶的微分中产生的大量变量，而是统一通过编译器进行额外变量优化和重计算等优化</p></li>
<li><p>进一步提升性能，没有产生额外的 tape 数据结构和 tape
读写操作，除了利于实现高阶微分以外，还能够对计算表达式进行统一的编译优化</p></li>
</ul>
<p>源码转换法的<strong>缺点</strong>可以总结如下：</p>
<ul class="simple">
<li><p>实现复杂，需要扩展语言的预处理器、编译器或解释器，深入计算机体系和底层编译</p></li>
<li><p>支持更多数据类型和操作，用户自由度虽然更高，但同时更容易写出不支持的代码导致错误</p></li>
<li><p>微分结果是以代码的形式存在，在执行计算的过程当中，特别是深度学习中大量使用for循环过程中间错误了，或者是数据处理流程中出现错误，并不利于深度调试</p></li>
</ul>
</div>
<div class="section" id="id6">
<h2><span class="section-number">2.4.5. </span>本节视频<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h2>
<html><iframe src="https://player.bilibili.com/player.html?aid=901075118&amp;bvid=BV1BN4y1P76t&amp;cid=911355268&amp;page=1&amp;as_wide=1&amp;high_quality=1&amp;danmaku=0&amp;t=30&amp;autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe></html></div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">2.4. 微分实现方式</a><ul>
<li><a class="reference internal" href="#id2">2.4.1. 微分实现关键步骤</a></li>
<li><a class="reference internal" href="#id3">2.4.2. 基本表达式</a></li>
<li><a class="reference internal" href="#id4">2.4.3. 操作符重载</a></li>
<li><a class="reference internal" href="#id5">2.4.4. 源代码转换</a></li>
<li><a class="reference internal" href="#id6">2.4.5. 本节视频</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="03.grad_mode.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>2.3. 微分计算模式</div>
         </div>
     </a>
     <a id="button-next" href="05.forward_mode.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>2.5. 动手实现自动微分</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>