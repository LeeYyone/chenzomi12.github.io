<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>自动微分实现：正向OO实现自动微分</title>
    <link href="/2022/07/31/ad04/"/>
    <url>/2022/07/31/ad04/</url>
    
    <content type="html"><![CDATA[<p>写【自动微分】原理和实现系列文章，存粹是为了梳理在 MindSpore 当SE时候最核心的自动微分原理。网上看了很多文章，基本上都是很零散，当然Automatic Differentiation in Machine Learning: a Survey[1] 这篇文章是目前ZOMI觉得比较好关于自动微分的综述论文。</p><ul><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/518198564">01. 一文看懂AD原理</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/518296942">02. AD的正反向模式</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/520065656">03. AD常用实现方案</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/520065656">04. 正向OO实现自动微分</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/547865589/">05. 反向OO实现自动微分(Pytroch核心机制)</a></li></ul><p>在这篇文章里，ZOMI会介绍是怎么实现自动微分的，因为代码量非常小，也许你也可以写一个玩玩。前面的文章当中，已经把自动微分的原理深入浅出的讲了一下，也引用了非常多的论文。有兴趣的可以顺着综述A survey这篇深扒一下。</p><h1 id="前向自动微分原理"><a href="#前向自动微分原理" class="headerlink" title="前向自动微分原理"></a>前向自动微分原理</h1><p>了解自动微分的不同实现方式非常有用。在这里呢，我们将介绍主要的前向自动微分，通过Python这个高级语言来实现操作符重载。在正反向模式中的这篇的文章中，我们介绍了前向自动微分的基本数学原理。</p><blockquote><p>前向模式（Forward Automatic Differentiation，也叫做 tangent mode AD）或者前向累积梯度（前向模式）</p></blockquote><p>前向自动微分中，从计算图的起点开始，沿着计算图边的方向依次向前计算，最终到达计算图的终点。它根据自变量的值计算出计算图中每个节点的值 以及其导数值，并保留中间结果。一直得到整个函数的值和其导数值。整个过程对应于一元复合函数求导时从最内层逐步向外层求导。</p><p><img src="/images/ad/WX20220522-180607@2x.png"></p><p>简单确实简单，可以总结前向自动微分关键步骤为：</p><ul><li>分解程序为一系列已知微分规则的基础表达式的组合</li><li>根据已知微分规则给出各基础表达式的微分结果</li><li>根据基础表达式间的数据依赖关系，使用链式法则将微分结果组合完成程序的微分结果</li></ul><p>而通过Python高级语言，进行操作符重载后的关键步骤其实也相类似：</p><ul><li>分解程序为一系列已知微分规则的基础表达式组合，并使用高级语言的重载操作</li><li>在重载运算操作的过程中，根据已知微分规则给出各基础表达式的微分结果</li><li>根据基础表达式间的数据依赖关系，使用链式法则将微分结果组合完成程序的微分结果</li></ul><h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p>首先呢，我们需要加载通用的numpy库，用于实际运算的，如果不用numpy，在python中也可以使用math来代替。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br></code></pre></td></tr></table></figure><p>前向自动微分又叫做tangent mode AD，所以我们准备一个叫做ADTangent的类，这类初始化的时候有两个参数，一个是 x，表示输入具体的数值；另外一个是 dx，表示经过对自变量 x 求导后的值。</p><p>需要注意的是，操作符重载自动微分不像源码转换可以给出求导的公式，一般而言并不会给出求导公式，而是直接给出最后的求导值，所以就会有 dx 的出现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ADTangent</span>:<br>    <br>    <span class="hljs-comment"># 自变量 x，对自变量进行求导得到的 dx</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, dx</span>):<br>        self.x = x<br>        self.dx = dx<br>    <br>    <span class="hljs-comment"># 重载 str 是为了方便打印的时候，看到输入的值和求导后的值</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__str__</span>(<span class="hljs-params">self</span>):<br>        context = <span class="hljs-string">f&#x27;value:<span class="hljs-subst">&#123;self.x:<span class="hljs-number">.4</span>f&#125;</span>, grad:<span class="hljs-subst">&#123;self.dx&#125;</span>&#x27;</span><br>        <span class="hljs-keyword">return</span> context<br></code></pre></td></tr></table></figure><p>下面是核心代码，也就是操作符重载的内容，在 ADTangent 类中通过 Python 私有函数重载加号，首先检查输入的变量 other 是否属于 ADTangent，如果是那么则把两者的自变量 x 进行相加。</p><p>其中值得注意的就是 dx 的计算，因为是正向自动微分，因此每一个前向的计算都会有对应的反向求导计算。求导的过程是这个程序的核心，不过不用担心的是这都是最基础的求导法则。最后返回自身的对象 ADTangent(x, dx)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__add__</span>(<span class="hljs-params">self, other</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(other, ADTangent):<br>        x = self.x + other.x<br>        dx = self.dx + other.dx<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(other, <span class="hljs-built_in">float</span>):<br>        x = self.x + other<br>        dx = self.dx<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> NotImplementedError<br>    <span class="hljs-keyword">return</span> ADTangent(x, dx)<br></code></pre></td></tr></table></figure><p>下面则是对减号、乘法、log、sin几个操作进行操作符重载，正向的重载的过程比较简单，基本都是按照上面的 <strong>add</strong> 的代码讨论来实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__sub__</span>(<span class="hljs-params">self, other</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(other, ADTangent):<br>        x = self.x - other.x<br>        dx = self.dx - other.dx<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(other, <span class="hljs-built_in">float</span>):<br>        x = self.x - other<br>        ex = self.dx<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> NotImplementedError<br>    <span class="hljs-keyword">return</span> ADTangent(x, dx)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__mul__</span>(<span class="hljs-params">self, other</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(other, ADTangent):<br>        x = self.x * other.x<br>        dx = self.x * other.dx + self.dx * other.x<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(other, <span class="hljs-built_in">float</span>):<br>        x = self.x * other<br>        dx = self.dx * other<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> NotImplementedError<br>    <span class="hljs-keyword">return</span> ADTangent(x, dx)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log</span>(<span class="hljs-params">self</span>):<br>    x = np.log(self.x)<br>    dx = <span class="hljs-number">1</span> / self.x * self.dx<br>    <span class="hljs-keyword">return</span> ADTangent(x, dx)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sin</span>(<span class="hljs-params">self</span>):<br>    x = np.sin(self.x)<br>    dx = self.dx * np.cos(self.x)<br>    <span class="hljs-keyword">return</span> ADTangent(x, dx)<br></code></pre></td></tr></table></figure><p>以公式5为例：</p><p>$$<br>f(x1,x2)&#x3D;ln(x1)+x1x2−sin(x2) \tag{1}<br>$$</p><p>因为是基于 ADTangent 类进行了操作符重载，因此在初始化自变量 x 和 y 的值需要使用 ADTangent 来初始化，然后通过代码 f &#x3D; ADTangent.log(x) + x * y - ADTangent.sin(y) 来实现。</p><p>由于这里是求 f 关于自变量 x 的导数，因此初始化数据的时候呢，自变量 x 的 dx 设置为1，而自变量 y 的 dx 设置为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x = ADTangent(x=<span class="hljs-number">2.</span>, dx=<span class="hljs-number">1</span>)<br>y = ADTangent(x=<span class="hljs-number">5.</span>, dx=<span class="hljs-number">0</span>)<br>f = ADTangent.log(x) + x * y - ADTangent.sin(y)<br><span class="hljs-built_in">print</span>(f)<br></code></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">value:11.6521, grad:5.5<br></code></pre></td></tr></table></figure><p>从输出结果来看，正向计算的输出结果是跟上面图相同，而反向的导数求导结果也与上图相同。下面一个是 Pytroch 的实现结果对比，最后是MindSpore的实现结果对比。</p><p>可以看到呢，上面的简单实现的自动微分结果和 Pytroch 、MindSpore是相同的。还是很有意思的。</p><p>Pytroch 对公式1的自动微分结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable<br><br>x = Variable(torch.Tensor([<span class="hljs-number">2.</span>]), requires_grad=<span class="hljs-literal">True</span>)<br>y = Variable(torch.Tensor([<span class="hljs-number">5.</span>]), requires_grad=<span class="hljs-literal">True</span>)<br>f = torch.log(x) + x * y - torch.sin(y)<br>f.backward()<br><br><span class="hljs-built_in">print</span>(f)<br><span class="hljs-built_in">print</span>(x.grad)<br><span class="hljs-built_in">print</span>(y.grad)<br></code></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">tensor([11.6521], grad_fn=&lt;SubBackward0&gt;)<br>tensor([5.5000])<br>tensor([1.7163])<br></code></pre></td></tr></table></figure><p>MindSpore 对公式1的自动微分结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> mindspore.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> mindspore <span class="hljs-keyword">import</span> Parameter, Tensor<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Fun</span>(nn.Cell):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Fun, self).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">construct</span>(<span class="hljs-params">self, x, y</span>):<br>        f = ops.log(x) + x * y - ops.sin(y)<br>        <span class="hljs-keyword">return</span> f<br>    <br>x = Tensor(np.array([<span class="hljs-number">2.</span>], np.float32))<br>y = Tensor(np.array([<span class="hljs-number">5.</span>], np.float32))<br>f = Fun()(x, y)<br><br>grad_all = ops.GradOperation()<br>grad = grad_all(Fun())(x, y)<br><br><span class="hljs-built_in">print</span>(f)<br><span class="hljs-built_in">print</span>(grad[<span class="hljs-number">0</span>])<br><br></code></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">[11.65207]<br>5.5<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>AutoGrad</tag>
      
      <tag>AI System</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动微分原理：AD常用实现方案</title>
    <link href="/2022/07/31/ad03/"/>
    <url>/2022/07/31/ad03/</url>
    
    <content type="html"><![CDATA[<p>写【自动微分】原理和实现系列文章，存粹是为了梳理在 MindSpore 当SE时候最核心的自动微分原理。网上看了很多文章，基本上都是很零散，当然Automatic Differentiation in Machine Learning: a Survey[1] 这篇文章是目前ZOMI觉得比较好关于自动微分的综述论文。</p><ul><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/518198564">01. 一文看懂AD原理</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/518296942">02. AD的正反向模式</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/520065656">03. AD常用实现方案</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/520065656">04. 正向OO实现自动微分</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/547865589/">05. 反向OO实现自动微分(Pytroch核心机制)</a></li></ul><p>第一篇<a href="https://zhuanlan.zhihu.com/p/518198564">自动微分原理</a>文章中我们大概初步谈了谈从手动微分到自动微分的过程，第二篇<a href="https://zhuanlan.zhihu.com/p/518296942">自动微分正反模式</a>中深入了自动微分的正反向模式具体公式和推导。</p><p>实际上第二章了解到正反向模式只是自动微分的原理模式，在实际代码实现的过程，正方向模式只是提供一个原理性的指导，在真正编码过程会有很多细节需要打开，例如如何解析表达式，如何记录反向求导表达式的操作等等。这一篇文章中，ZOMI希望通过介绍目前比较热门的方法给大家普及一下自动微分的具体实现。</p><h2 id="自动微分实现"><a href="#自动微分实现" class="headerlink" title="自动微分实现"></a>自动微分实现</h2><p>了解自动微分的不同实现方式非常有用。在这里呢，我们将介绍主要的自动微分实现方法。在上一篇的文章中，我们介绍了自动微分的基本数学原理。可以总结自动微分的关键步骤为：</p><ul><li>分解程序为一系列已知微分规则的基础表达式的组合</li><li>根据已知微分规则给出各基础表达式的微分结果</li><li>根据基础表达式间的数据依赖关系，使用链式法则将微分结果组合完成程序的微分结果</li></ul><p>虽然自动微分的数学原理已经明确，包括正向和反向的数学逻辑和模式。但具体的实现方法则可以有很大的差异，2018 年，Siskind 等学者在其综述论文Automatic Differentiation in Machine Learning: a Survey [1] 中对自动微分实现方案划分为三类：</p><p><strong>基本表达式</strong>：基本表达式或者称元素库（Elemental Libraries），基于元素库中封装一系列基本的表达式（如：加减乘除等）及其对应的微分结果表达式，作为库函数。用户通过调用库函数构建需要被微分的程序。而封装后的库函数在运行时会记录所有的基本表达式和相应的组合关系，最后使用链式法则对上述基本表达式的微分结果进行组合完成自动微分。</p><p><strong>操作符重载</strong>：操作符重载或者称运算重载（Operator Overloading，OO），利用现代语言的多态特性（例如C++&#x2F;JAVA&#x2F;Python等高级语言），使用操作符重载对语言中基本运算表达式的微分规则进行封装。同样，重载后的操作符在运行时会记录所有的操作符和相应的组合关系，最后使用链式法则对上述基本表达式的微分结果进行组合完成自动微分。</p><p><strong>源代码变换</strong>：源代码变换或者叫做源码转换（Source Code Transformation，SCT）则是通过对语言预处理器、编译器或解释器的扩展，将其中程序表达（如：源码、AST抽象语法树 或 编译过程中的中间表达 IR）的基本表达式微分规则进行预定义，再对程序表达进行分析得到基本表达式的组合关系，最后使用链式法则对上述基本表达式的微分结果进行组合生成对应微分结果的新程序表达，完成自动微分。</p><p>任何 AD 实现中的一个主要考虑因素是 AD 运算时候引入的性能开销。就计算复杂性而言，AD 需要保证算术量增加不超过一个小的常数因子。另一方面，如果不小心管理 AD 算法，可能会带来很大的开销。例如，简单的分配数据结构来保存对偶数（正向运算和反向求导），将涉及每个算术运算的内存访问和分配，这通常比现代计算机上的算术运算更昂贵。同样，使用运算符重载可能会引入伴随成本的方法分派，与原始函数的原始数值计算相比，这很容易导致一个数量级的减速。</p><p>下面这个图是论文作者回顾了一些比较通用的 AD 实现。</p><p><img src="/images/ad/WX20220525-180618@2x.png"></p><h2 id="基本表达式"><a href="#基本表达式" class="headerlink" title="基本表达式"></a>基本表达式</h2><p>基本表达式法也叫做元素库（Elemental Libraries），程序中实现构成自动微分中计算的最基本的类别或者表达式，并通过调用自动微分中的库，来代替数学逻辑运算来工作。然后在函数定义中使用库公开的方法，这意味着在编写代码时，手动将任何函数分解为基本操作。</p><p>这个方法呢从自动微分刚出现的时候就已经被广泛地使用，典型的例子是 Lawson (1971) 的 WCOMP 和 UCOMP 库，Neidinger (1989) 的 APL 库，以及 Hinkins (1994) 的工作。同样，Rich 和 Hill (1992) 使用基本表达式法在 MATLAB 中制定了他们的自动微分实现。</p><p>以公式为例子：</p><p>$$<br>f(x1,x2)&#x3D;ln(x1)+x1*x2−sin(x2)<br>\tag{1}<br>$$</p><p>用户首先需要手动将公式1中的各个操作，或者叫做子函数，分解为库函数中基本表达式组合：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = log(x)<br>t3 = sin(x)<br>t2 = x1 * x2<br>t4 = x1 + x2<br>t5 = x1 - x2<br></code></pre></td></tr></table></figure><p>使用给定的库函数，完成上述函数的程序设计：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">// 参数为变量 x,y,t 和对应的导数变量 dx,dy,dt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ADAdd</span>(<span class="hljs-params">x, y, dx, dy, t, dt</span>)<br><br>// 同理对上面的公式实现对应的函数<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ADSub</span>(<span class="hljs-params">x, y, dx, dy, t, dt</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ADMul</span>(<span class="hljs-params">x, y, dx, dy, t, dt</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ADLog</span>(<span class="hljs-params">x, dx, t, dt</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ADSin</span>(<span class="hljs-params">x, dx, t, dt</span>)<br></code></pre></td></tr></table></figure><p>而库函数中则定义了对应表达式的数学微分规则，和对应的链式法则:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">// 参数为变量 x,y,t 和对应的导数变量 dx,dy,dt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ADAdd</span>(<span class="hljs-params">x, y, dx, dy, t, dt</span>):<br>    t = x + y<br>    dt = dy + dx<br><br>// 参数为变量 x,y,t 和对应的导数变量 dx,dy,dt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ADSub</span>(<span class="hljs-params">x, y, dx, dy, t, dt</span>):<br>    t = x - y<br>    dt = dy - dx<br><br>// ... 以此类推<br></code></pre></td></tr></table></figure><p>针对公式1中基本表达式法，可以按照下面示例代码来实现正向的推理功能，反向其实也是一样，不过调用代码更复杂一点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">x1 = xxx<br>x2 = xxx<br>t1 = ADlog(x1)<br>t2 = ADSin(x2)<br>t3 = ADMul(x1, x2)<br>t4 = ADAdd(t1, t3)<br>t5 = ADSub(t4, t2)<br></code></pre></td></tr></table></figure><p>基本表达式法的<strong>优点</strong>可以总结如下：</p><ul><li>实现简单，基本可在任意语言中快速地实现为库</li></ul><p>基本表达式法的<strong>缺点</strong>可以总结如下：</p><ul><li>用户必须使用库函数进行编程，而无法使用语言原生的运算表达式；</li><li>另外实现逻辑和代码也会冗余较长，依赖于开发人员较强的数学背景</li></ul><p>基本表达式法在没有操作符重载AD的80到90年代初期，仍然是计算机中实现自动微分功能最简单和快捷的策略啦。</p><h2 id="操作符重载"><a href="#操作符重载" class="headerlink" title="操作符重载"></a>操作符重载</h2><p>在具有多态特性的现代编程语言中，运算符重载提供了实现自动微分的最直接方式，利用了编程语言的第一特性（first class feature），重新定义了微分基本操作语义的能力。</p><p>在 C++ 中使用运算符重载实现的流行工具是 ADOL-C（Walther 和 Griewank，2012）。 ADOL-C 要求对变量使用启用 AD 的类型，并在 Tape 数据结构中记录变量的算术运算，随后可以在反向模式 AD 计算期间“回放”。 Mxyzptlk 库 (Michelotti, 1990) 是 C++ 能够通过前向传播计算任意阶偏导数的另一个例子。 FADBAD++ 库（Bendtsen 和 Stauning，1996 年）使用模板和运算符重载为 C++ 实现自动微分。对于 Python 语言来说，autograd 提供正向和反向模式自动微分，支持高阶导数。</p><p>在机器学习 ML 或者深度学习 DL 领域，目前AI框架中使用操作符重载的一个典型代表是 Pytroch，其中使用数据结构 Tape 来记录计算流程，在反向模式求解梯度的过程中进行replay Operator。</p><ol><li>操作符重载来实现自动微分的功能里面，很重要的是利用高级语言的特性。下面简单看看伪代码，这里面我们定义一个特殊的数据结构 <code>Variable</code>，然后基于 <code>Variable</code> 重载一系列的操作如 <code>__mul__</code> 代替 * 操作。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Variable</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, value</span>):<br>        self.value = value<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__mul__</span>(<span class="hljs-params">self, other</span>):<br>        <span class="hljs-keyword">return</span> ops_mul(self, other)<br>        <br>   <span class="hljs-comment"># 同样重载各种不同的基础操作</span><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">__add__</span>(<span class="hljs-params">self, other</span>)<br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">__sub__</span>(<span class="hljs-params">self, other</span>)<br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">__div__</span>(<span class="hljs-params">self, other</span>)<br></code></pre></td></tr></table></figure><ol start="2"><li>实现操作符重载后的计算。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ops_mul</span>(<span class="hljs-params">self, other</span>):<br>    x = Variable(self.value * other.value)<br></code></pre></td></tr></table></figure><ol start="3"><li>接着通过一个 Tape 的数据结构，来记录每次 <code>Variable</code> 执行计算的顺序，Tape 这里面主要是记录正向的计算，把输入、输出和执行运算的操作符记录下来。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Tape</span>(<span class="hljs-title class_ inherited__">NamedTuple</span>):<br>    inputs : []<br>    outputs : []<br>    propagate : (inputs, outpus)<br></code></pre></td></tr></table></figure><ol start="4"><li>因为大部分 ML 系统或者 AI 框架采用的是反向模式，因此最后会逆向遍历 Tape 里面的数据（相当于反向传播或者反向模式的过程），然后累积反向计算的梯度。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 反向求导的过程，类似于 Pytroch 的 backward 接口</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">grad</span>(<span class="hljs-params">l, results</span>):<br><br><span class="hljs-comment"># 通过 reversed 操作把带有梯度信息的 tape 逆向遍历</span><br><span class="hljs-keyword">for</span> entry <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(gradient_tape):<br>    <span class="hljs-comment"># 进行梯度累积，反向传播给上一次的操作计算</span><br>    dl_d[<span class="hljs-built_in">input</span>] += dl_dinput<br></code></pre></td></tr></table></figure><p>当然啦，我们会在下一节当中带着大家亲自通过操作符重载实现一个前向的自动微分和后向的自动微分。下面总结一下操作符重载的一个基本流程：</p><ul><li>预定义了特定的数据结构，并对该数据结构重载了相应的基本运算操作符</li><li>程序在实际执行时会将相应表达式的操作类型和输入输出信息记录至特殊数据结构</li><li>得到特殊数据结构后，将对数据结构进行遍历并对其中记录的基本运算操作进行微分</li><li>把结果通过链式法则进行组合，完成自动微分</li></ul><p>操作符重载法的<strong>优点</strong>可以总结如下：</p><ul><li>实现简单，只要求语言提供多态的特性能力</li><li>易用性高，重载操作符后跟使用原生语言的编程方式类似</li></ul><p>操作符重载法的<strong>缺点</strong>可以总结如下：</p><ul><li>需要显式的构造特殊数据结构和对特殊数据结构进行大量读写、遍历操作，这些额外数据结构和操作的引入不利于高阶微分的实现</li><li>对于一些类似 if，while 等控制流表达式，难以通过操作符重载进行微分规则定义。对于这些操作的处理会退化成基本表达式方法中特定函数封装的方式，难以使用语言原生的控制流表达式</li></ul><h2 id="源代码转换"><a href="#源代码转换" class="headerlink" title="源代码转换"></a>源代码转换</h2><p>源码转换（Source Code Transformation，SCT）是最复杂的，实现起来也是非常具有挑战性。</p><p>源码转换的实现提供了对编程语言的扩展，可自动将算法分解为支持自动微分的基本操作。通常作为预处理器执行，以将扩展语言的输入转换为原始语言。简单来说就是利用源语言来实现领域扩展语言 DSL 的操作方式。</p><p>源代码转换的经典实例包括 Fortran 预处理器 GRESS（Horwedel 等人，1988 年）和 PADRE2（Kubo 和 Iri，1990 年），在编译之前将启用 AD 的 Fortran 变体转换为标准 Fortran。类似地，ADIFOR 工具 (Bischof et al., 1996) 给定一个 Fortran 源代码，生成一个增强代码，其中除了原始结果之外还计算所有指定的偏导数。对于以 ANSI C 编码的过程，ADIC 工具（Bischof 等人，1997）在指定因变量和自变量之后将 AD 实现为源代码转换。 Tapenade（Pascual 和 Hasco¨et，2008 年；Hasco¨et 和 Pascual，2013 年）是过去10年终 SCT 的流行工具，它为 Fortran 和 C 程序实现正向和反向模式 AD。 </p><p>除了通过源代码转换进行语言扩展外，还有一些实现通过专用编译器或解释器引入了具有紧密集成的 AD 功能的新语言。一些最早的 AD 工具，例如 SLANG (Adamson and Winant, 1969) 和 PROSE (Pfeiffer, 1987) 属于这一类。 NAGWare Fortran 编译器 (Naumann and Riehme, 2005) 是一个较新的示例，其中使用与 AD 相关的扩展会在编译时触发衍生代码的自动生成。</p><p>作为基于解释器的实现的一个例子，代数建模语言 AMPL (Fourer et al., 2002) 可以用数学符号表示目标和约束，系统从中推导出活动变量并安排必要的 AD 计算。此类别中的其他示例包括基于类似 Algol 的 DIFALG 语言的 FM&#x2F;FAD 包 (Mazourik, 1991)，以及类似于 Pascal 的面向对象的 COZY 语言 (Berz et al., 1996)。</p><p>而华为全场景AI框架 MindSpore 则是基于 Python 语言使用源代码转换实现 AD 的正反向模式，并采用了函数式编程的风格，该机制可以用控制流表示复杂的组合。函数被转换成函数中间表达（Intermediate Representation，IR），中间表达构造出一个能够在不同设备上解析和执行的计算图。在执行前，计算图上应用了多种软硬件协同优化技术，以提升端、边、云等不同场景下的性能和效率。</p><p>其主要流程是：分析获得源程序的 AST 表达形式；然后基于 AST 完成基本表达式的分解和微分操作；再通过遍历 AST 得到基本表达式间的依赖关系，从而应用链式法则完成自动微分。</p><p>因为源码转换涉及到底层的抽象语法树、编译执行等细节，因此这里就不给出伪代码了（实在太难了给不出来），我们通过下面这张图来简单了解下 SCT 的一般性过程。</p><p><img src="/images/ad/ast.png"></p><p>从图中可以看到源码转换的整体流程分为编译时间和执行时间，以 MindSpore 为例，其在运行之前的第一个 epoch 会等待一段时间，是因为需要对源码进行编译转换解析等一系列的操作。然后再 run time 运行时则会比较顺畅，直接对数据和代码不断地按照计算机指令来高速执行。</p><p>编译阶段呢，在 Initialization 过程中会对源码进行 Parse 转换成为抽象语法树 AST，接着转换为基于图表示的中间表达 IR，这个基于图的IR从概念上理解可以理解为计算图，神经网络层数的表示通过图表示会比较直观。</p><p>接着对 Graph base IR进行一些初级的类型推导，特别是针对 Tensor&#x2F;List&#x2F;Str 等不同的基础数据表示，然后进行宏展开，还有语言单态化，最后再对变量或者自变量进行类型推导。可以从图中看到，很多地方出现了不同形式的 IR，IR 其实是编译器中常用的一个中间表达概念，在编译的 Pass 中会有很多处理流程，每一步处理流程产生一个 IR，交给下一个Pass进行处理。</p><p>最后通过 LLVM 或者其他等不同的底层编译器，最后把 IR 编译成机器码，然后就可以真正地在runtime执行起来。</p><p>源码转换法的<strong>优点</strong>可以总结如下：</p><ul><li>支持更多的数据类型（原生和用户自定义的数据类型） + 原生语言操作（基本数学运算操作和控制流操作）</li><li>高阶微分中实现容易，不用每次使用 Tape 来记录高阶的微分中产生的大量变量，而是统一通过编译器进行额外变量优化和重计算等优化</li><li>进一步提升性能，没有产生额外的 tape 数据结构和 tape 读写操作，除了利于实现高阶微分以外，还能够对计算表达式进行统一的编译优化</li></ul><p>源码转换法的<strong>缺点</strong>可以总结如下：</p><ul><li>实现复杂，需要扩展语言的预处理器、编译器或解释器，深入计算机体系和底层编译</li><li>支持更多数据类型和操作，用户自由度虽然更高，但同时更容易写出不支持的代码导致错误</li><li>微分结果是以代码的形式存在，在执行计算的过程当中，特别是深度学习中大量使用for循环过程中间错误了，或者是数据处理流程中出现错误，并不利于深度调试</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] Automatic Differentiation in Machine Learning: a Survey: <a href="https://arxiv.org/abs/1502.05767">https://arxiv.org/abs/1502.05767</a></p><p>[2] Rirchard L. Burden and J. Douglas Faires. Numerical Analysis. Brooks&#x2F;Cole, 2001.</p><p>[3] Max E. Jerrell. Automatic differentiation and interval arithmetic for estimation of disequilibrium models. Computational Economics, 10(3):295–316, 1997.</p><p>[4] Johannes Grabmeier and Erich Kaltofen. Computer Algebra Handbook: Foundations, Applications, Systems. Springer, 2003.</p><p>[5] G. W. Leibniz. Machina arithmetica in qua non additio tantum et subtractio sed et multiplicatio nullo, diviso vero paene nullo animi labore peragantur. Hannover, 1685.</p><p>[6] George F. Corliss. Application of differentiation arithmetic, volume 19 of Perspectives in Computing, pages 127–48. Academic Press, Boston, 1988.</p><p>[7] Arun Verma. An introduction to automatic differentiation. Current Science, 78(7):804–7, 2000.</p><p>[8] Andreas Griewank and Andrea Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. Society for Industrial and Applied Mathematics, Philadelphia, 2008. doi: 10.1137&#x2F;1.9780898717761.</p><p>[9] John F. Nolan. Analytical differentiation on a digital computer. Master’s thesis, Massachusetts Institute of Technology, 1953.</p><p>[10] L. M. Beda, L. N. Korolev, N. V. Sukkikh, and T. S. Frolova. Programs for automatic differentiation for the machine BESM (in Russian). Technical report, Institute for Precise Mechanics and Computation Techniques, Academy of Science, Moscow, USSR, 1959.</p><p>[11] Robert E. Wengert. A simple automatic derivative evaluation program. Communications of the ACM, 7:463–4, 1964.</p><p>[12] Andreas Griewank. On automatic differentiation. pages 83–108, 1989.</p><p>[13] Hascoet, Laurent, and Valérie Pascual. “The Tapenade automatic differentiation tool: principles, model, and specification.” ACM Transactions on Mathematical Software (TOMS) 39.3 (2013): 1-43.</p><p>[14] 知乎专栏：自动微分（Automatic Differentiation）: <a href="https://zhuanlan.zhihu.com/p/61103504">https://zhuanlan.zhihu.com/p/61103504</a></p><p>[15] 知乎专栏：数值计算方法 第六章 数值积分和数值微分: <a href="https://zhuanlan.zhihu.com/p/14">https://zhuanlan.zhihu.com/p/14</a></p><p>[16] 知乎专栏：技术分享 | 从自动微分到可微编程语言设计 <a href="https://zhuanlan.zhihu.com/p/395725922">https://zhuanlan.zhihu.com/p/395725922</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>AutoGrad</tag>
      
      <tag>AI System</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动微分原理：AD的正反向模式</title>
    <link href="/2022/07/31/ad02/"/>
    <url>/2022/07/31/ad02/</url>
    
    <content type="html"><![CDATA[<p>写【自动微分】原理和实现系列文章，存粹是为了梳理在 MindSpore 当SE时候最核心的自动微分原理。网上看了很多文章，基本上都是很零散，当然Automatic Differentiation in Machine Learning: a Survey[1] 这篇文章是目前ZOMI觉得比较好关于自动微分的综述论文。</p><ul><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/518198564">01. 一文看懂AD原理</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/518296942">02. AD的正反向模式</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/520065656">03. AD常用实现方案</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/520065656">04. 正向OO实现自动微分</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/547865589/">05. 反向OO实现自动微分(Pytroch核心机制)</a></li></ul><h1 id="自动微分的两种模式"><a href="#自动微分的两种模式" class="headerlink" title="自动微分的两种模式"></a>自动微分的两种模式</h1><p>上一篇文章我们简单了解了计算机中常用的几种微分方式。</p><p>本章将深入介绍AI框架离不开的核心功能自动微分，而自动微分则是分为前向微分和后向微分两种实现模式，不同的实现模式有不同的机制和计算逻辑，而无论哪种模式都离不开雅克比矩阵，所以我们也会深入了解一下雅克比矩阵的原理。</p><h2 id="雅克比矩阵"><a href="#雅克比矩阵" class="headerlink" title="雅克比矩阵"></a>雅克比矩阵</h2><p>在向量微积分中，Jacobian矩阵是一阶偏导数以一定方式排列成的矩阵，其行列式称为Jacobian行列式。Jacobian矩阵的重要性在于它体现了一个可微方程与给出点的最优线性逼近。</p><p>Jacobian矩阵表示两个向量所有可能的偏导数。它是一个向量相对于另一个向量的梯度，其实现的是 n维向量 到 m 维向量的映射。</p><p>在矢量运算中，Jacobian矩阵是基于函数对所有变量一阶偏导数的数值矩阵，当输入个数等于输出个数时又称为Jacobian行列式。</p><p>假设输入向量 𝑥∈𝑅𝑛，而输出向量 𝑦∈𝑅𝑚，则Jacobian矩阵定义为：</p><p>$$<br>J_f&#x3D;<br>\left[<br> \begin{matrix}<br>   \dfrac{\delta y_1}{\delta x_1} &amp; \cdots &amp; \dfrac{\delta y_1}{\delta x_n} \<br>   \vdots &amp; \ddots &amp; \vdots \<br>   \dfrac{\delta y_m}{\delta x_1} &amp; \vdots &amp; \dfrac{\delta y_m}{\delta x_n}<br>  \end{matrix}<br>  \right]<br>\tag{1}<br>$$</p><h2 id="微分计算模式"><a href="#微分计算模式" class="headerlink" title="微分计算模式"></a><strong>微分计算模式</strong></h2><p>根据对分解后的基本操作求导和链式规则组合顺序的不同，自动微分可以分为两种模式：</p><ul><li><em>前向模式</em>（Forward Automatic Differentiation，也叫做 tangent mode AD）或者前向累积梯度（前向模式）</li><li><em>反向模式</em>（Reverse Automatic Differentiation，也叫做 adjoint mode AD）或者说反向累计梯度（反向模式）。</li></ul><h3 id="计算模式区别"><a href="#计算模式区别" class="headerlink" title="计算模式区别"></a>计算模式区别</h3><p>两种自动微分模式都通过递归方式来求 dy&#x2F;dx，只不过根据链式法则展开的形式不太一样。</p><p>前向梯度累积会指定从内到外的链式法则遍历路径，即先计算 dw_1&#x2F;dx，再计算 dw_2&#x2F;dw_1，最后计算 dy&#x2F;dw_2，即，前向模式是在计算图前向传播的同时计算微分。因此前向模式的一次正向传播就可以计算出输出值和导数值。</p><p>$$<br>\frac{dw_i}{dx}&#x3D;\frac{dw_i}{dw_{i-1}}\frac{dw_{i-1}}{dx}<br>\tag{2}<br>$$</p><p>反向梯度累积正好相反，它会先计算dy&#x2F;dw_2，然后计算 dw_2&#x2F;dw_1，最后计算dw_1&#x2F;dx。这是最为熟悉的反向传播模式，它非常符合<strong>沿模型误差反向传播</strong>这一直观思路。</p><p>即，反向模式需要对计算图进行一次正向计算， 得出输出值，再进行反向传播。反向模式需要保存正向传播的中间变量值（比如𝑤𝑖），这些中间变量数值在反向传播时候被用来计算导数，所以反向模式的内存开销要大。</p><p>$$<br>\frac{dy}{dw_i}&#x3D;\frac{dy}{dw_{i+1}}\frac{dw_{i+1}}{dw_i}<br>\tag{3}<br>$$</p><p>即如图所示，前向自动微分（tangent mode AD）和后向自动微分（adjoint mode AD）分别计算了Jacobian矩阵的一列和一行。</p><p><img src="/images/ad/tangent_adjoint.png"></p><p>前向自动微分（tangent mode AD）可以在一次程序计算中通过链式法则，得到：</p><p>$$<br>\frac{\delta x^k}{\delta x^0_j}&#x3D;<br>\frac{\delta x^k}{\delta x^{k-1}}<br>\frac{\delta ^{k-1}}{\delta x^0_j}<br>\tag{4}<br>$$</p><p>递推得到Jacobian矩阵中与单个输入有关的参数，即Jacobian矩阵的一列。</p><p>后向自动微分（adjoint mode AD）利用链式法则，得到：</p><p>$$<br>\frac{\delta x^L_i}{\delta x^k}&#x3D;<br>\frac{\delta x^L_i}{\delta x^{k+1}}<br>\frac{\delta ^{k+1}}{\delta x^k}<br>\tag{5}<br>$$</p><p>可以仅通过一次对计算过程的遍历得到Jacobian矩阵的一行。但它的导数链式法则传递方向和程序执行方向相反，所以需要在程序计算过程中记录一些额外的信息来辅助求导，这些辅助信息包括计算图和计算过程的中间变量。</p><h3 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h3><p>我们以公式为例，首先把它转换成一个计算图：</p><p>$$<br>f(x1,x2)&#x3D;ln(x1)+x1x2−sin(x2)<br>\tag{5}<br>$$</p><ul><li>输入变量 ：自变量维度为 n，这里 n &#x3D; 2，输入变量就是 x1, x2</li><li>中间变量 ：中间变量这里是 v-1到 v5，在计算过程中，只需要针对这些中间变量做处理即可。将符号微分法应用于最基本的算子，然后代入数值，保留中间结果，最后再应用于整个函数</li><li>输出变量 ：假设输出变量维度为 m，这里 m &#x3D; 1，输出变量就是 y1，也就是f(x1,x2)</li></ul><p><img src="/images/ad/forward%20mode.png"></p><p>转化成如上DAG（有向无环图）结构之后，我们可以很容易分步计算函数的值，并求取它每一步的导数值，然后，我们把 df&#x2F;dx_1 求导过程利用链式法则表示成如下的形式：</p><p>$$<br>\dfrac{df}{dx_1}&#x3D;<br>\dfrac{dv_{-1}}{dx_1} \cdot<br>(\dfrac{dv_{1}}{dv_{-1}} \cdot \dfrac{dv_{4}}{dv_{1}} +<br>\dfrac{dv_{2}}{dv_{-1}} \cdot \dfrac{dv_{4}}{dx_{2}}) \cdot<br>\dfrac{dv_{5}}{dv_{4}} \cdot \dfrac{df}{dv_{5}}<br>\tag{6}<br>$$</p><blockquote><p>整个求导可以被拆成一系列微分算子的组合。</p></blockquote><h2 id="前向模式-Foward-Mode"><a href="#前向模式-Foward-Mode" class="headerlink" title="前向模式 Foward Mode"></a>前向模式 Foward Mode</h2><p>前向模式从计算图的起点开始，沿着计算图边的方向依次向前计算，最终到达计算图的终点。它根据自变量的值计算出计算图中每个节点的值 以及其导数值，并保留中间结果。一直得到整个函数的值和其导数值。整个过程对应于一元复合函数求导时从最内层逐步向外层求导。</p><p>同样，以公式为例子：</p><p>$$<br>f(x1,x2)&#x3D;ln(x1)+x1x2−sin(x2)<br>\tag{7}<br>$$</p><p>面是前向模式的计算过程，下表中，左半部分是从左往右每个图节点的求值结果和计算过程，右半部分是每个节点对 x1的求导结果和计算过程。这里 𝑉˙𝑖 表示 𝑉𝑖 对 𝑥1的偏导数。即：</p><p>$$<br>\dot{v_i}&#x3D;\dfrac{\delta v_i}{\delta x_1}<br>\tag{8}<br>$$</p><p>在该示例中，我们希望计算函数在x_1&#x3D;2, x_2&#x3D;5处的导数dy&#x2F;dx1，即：</p><p>$$<br>\dot{y_j}&#x3D;\dfrac{\delta y_j}{\delta x_i}<br>\tag{9}<br>$$</p><p>可以看出，左侧是源程序分解后得到的基本操作集合，而右侧则是每一个基本操作根据已知的求导规则和链式法则由上至下计算的求导结果。<img src="/images/ad/WX20220522-180607@2x.png"></p><h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><p>根据上图左边的Forward Primal Trace直接计算公式，对于节点数值的计算如下：</p><ol><li>我们给输入节点赋值，$v_{−1}&#x3D;x_1&#x3D;2，v_0&#x3D;x_2&#x3D;5$</li><li>计算 $v_1$ 节点，$v_1&#x3D;lnv_{−1}&#x3D;lnx_1&#x3D;ln2$</li><li>计算 $v_2$ 节点，节点 $v_2$ 依赖于 $v_{-1}$ 和 $v_0$，$v_2&#x3D;10$</li><li>计算 $v_3$ 节点，$v_3&#x3D;sinv_0&#x3D;sin5$</li><li>计算 $v_4$ 节点，$v_4&#x3D;v_1+v_2&#x3D;0.693+10$</li><li>计算 $v_5$ 节点，$v_5&#x3D;v_1+v_2&#x3D;10.693+0.959$</li><li>最终 $y&#x3D;v_5&#x3D;11.652$</li></ol><p>此时，已经得到了图中所有节点的数值。自动微分正向模式中（上图右边Forward Tangent Trace），在计算节点数值的同时，也一起计算导数，假设求 $\delta y&#x2F; \delta x_1$，则是从输入开始计算。</p><ol><li>计算 $v_{-1}$ 节点对于 $x_1$ 的梯度：$v_{-1}&#x3D;x_1$，所以 $\delta v_{-1}&#x2F; \delta x_1&#x3D;1$</li><li>计算 $v_0$ 节点对于 $x_1$ 的梯度：$v_0&#x3D;x_2$，所以 $\delta v_0&#x2F; \delta x_1&#x3D;0$</li><li>计算 $v_{1}$ 节点对于 $x_1$ 的梯度：$\delta v_1&#x2F; \delta x_1&#x3D;0.5$</li><li>计算 $v_{2}$ 节点对于 $x_1$ 的梯度：$\delta v_{2}&#x2F; \delta x_1&#x3D;(\delta v_{-1}&#x2F; \delta x_1)v_0+(\delta v_{0}&#x2F; \delta x_1)v_{-1}&#x3D;5$</li><li>计算 $v_{3}$ 节点对于 $x_1$ 的梯度：$\delta v_{3}&#x2F; \delta x_1&#x3D;(\delta v_{0}&#x2F; \delta x_1)cosv_0&#x3D;0$</li><li>计算 $v_{4}$ 节点对于 $x_1$ 的梯度：$\delta v_{4}&#x2F; \delta x_1&#x3D;\delta v_{1}&#x2F; \delta x_1+\delta v_{2}&#x2F; \delta x_1&#x3D;0.5+5$</li><li>计算 $v_{5}$ 节点对于 $x_1$ 的梯度：$\delta v_{5}&#x2F; \delta x_1&#x3D;5.5&#x3D;0$</li><li>因此，得到 $\delta y&#x2F; \delta x_1&#x3D;\delta v_{5}&#x2F; \delta x_1&#x3D;5.5$</li></ol><p>从计算过程来看啦，自动微分的前向模式实际上与我们在微积分里所学的求导过程一致。</p><h3 id="雅克比-向量矩阵"><a href="#雅克比-向量矩阵" class="headerlink" title="雅克比-向量矩阵"></a>雅克比-向量矩阵</h3><p>把上述过程当做雅克比矩阵求解问题，假设一个函数有 n 个输入变量 $x_i$，m个输入变量 $y_j$，即输入向量 $x \in R^n, y \in R^m$，则这个的映射是：</p><p>$$<br>f:R^n \to R^m \tag{10}<br>$$</p><p>在这种情况下，每个自动微分的前向传播计算时候，初始输入被设置为 $\dot{x_i}&#x3D;1$，其余被设置为 0。对应Jacobian矩阵定义为：</p><p>$$<br>J_f&#x3D;<br>\left[<br> \begin{matrix}<br>   \dfrac{\delta y_1}{\delta x_1} &amp; \cdots &amp; \dfrac{\delta y_1}{\delta x_n} \<br>   \vdots &amp; \ddots &amp; \vdots \<br>   \dfrac{\delta y_m}{\delta x_1} &amp; \vdots &amp; \dfrac{\delta y_m}{\delta x_n}<br>  \end{matrix}<br>  \right]<br>\tag{11}<br>$$</p><p>一次前向计算，可以求出Jacobian矩阵的一列数据，如 $\dot{x_3}&#x3D;1$ 对应就可以求出来第3列。tangent mode AD可以在一次程序计算中，通过链式法则递推得到Jacobian矩阵中与单个输入有关的部分，即Jacobian矩阵的一列。</p><p>如下图所示，如果想用正向模式求对所有输入的导数，需要计算 n 次才能求出所有列。</p><p><img src="/images/ad/tangent_adjoint.png"></p><p>进一步，设置 $\dot{x}&#x3D;r$，可以在一次前向传播中直接计算 Jacobian–vector 乘积：</p><p>$$<br>J_f \cdot r&#x3D;<br>\left[<br> \begin{matrix}<br>   \dfrac{\delta y_1}{\delta x_1} &amp; \cdots &amp; \dfrac{\delta y_1}{\delta x_n} \<br>   \vdots &amp; \ddots &amp; \vdots \<br>   \dfrac{\delta y_m}{\delta x_1} &amp; \vdots &amp; \dfrac{\delta y_m}{\delta x_n}<br>  \end{matrix}<br>  \right]<br>\left[<br>\begin{matrix}<br>    r_1 \<br>    \vdots \<br>     r_n \<br>  \end{matrix}<br>  \right]<br>\tag{12}<br>$$</p><p>最终我们可以递归的得到本次迭代的计算目标：雅克比矩阵中的第 i 行。</p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>前向模式的优点：</p><ul><li>实现起来很简单</li><li>也不需要很多额外的内存空间</li></ul><p>向前模式的缺点：</p><ul><li>每次前向计算只能计算对一个自变量的偏导数，对于一元函数求导是高效的，但是机器学习模型的自参数（入参）数量级大。</li><li>如果有一个函数，其输入有 n 个，输出有 m个，对于每个输入来说，前向模式都需要遍历计算过程以得到当前输入的导数，求解整个函数梯度需要 n 遍如上计算过程。</li></ul><h2 id="反向模式-Reverse-Mode"><a href="#反向模式-Reverse-Mode" class="headerlink" title="反向模式 Reverse Mode"></a>反向模式 Reverse Mode</h2><p>反向自动微分同样是基于链式法则。仅需要一个前向过程和反向过程，就可以计算所有参数的导数或者梯度。</p><p>因为需要结合前向和后向两个过程，因此反向自动微分会使用一个特殊的数据结构，来存储计算过程。</p><p>而这个特殊的数据结构例如Tensorflow或者MindSpore，则是把所有的操作以一张图的方式存储下来，这张图可以是一个有向无环（DAG）的计算图；而Pytroch则是使用Tape来记录每一个操作，他们都表达了函数和变量的关系。</p><p>反向模式根据从后向前计算，依次得到对每个中间变量节点的偏导数，直到到达自变量节点处，这样就得到了每个输入的偏导数。在每个节点处，根据该节点的后续节点（前向传播中的后续节点）计算其导数值。</p><p>整个过程对应于多元复合函数求导时从最外层逐步向内侧求导。这样可以有效地把各个节点的梯度计算解耦开，每次只需要关注计算图中当前节点的梯度计算。</p><p>从下图可以看出来，reverse mode和forward mode是一对相反过程，reverse mode从最终结果开始求导，利用最终输出对每一个节点进行求导。下图虚线就是反向模式。</p><p>![](images&#x2F;ad&#x2F;reversed mode.png)</p><h3 id="计算过程-1"><a href="#计算过程-1" class="headerlink" title="计算过程"></a>计算过程</h3><p>前向和后向两种模式的过程表达如下，表的左列浅色为前向计算函数值的过程，与前向计算时相同，右面列深色为反向计算导数值的过程。</p><p>反向模式的计算过程如图所示，其中：</p><p>$$<br>\overline{v_i}&#x3D;\dfrac{\delta y}{\delta v_i}<br>$$</p><p>根据链式求导法则展开有：</p><p>$$<br>\frac{\partial f}{\partial x}&#x3D;\sum_{k&#x3D;1}^{N} \frac{\partial f}{\partial v_{k}} \frac{\partial v_{k}}{\partial \boldsymbol{x}}<br>$$</p><p>可以看出，左侧是源程序分解后得到的基本操作集合，而右侧则是每一个基本操作根据已知的求导规则和链式法则<strong>由下至上</strong>计算的求导结果。</p><p><img src="/images/ad/WX20220522-180618@2x.png"></p><ol><li>计算 $y$ 对 $v_5$ 的导数值，即 $\overline{v}_5&#x3D;\overline{y}&#x3D;1$</li><li>计算 y 对 $v_4$ 的导数值，$\overline{v}_4&#x3D;\overline{v}_5\frac{\delta{v_5}}{\delta{v_4}}&#x3D;1$</li><li>计算 y 对 $v_3$ 的导数值，$\overline{v}_3&#x3D;\overline{v}_5\frac{\delta{v_5}}{\delta{v_3}}&#x3D;-1$</li><li>计算 y 对 $v_1$ 的导数值，$\overline{v}_1&#x3D;\overline{v}_4\frac{\delta{v_4}}{\delta{v_1}}&#x3D;1$</li><li>计算 y 对 $v_2$ 的导数值，$\overline{v}_2&#x3D;\overline{v}_4\frac{\delta{v_4}}{\delta{v_1}}&#x3D;1$</li><li>接下来要计算 y 对 $v_0$ 的导数值和 y 对 $v_{-1}$ 的导数值，因为 $v_0$ 和 $v_{-1}$ 都是后续有两个节点，因此需要分开计算。</li><li>计算 $\frac{\delta{v_3}}{\delta{v_0}}&#x3D;cosv_0&#x3D;0.284$</li><li>计算 $\frac{\delta{v_2}}{\delta{v_0}}&#x3D;v_{-1}&#x3D;2$</li><li>计算 $\frac{\delta{v_2}}{\delta{v_{-1}}}&#x3D;v_0&#x3D;5$</li><li>计算 $\frac{\delta{v_1}}{\delta{v_{-1}}}&#x3D;\frac{1}{x_1}&#x3D;0.5$</li></ol><p>到目前为止，我们已经计算出来了所有步骤的偏导数的数值。现在需要计算 $\overline{v}_1$ 和 $\overline{v}_2$ 。计算 $\overline{v}_1$ 从最后的位置往前到自变量 x_1，有多条路径，需要将这个路径上的数值连乘起来得到一个乘积数值，然后将这多条路径的乘积数值相加起来得到最后的结果。</p><p>从 y 到 x_1 的路径有两条，分别是：</p><ol><li><p>$v_5 \to v_4 \to v_1 \to v_{-1}$，其数值乘积是 1∗1∗0.5&#x3D;0.5</p></li><li><p>$v_5 \to v_4 \to v_2 \to v_{-1}$，其数值乘积是 1∗1∗ 5&#x3D; 5</p></li></ol><p>因此，$\overline{v}_1&#x3D;0.5+5&#x3D;5.5$，同理有 $\overline{v}_2&#x3D;2.0-0.284&#x3D;1.716$</p><h3 id="向量-雅克比矩阵"><a href="#向量-雅克比矩阵" class="headerlink" title="向量-雅克比矩阵"></a>向量-雅克比矩阵</h3><p>对于函数 $\overline{y}&#x3D;f(\overline{x})$，其中 $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$，那么 $\overline{y}$ 中关于 $\overline{x}$ 的梯度可以表示为 Jacobian 矩阵：</p><p>$$<br>J_{f}&#x3D;<br>\left[\begin{array}{ccc}<br>\dfrac{\partial y}{\partial x_{1}} &amp; \cdots &amp; \dfrac{\partial y}{\partial x_{1}}<br>\end{array}\right]&#x3D;<br>\left[\begin{array}{ccc}<br>\dfrac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \dfrac{\partial y_{1}}{\partial x_{n}} \<br>\vdots &amp; \ddots &amp; \vdots \<br>\dfrac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \dfrac{\partial y_{m}}{\partial x_{n}}<br>\end{array}\right]<br>$$</p><p>设置 $\overline{v}$ 是关于函数 $l&#x3D;g(\overline{y})$ 的梯度：</p><p>$$<br>\overline{v}&#x3D;<br>\left[\begin{array}{lll}<br>\dfrac{\partial l}{\partial y_{1}} &amp;<br>\cdots &amp;<br>\dfrac{\partial l}{\partial y_{m}}<br>\end{array}\right]^{\mathrm{T}}<br>$$</p><p>Jacobian-vector 积就是函数 l 中关于 x_1 的梯度：</p><p>$$<br>\boldsymbol{J} \cdot \overline{v}&#x3D;<br>\left[\begin{array}{ccc}<br>\dfrac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp;<br>\dfrac{\partial y_{1}}{\partial x_{n}} \<br>\vdots &amp; \ddots &amp; \vdots \<br>\dfrac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp;<br>\dfrac{\partial y_{m}}{\partial x_{n}}<br>\end{array}\right] \cdot\left[\begin{array}{c}<br>\dfrac{\partial l}{\partial y_{1}} \<br>\vdots \<br>\dfrac{\partial l}{\partial y_{m}}<br>\end{array}\right]&#x3D;\left[\begin{array}{c}<br>\dfrac{\partial y_{1}}{\partial x_{1}} \<br>\vdots \<br>\dfrac{\partial y_{m}}{\partial x_{1}}<br>\end{array}\right]<br>$$</p><p>即通过雅克比矩阵转置与后续节点梯度值的乘积，可以得到当前节点的梯度值。</p><h3 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h3><p>前向模式在计算之中，计算图各个节点的数值和该节点的导数可同步求出，但是代价就是对于多个输入需要多次计算才行。</p><p>反向模式的优点：</p><ul><li><p>通过一次反向传输，就计算出所有偏导数，中间的偏导数计算只需计算一次</p></li><li><p>减少了重复计算的工作量，在多参数的时候后向自动微分的时间复杂度更低</p></li></ul><p>反向模式的缺点：</p><ul><li><p>需要额外的数据结构记录正向过程的计算操作，用于反向使用</p></li><li><p>带来了大量内存占用，为了减少内存操作，需要深度学习框架进行各种优化，也带来了额外限制和副作用</p></li></ul><h2 id="正反向模式的比较"><a href="#正反向模式的比较" class="headerlink" title="正反向模式的比较"></a>正反向模式的比较</h2><p>前向自动微分（tangent mode AD和后向自动微分（adjoint mode AD）分别计算了Jacobian矩阵的一列和一行。</p><p><img src="/images/ad/tangent_adjoint.png"></p><p>前向模式和反向模式的不同之处在于矩阵相乘的起始之处不同。</p><p>当输出维度小于输入维度，反向模式的乘法次数要小于前向模式。因此，当输出的维度大于输入的时候，适宜使用前向模式微分；当输出维度远远小于输入的时候，适宜使用反向模式微分。</p><p>即，后向自动微分更加适合多参数的情况，多参数的时候后向自动微分的时间复杂度更低，只需要一遍reverse mode的计算过程，便可以求出输出对于各个输入的导数，从而轻松求取梯度用于后续优化更新。</p><p>因此，目前大部分AI框架都会优先采用反向模式，但是也有例如MindSpore等AI框架同事支持正反向的实现模式。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] Automatic Differentiation in Machine Learning: a Survey: <a href="https://arxiv.org/abs/1502.05767">https://arxiv.org/abs/1502.05767</a></p><p>[2] Rirchard L. Burden and J. Douglas Faires. Numerical Analysis. Brooks&#x2F;Cole, 2001.</p><p>[3] Max E. Jerrell. Automatic differentiation and interval arithmetic for estimation of disequilibrium models. Computational Economics, 10(3):295–316, 1997.</p><p>[4] Johannes Grabmeier and Erich Kaltofen. Computer Algebra Handbook: Foundations, Applications, Systems. Springer, 2003.</p><p>[5] G. W. Leibniz. Machina arithmetica in qua non additio tantum et subtractio sed et multiplicatio nullo, diviso vero paene nullo animi labore peragantur. Hannover, 1685.</p><p>[6] George F. Corliss. Application of differentiation arithmetic, volume 19 of Perspectives in Computing, pages 127–48. Academic Press, Boston, 1988.</p><p>[7] Arun Verma. An introduction to automatic differentiation. Current Science, 78(7):804–7, 2000.</p><p>[8] Andreas Griewank and Andrea Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. Society for Industrial and Applied Mathematics, Philadelphia, 2008. doi: 10.1137&#x2F;1.9780898717761.</p><p>[9] John F. Nolan. Analytical differentiation on a digital computer. Master’s thesis, Massachusetts Institute of Technology, 1953.</p><p>[10] L. M. Beda, L. N. Korolev, N. V. Sukkikh, and T. S. Frolova. Programs for automatic differentiation for the machine BESM (in Russian). Technical report, Institute for Precise Mechanics and Computation Techniques, Academy of Science, Moscow, USSR, 1959.</p><p>[11] Robert E. Wengert. A simple automatic derivative evaluation program. Communications of the ACM, 7:463–4, 1964.</p><p>[12] Andreas Griewank. On automatic differentiation. pages 83–108, 1989.</p><p>[13] Hascoet, Laurent, and Valérie Pascual. “The Tapenade automatic differentiation tool: principles, model, and specification.” ACM Transactions on Mathematical Software (TOMS) 39.3 (2013): 1-43.</p><p>[14] 知乎专栏：自动微分（Automatic Differentiation）: <a href="https://zhuanlan.zhihu.com/p/61103504">https://zhuanlan.zhihu.com/p/61103504</a></p><p>[15] 知乎专栏：数值计算方法 第六章 数值积分和数值微分: <a href="https://zhuanlan.zhihu.com/p/14">https://zhuanlan.zhihu.com/p/14</a></p><p>[16] 知乎专栏：技术分享 | 从自动微分到可微编程语言设计 <a href="https://zhuanlan.zhihu.com/p/393160344">https://zhuanlan.zhihu.com/p/393160344</a></p><p>[17] 博客园：深度学习利器之自动微分 <a href="https://www.cnblogs.com/rossiXYZ/p/15395742.html">https://www.cnblogs.com/rossiXYZ/p/15395742.html</a><br>](images&#x2F;ad&#x2F;</p>]]></content>
    
    
    
    <tags>
      
      <tag>AutoGrad</tag>
      
      <tag>AI System</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动微分原理：一文看懂AD原理</title>
    <link href="/2022/07/31/ad01/"/>
    <url>/2022/07/31/ad01/</url>
    
    <content type="html"><![CDATA[<p>写【自动微分】原理和实现系列文章，存粹是为了梳理在 MindSpore 当SE时候最核心的自动微分原理。网上看了很多文章，基本上都是很零散，当然Automatic Differentiation in Machine Learning: a Survey[1] 这篇文章是目前ZOMI觉得比较好关于自动微分的综述论文。</p><ul><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/518198564">01. 一文看懂AD原理</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/518296942">02. AD的正反向模式</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/520065656">03. AD常用实现方案</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/520065656">04. 正向OO实现自动微分</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/547865589/">05. 反向OO实现自动微分(Pytroch核心机制)</a></li></ul><h1 id="自动微分原理"><a href="#自动微分原理" class="headerlink" title="自动微分原理"></a>自动微分原理</h1><p><strong>自动微分</strong>（Automatic Differentiation，AD）是一种对计算机程序进行高效准确求导的技术，一直被广泛应用于计算流体力学、大气科学、工业设计仿真优化等领域。而近年来，机器学习技术的兴起也驱动着对自动微分技术的研究进入一个新的阶段。随着自动微分和其他微分技术研究的深入，其与编程语言、计算框架、编译器等领域的联系愈发紧密，从而衍生扩展出更通用的<strong>可微编程</strong>概念。</p><p>本章将从常见的微分方法开始介绍，然后深入自动微分基本概念。</p><h2 id="常见计算机求导方法"><a href="#常见计算机求导方法" class="headerlink" title="常见计算机求导方法"></a><strong>常见计算机求导方法</strong></h2><p>对计算机程序求导的方法可以归纳为以下四种：</p><ul><li><strong>手动求解法(Manual Differentiation)</strong> ：完全手动完成，手工求导并编写对应的结果程序，依据链式法则解出梯度公式，带入数值，得到梯度。</li><li>**数值微分法(Numerical Differentiation)**：利用导数的原始定义，通过有限差分近似方法完成求导，直接求解微分值。</li><li>**符号微分法(Symbolic Differentiation)**：基于数学规则和程序表达式变换完成求导。利用求导规则对表达式进行自动计算，其计算结果是导函数的表达式而非具体的数值。即，先求解析解，然后转换为程序，再通过程序计算出函数的梯度。</li><li>**自动微分法(Automatic Differentiation)**：介于数值微分和符号微分之间的方法，采用类似有向图的计算来求解微分值，介于数值微分和符号微分之间的一种求导方法，也是本文介绍的重点。</li></ul><p><img src="/images/ad/WX20220522-143400@2x.png"></p><h2 id="手动微分"><a href="#手动微分" class="headerlink" title="手动微分"></a>手动微分</h2><p>手动微分就是对每一个目标函数都需要利用求导公式手动写出求导公式，然后依照公式编写代码，带入数值，求出最终梯度。</p><p>这种方法准确有效，但是不适合工程实现，因为通用性和灵活性很差，每一次我们修改算法模型，都要修改对应的梯度求解算法。如果模型复杂或者项目频繁反复迭代，那么工作量将会是巨大的。</p><h2 id="数值微分"><a href="#数值微分" class="headerlink" title="数值微分"></a><strong>数值微分</strong></h2><p>数值微分方式应该是最直接而且简单的一种自动求导方式，使用差分近似方法完成，其本质是根据导数的定义推导而来。</p><p>$$<br>f’(x)&#x3D;lim_{h \to 0}\frac{f(x+h)-f(x)}{h} \tag{1}<br>$$</p><p>当 $h$ 取很小的数值，比如 0.000001 时，导数是可以利用差分来近似计算出来的。只需要给出函数值以及自变量的差值，数值微分算法就可计算出导数值。单侧差分公式根据导数的定义直接近似计算某一点处的导数值。</p><p>观察导数的定义容易想到，当 $h$ 充分小时，可以用差商 $\frac{f(x+h)-f(x)}{h}$ 近似导数结果。而近似的一部分误差（<em>截断误差</em>，Truncation Error）可以由泰勒公式中的二阶及二阶后的所有余项给出：</p><p>$$<br>f(x \pm h)&#x3D;f(x)\pm hf’(x)+\frac{h^2}{2!}f’’(x) \pm \frac{h^3}{3!}f’’(x) + … + (\pm h)^n n!f^{(n)}(x) \tag{2}<br>$$</p><p>因此数值微分中常用的三种计算方式及其对应的截断误差可以归纳为三种。</p><ul><li><strong>向前差商</strong>（Forward Difference）：</li></ul><p>$$<br>\frac{\delta f(x)}{\delta x} \approx \frac{f(x+h)-f(x)}{h} \tag{3}<br>$$</p><p>    其中Forward Difference的阶段误差为$O(h)$。</p><ul><li><strong>向后差商</strong>（Reverse Difference）：</li></ul><p>$$<br>\frac{\delta f(x)}{\delta x} \approx \frac{f(x)-f(x-h)}{h} \tag{4}<br>$$</p><p>    其中Reverse Difference的阶段误差为$O(h)$。</p><ul><li><strong>中心差商</strong>（Center Difference）</li></ul><p>$$<br>\frac{\delta f(x)}{\delta x} \approx \frac{f(x+h)-f(x-h)}{2h} \tag{5}<br>$$</p><p>    其中Center Difference的阶段误差为 $O(h^2)$。</p><p>可以看出来，数值微分中的截断误差与步长 $h$ 有关，$h$ 越小则截断误差越小，近似程序越高。</p><p>但实际情况数值微分的精确度并不会随着 $h$ 的减小而无限减小，因为计算机系统中对于浮点数的运算由于其表达方式存在另外一种误差（<em>舍入误差</em>，Round-off Error），而舍入误差则会随着 $h$ 变小而逐渐增大。因此在截断误差和舍入误差的共同作用下，数值微分的精度将会形成一个变化的函数并在某一个 $h$值处达到最小值。</p><p>为了缓解截断错误，提出了中心微分近似（Center Difference Approximation），这方法仍然无法解决舍入误差，只是减少误差，但是它比单侧差分公式有更小的误差和更好的稳定性：</p><p>$$<br>\frac{\delta f(x)}{\delta x} \approx \frac{f(x+h)-f(x-h)}{2h}+O(h^2) \tag{6}<br>$$</p><p><img src="/images/ad/WX20220522-143421@2x.png"></p><p>数值微分的<mark>优点</mark>是：</p><ul><li><p>具有计算适用性，对大部分表达式适用</p></li><li><p>对用于显示地隐藏了求导过程</p></li><li><p>简单容易实现</p></li></ul><p>数值微分的<mark>缺点</mark>是：</p><ul><li>计算量大，求解速度最慢，因为每计算一个参数的导数，都需要重新计算。</li><li>引入误差，因为是数值逼近，所有会不可靠，不稳定的情况，无法获得一个相对准确的导数值。如果 h 选取不当，可能会得到与符号相反的结果，导致误差增大。尤其是两个严重问题：<ul><li>截断错误（Truncation error）：在数值计算中 h 无法真正取零导致的近似误差。</li><li>舍入误差（Round-off Error）：在计算过程中出现的对小数位数的不断舍入会导致求导过程中的误差不断累积。</li></ul></li></ul><h2 id="符号微分"><a href="#符号微分" class="headerlink" title="符号微分"></a><strong>符号微分</strong></h2><p>符号微分（Symbolic Differentiation）属符号计算的范畴，利用求导规则对表达式进行自动计算，其计算结果是导函数的表达式。符号计算用于求解数学中的公式解，得到的是解的表达式而非具体的数值。</p><p>符号微分适合符号表达式的自动求导，符号微分的原理是用下面的简单求导规则，对计算机程序中的表达式进行递归变换来完成求导替代手动微分：</p><p>$$<br>\frac{\delta}{\delta x}((f(x)+g(x))&#x3D;\frac{\delta}{\delta x}f(x)+\frac{\delta}{\delta x}g(x) \tag{7}<br>$$</p><p>另外有：</p><p>$$<br>\frac{\delta}{\delta x}(f(x)g(x))&#x3D;(\frac{\delta}{\delta x}f(x))g(x)+f(x)(\frac{\delta}{\delta x}g(x)) \tag{8}<br>$$</p><p>由于变换过程中并不涉及计算且是严格等价，因此其可以大大减小微分结果的误差（仅存在变换完成后计算过程中的舍入误差）。除此之外，符号微分的计算方式使其还能用于类似极值 $\frac{\delta}{\delta x}f(x)&#x3D;0$ 的数学问题求解。</p><p>从某种角度看，这种递归思想和严格的程序变换让符号微分看上去是一种“完美”的计算过程。</p><p>符号微分利用代数软件，实现微分的一些公式，然后根据基本函数的求导公式以及四则运算、复合函数的求导法则，将公式的计算过程转化成微分过程，这样就可以对用户提供的具有closed form的数学表达式进行”自动微分”求解。就是先求解析解，然后转换为程序，再通过程序计算出函数的梯度。</p><p>符号微分计算出的表达式需要用字符串或其他数据结构存储，如表达式树。因为符号微分的这些优点，其也在包括 Mathematica、Maple、matlab、Maxima 等现代代数系统工具软件中使用。</p><p>但符号微分的最大弊病在于其对表达式的严格展开和变换也导致了所谓的<strong>表达式膨胀</strong>（expression swell）问题。以递归表达式为例：</p><p>$$<br>l_{n+1}&#x3D;4l_n(1-l_n) \tag{9}<br>$$</p><p>可以看到在不同的迭代中其符号微分的结果相比人工简化后的结果复杂很多，且随着迭代次数而增大。</p><p><img src="/images/ad/WX20220522-143441@2x.png"></p><p>数值微分的优点是：</p><ul><li>精度高，可适用于更复杂的数学问题求解等场景</li><li>简单容易实现</li></ul><p>数值微分的缺点是：</p><ul><li>表达式必须是闭包（closed form），也就是必须能写成完整数学表达式的，不能有编程语言中的循环结构，条件结构等，这样才能将整个问题转换为一个纯数学符号问题</li><li>表达式复杂时候，求导结果存在表达式膨胀问题</li></ul><h2 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a><strong>自动微分</strong></h2><p>其实，对于机器学习中的应用，不需要得到导数的表达式，而只需计算函数在某一点处的导数值。</p><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a><strong>基本原理</strong></h3><p>自动微分是介于数值微分和符号微分之间的方法，采用类似有向图的计算来求解微分值。</p><ul><li><p>数值微分：一开始就直接代入数值近似求解</p></li><li><p>符号微分：直接对代数表达式求解析解，最后才代入数值进行计算</p></li><li><p>自动微分：首先对基本算子（函数）应用符号微分方法，其次带入数值进行计算，保留中间结果，最后通过链式求导法将中间结果应用于整个函数，这样可以做到完全向用户隐藏微分求解过程，也可以灵活于编程语言的循环结构、条件结构等结合起来</p></li></ul><p>关于解析解我们还要做一些说明。几乎所有机器学习算法在训练或预测时都可以归结为求解最优化问题，如果目标函数可导，则问题就变为求训练函数的驻点。但是通常情况下我们无法得到驻点的解析解，因此只能采用数值优化算法，如梯度下降法，牛顿法，拟牛顿法等等。这些数值优化算法都依赖于函数的一阶导数值或二阶导数值（包括梯度与Hessian矩阵）。因此需要解决如何求一个复杂函数的导数问题，自动微分技术是解决此问题的一种通用方法。</p><p>由于自动微分法只对基本函数或常数运用符号微分法则，所以它可以灵活结合编程语言的循环结构，条件结构等。使用自动微分和不使用自动微分对代码总体改动非常小，由于它实际是一种图计算，可以对其做很多优化，所以该方法在现代深度学习系统中得到广泛应用。</p><h3 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h3><p>在计算链式法则之前，我们先回顾一下复合函数。复合函数在本质上就是有关函数的函数（function of functions）。它将一个函数的返回值作为参数传递给另一个函数，并且将另一个函数的返回值作为参数再传递给下一个函数，也就是 函数套函数，把几个简单的函数复合为一个较为复杂的函数。</p><p>链式法则是微积分中的求导法则，用于求一个复合函数的导数，是在微积分的求导运算中一种常用的方法。复合函数的导数将是构成复合这有限个函数在相应点的 导数的乘积，就像锁链一样一环套一环，故称链式法则。</p><p>自动微分的思想则是将计算机程序中的运算操作分解为一个有限的基本操作集合，且集合中基本操作的求导规则均为已知在完成每一个基本操作的求导后，使用<strong>链式法则</strong>将结果组合得到整体程序的求导结果。</p><p>$$<br>(f \cdot g)’(x)&#x3D;f’(g(x))g’(x) \tag{10}<br>$$</p><p>比如求导：</p><p>$$<br>y&#x3D;sin(x^2+1) \tag{11}<br>$$</p><p>链式求导，令：</p><p>$$<br>f(x)&#x3D;sin(x),g(x)&#x3D;x^2+1 \tag{12}<br>$$</p><p>有：</p><p>$$<br>(f(g(x)))’&#x3D;f’(g(x))g’(x)&#x3D;[sin(x^2+1)]’ \cdot 2x&#x3D;2cos(x^2+1) \cdot x \tag{13}<br>$$</p><h3 id="自动微分-1"><a href="#自动微分-1" class="headerlink" title="自动微分"></a>自动微分</h3><p>自动微分的精髓在于它发现了微分计算的本质：<strong>微分计算就是一系列有限的可微算子的组合。</strong></p><p>自动微分法被认为是对计算机程序进行非标准的解释。自动微分基于一个事实，即每一个计算机程序，不论它有多么复杂，都是在执行加减乘除这一系列基本算数运算，以及指数、对数、三角函数这类初等函数运算。于是自动微分先将符号微分法应用于最基本的算子，比如常数，幂函数，指数函数，对数函数，三角函数等，然后代入数值，保留中间结果，最后再通过链式求导法则应用于整个函数。</p><p>通过将链式求导法则应用到这些运算上，我们能以任意精度自动地计算导数，而且最多只比原始程序多一个常数级的运算。</p><p>我们以如下为例，这是原始公式：</p><p>$$<br>y&#x3D;f(g(ℎ(x)))&#x3D;f(g(ℎ(w_0)))&#x3D;f(g(w_1))&#x3D;f(w_2)&#x3D;w_3 \tag{14}<br>$$</p><p>自动微分以链式法则为基础，把公式中一些部分整理出来成为一些新变量，然后用这些新变量整体替换这个公式，于是得到：</p><p>$$<br>w_0&#x3D;x \<br>w_1&#x3D;h(w_0) \<br>w_2&#x3D;g(w_1) \<br>w_3&#x3D;f(w_2)&#x3D;y<br>$$</p><p>然后把这些新变量作为节点，依据运算逻辑把公式整理出一张有向无环图（DAG）。即，原始函数建立计算图，数据正向传播，计算出中间节点 xi，并记录计算图中的节点依赖关系。</p><p>因此，自动微分可以被认为是将一个复杂的数学运算过程分解为一系列简单的基本运算， 其中每一项基本运算都可以通过查表得出来。</p><p>因此自动微分的优缺点可以简单总结如下：</p><ul><li>优点：精度高，无表达式膨胀问题</li><li>缺点：需要存储一些中间求导结果，内存占用会增加</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] Automatic Differentiation in Machine Learning: a Survey: <a href="https://arxiv.org/abs/1502.05767">https://arxiv.org/abs/1502.05767</a></p><p>[2] Rirchard L. Burden and J. Douglas Faires. Numerical Analysis. Brooks&#x2F;Cole, 2001.</p><p>[3] Max E. Jerrell. Automatic differentiation and interval arithmetic for estimation of disequilibrium models. Computational Economics, 10(3):295–316, 1997.</p><p>[4] Johannes Grabmeier and Erich Kaltofen. Computer Algebra Handbook: Foundations, Applications, Systems. Springer, 2003.</p><p>[5] G. W. Leibniz. Machina arithmetica in qua non additio tantum et subtractio sed et multiplicatio nullo, diviso vero paene nullo animi labore peragantur. Hannover, 1685.</p><p>[6] George F. Corliss. Application of differentiation arithmetic, volume 19 of Perspectives in Computing, pages 127–48. Academic Press, Boston, 1988.</p><p>[7] Arun Verma. An introduction to automatic differentiation. Current Science, 78(7):804–7, 2000.</p><p>[8] Andreas Griewank and Andrea Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. Society for Industrial and Applied Mathematics, Philadelphia, 2008. doi: 10.1137&#x2F;1.9780898717761.</p><p>[9] John F. Nolan. Analytical differentiation on a digital computer. Master’s thesis, Massachusetts Institute of Technology, 1953.</p><p>[10] L. M. Beda, L. N. Korolev, N. V. Sukkikh, and T. S. Frolova. Programs for automatic differentiation for the machine BESM (in Russian). Technical report, Institute for Precise Mechanics and Computation Techniques, Academy of Science, Moscow, USSR, 1959.</p><p>[11] Robert E. Wengert. A simple automatic derivative evaluation program. Communications of the ACM, 7:463–4, 1964.</p><p>[12] Andreas Griewank. On automatic differentiation. pages 83–108, 1989.</p><p>[13] Hascoet, Laurent, and Valérie Pascual. “The Tapenade automatic differentiation tool: principles, model, and specification.” ACM Transactions on Mathematical Software (TOMS) 39.3 (2013): 1-43.</p><p>[14] 知乎专栏：自动微分（Automatic Differentiation）: <a href="https://zhuanlan.zhihu.com/p/61103504">https://zhuanlan.zhihu.com/p/61103504</a></p><p>[15] 知乎专栏：数值计算方法 第六章 数值积分和数值微分: <a href="https://zhuanlan.zhihu.com/p/14">https://zhuanlan.zhihu.com/p/14</a></p><p>[16] 知乎专栏：技术分享 | 从自动微分到可微编程语言设计 <a href="https://zhuanlan.zhihu.com/p/393160344">https://zhuanlan.zhihu.com/p/393160344</a></p><p>[17] 博客园：深度学习利器之自动微分 <a href="https://www.cnblogs.com/rossiXYZ/p/15395742.html">https://www.cnblogs.com/rossiXYZ/p/15395742.html</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>AutoGrad</tag>
      
      <tag>AI System</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自动微分实现：反向OO实现自动微分（Pytroch核心机制）</title>
    <link href="/2022/07/31/ad05/"/>
    <url>/2022/07/31/ad05/</url>
    
    <content type="html"><![CDATA[<p><strong>【自动微分实现】反向OO实现自动微分（Pytroch核心机制）</strong></p><p>写【自动微分】原理和实现系列文章，存粹是为了梳理在 MindSpore 当SE时候最核心的自动微分原理。网上看了很多文章，基本上都是很零散，当然Automatic Differentiation in Machine Learning: a Survey[1] 这篇文章是目前ZOMI觉得比较好关于自动微分的综述论文。</p><ul><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/518198564">01. 一文看懂AD原理</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/518296942">02. AD的正反向模式</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/520065656">03. AD常用实现方案</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/520065656">04. 正向OO实现自动微分</a></li><li>【自动微分原理】<a href="https://zhuanlan.zhihu.com/p/547865589/">05. 反向OO实现自动微分(Pytroch核心机制)</a></li></ul><p>这里记录一下使用操作符重载（OO）编程方式的自动微分，其中数学实现模式则是使用反向模式（Reverse Mode），综合起来就叫做反向OO实现AD啦。</p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><p>下面一起来回顾一下操作符重载和反向模式的一些基本概念，然后一起去尝试着用Python去实现Pytorch这个AI框架中最核心的自动微分机制是如何实现的。</p><h2 id="操作符重载-OO"><a href="#操作符重载-OO" class="headerlink" title="操作符重载 OO"></a>操作符重载 OO</h2><blockquote><p><strong>操作符重载</strong>：操作符重载或者称运算重载（Operator Overloading，OO），利用现代语言的多态特性（例如C++&#x2F;JAVA&#x2F;Python等高级语言），使用操作符重载对语言中基本运算表达式的微分规则进行封装。同样，重载后的操作符在运行时会记录所有的操作符和相应的组合关系，最后使用链式法则对上述基本表达式的微分结果进行组合完成自动微分。</p></blockquote><p>在具有多态特性的现代编程语言中，运算符重载提供了实现自动微分的最直接方式，利用了编程语言的第一特性（first class feature），重新定义了微分基本操作语义的能力。</p><p>在 C++ 中使用运算符重载实现的流行工具是 ADOL-C（Walther 和 Griewank，2012）。 ADOL-C 要求对变量使用启用 AD 的类型，并在 Tape 数据结构中记录变量的算术运算，随后可以在反向模式 AD 计算期间“回放”。 Mxyzptlk 库 (Michelotti, 1990) 是 C++ 能够通过前向传播计算任意阶偏导数的另一个例子。 FADBAD++ 库（Bendtsen 和 Stauning，1996 年）使用模板和运算符重载为 C++ 实现自动微分。对于 Python 语言来说，autograd 提供正向和反向模式自动微分，支持高阶导数。在机器学习 ML 或者深度学习 DL 领域，目前AI框架中使用操作符重载 OO 的一个典型代表是 Pytroch，其中使用数据结构 Tape 来记录计算流程，在反向模式求解梯度的过程中进行 replay Operator。</p><p>下面总结一下操作符重载的一个基本流程：</p><ul><li><strong>操作符重载</strong>：预定义了特定的数据结构，并对该数据结构重载了相应的基本运算操作符</li><li><strong>Tape记录</strong>：程序在实际执行时会将相应表达式的操作类型和输入输出信息记录至特殊数据结构</li><li><strong>遍历微分</strong>：得到特殊数据结构后，将对数据结构进行遍历并对其中记录的基本运算操作进行微分</li><li><strong>链式组合</strong>：把结果通过链式法则进行组合，完成自动微分</li></ul><p>操作符重载法的<strong>优点</strong>可以总结如下：</p><ul><li>实现简单，只要求语言提供多态的特性能力</li><li>易用性高，重载操作符后跟使用原生语言的编程方式类似</li></ul><p>操作符重载法的<strong>缺点</strong>可以总结如下：</p><ul><li>需要显式的构造特殊数据结构和对特殊数据结构进行大量读写、遍历操作，这些额外数据结构和操作的引入不利于高阶微分的实现</li><li>对于类似 if，while 等控制流表达式，难以通过操作符重载进行微分规则定义。对于这些操作的处理会退化成基本表达式方法中特定函数封装的方式，难以使用语言原生的控制流表达式</li></ul><h2 id="反向模式-Reverse-Mode"><a href="#反向模式-Reverse-Mode" class="headerlink" title="反向模式 Reverse Mode"></a>反向模式 Reverse Mode</h2><p>反向自动微分同样是基于链式法则。仅需要一个前向过程和反向过程，就可以计算所有参数的导数或者梯度。因为需要结合前向和后向两个过程，因此反向自动微分会使用一个特殊的数据结构，来存储计算过程。</p><p>而这个特殊的数据结构例如 Tensorflow 或者 MindSpore，则是把所有的操作以一张图的方式存储下来，这张图可以是一个有向无环（DAG）的计算图；而Pytroch 则是使用 Tape 来记录每一个操作，他们都表达了函数和变量的关系。</p><p>反向模式根据从后向前计算，依次得到对每个中间变量节点的偏导数，直到到达自变量节点处，这样就得到了每个输入的偏导数。在每个节点处，根据该节点的后续节点（前向传播中的后续节点）计算其导数值。</p><p>整个过程对应于多元复合函数求导时从最外层逐步向内侧求导。这样可以有效地把各个节点的梯度计算解耦开，每次只需要关注计算图中当前节点的梯度计算。</p><p>从下图可以看出来，reverse mode和forward mode是一对相反过程，reverse mode从最终结果开始求导，利用最终输出对每一个节点进行求导。下图虚线就是反向模式。</p><p><img src="/./images/ad/reversed_mode.png"></p><p>前向和后向两种模式的过程表达如下，表的左列浅色为前向计算函数值的过程，与前向计算时相同，右面列深色为反向计算导数值的过程。</p><p>反向模式的计算过程如图所示，其中：</p><p>$$<br>\overline{v_i}&#x3D;\dfrac{\delta y}{\delta v_i}<br>$$</p><p>根据链式求导法则展开有：</p><p>$$<br>\frac{\partial f}{\partial x}&#x3D;\sum_{k&#x3D;1}^{N} \frac{\partial f}{\partial v_{k}} \frac{\partial v_{k}}{\partial \boldsymbol{x}}<br>$$</p><p>可以看出，左侧是源程序分解后得到的基本操作集合，而右侧则是每一个基本操作根据已知的求导规则和链式法则<strong>由下至上</strong>计算的求导结果。</p><p><img src="/images/ad/WX20220522-180618@2x.png"></p><h1 id="反向操作符重载实现"><a href="#反向操作符重载实现" class="headerlink" title="反向操作符重载实现"></a>反向操作符重载实现</h1><p>下面的代码主要介绍反向模式自动微分的实现。目的是通过了解PyTorch的auto diff实现，来了解到上面复杂的反向操作符重载实现自动微分的原理，值的主要的是千万不要在乎这是 MindSpore 的实现还是 Tensorflow 版的实现（实际上都不是哈）。</p><p>首先，需要通过 typing 库导入一些辅助函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span>, NamedTuple, <span class="hljs-type">Callable</span>, <span class="hljs-type">Dict</span>, <span class="hljs-type">Optional</span><br><br>_name = <span class="hljs-number">1</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fresh_name</span>():<br>    <span class="hljs-keyword">global</span> _name<br>    name = <span class="hljs-string">f&#x27;v<span class="hljs-subst">&#123;_name&#125;</span>&#x27;</span><br>    _name += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> name<br></code></pre></td></tr></table></figure><p><code>fresh_name</code> 用于打印跟 <code>tape</code> 相关的变量，并用 <code>_name</code> 来记录是第几个变量。</p><p>为了能够更好滴理解反向模式自动微分的实现，实现代码过程中不依赖PyTorch的autograd。代码中添加了变量类 <code>Variable</code> 来跟踪计算梯度，并添加了梯度函数 <code>grad()</code> 来计算梯度。</p><p>对于标量损失l来说，程序中计算的每个张量 x 的值，都会计算值dl&#x2F;dX。反向模式从 dl&#x2F;dl&#x3D;1 开始，使用偏导数和链式规则向后传播导数，例如：</p><p>$$<br>dl&#x2F;dx*dx&#x2F;dy&#x3D;dl&#x2F;dy<br>$$</p><p>下面就是具体的实现过程，首先我们所有的操作都是通过Python进行操作符重载的，而操作符重载，通过 <code>Variable</code> 来封装跟踪计算的 Tensor。每个变量都有一个全局唯一的名称 <code>fresh_name</code>，因此可以在字典中跟踪该变量的梯度。为了便于理解，<code>__init__</code> 有时会提供此名称作为参数。否则，每次都会生成一个新的临时值。</p><p>为了适配上面图中的简单计算，这里面只提供了 乘、加、减、sin、log 五种计算方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Variable</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, value, name=<span class="hljs-literal">None</span></span>):<br>        self.value = value<br>        self.name = name <span class="hljs-keyword">or</span> fresh_name()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">repr</span>(self.value)<br>    <br>    <span class="hljs-comment"># We need to start with some tensors whose values were not computed</span><br>    <span class="hljs-comment"># inside the autograd. This function constructs leaf nodes. </span><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">constant</span>(<span class="hljs-params">value, name=<span class="hljs-literal">None</span></span>):<br>        var = Variable(value, name)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;var.name&#125;</span> = <span class="hljs-subst">&#123;value&#125;</span>&#x27;</span>)<br>        <span class="hljs-keyword">return</span> var<br>    <br>    <span class="hljs-comment"># Multiplication of a Variable, tracking gradients</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__mul__</span>(<span class="hljs-params">self, other</span>):<br>        <span class="hljs-keyword">return</span> ops_mul(self, other)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__add__</span>(<span class="hljs-params">self, other</span>):<br>        <span class="hljs-keyword">return</span> ops_add(self, other)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__sub__</span>(<span class="hljs-params">self, other</span>):<br>        <span class="hljs-keyword">return</span> ops_sub(self, other)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sin</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> ops_sin(self)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">log</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> ops_log(self)<br>    <br></code></pre></td></tr></table></figure><p>接下来需要跟踪 <code>Variable</code> 所有计算，以便向后应用链式规则。那么数据结构 <code>Tape</code> 有助于实现这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Tape</span>(<span class="hljs-title class_ inherited__">NamedTuple</span>):<br>    inputs : <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]<br>    outputs : <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]<br>    <span class="hljs-comment"># apply chain rule</span><br>    propagate : <span class="hljs-string">&#x27;Callable[List[Variable], List[Variable]]&#x27;</span><br></code></pre></td></tr></table></figure><p>输入 <code>inputs</code> 和输出 <code>outputs</code> 是原始计算的输入和输出变量的唯一名称。反向传播使用链式规则，将函数的输出梯度传播给输入。其输入为 dL&#x2F;dOutputs，输出为 dL&#x2F;dinput。Tape只是一个记录所有计算的累积 List 列表。</p><p>下面提供了一种重置 Tape 的方法 <code>reset_tape</code>，方便运行多次自动微分，每次自动微分过程都会产生 Tape List。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">gradient_tape : <span class="hljs-type">List</span>[Tape] = []<br><br><span class="hljs-comment"># reset tape</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reset_tape</span>():<br>    <span class="hljs-keyword">global</span> _name<br>    _name = <span class="hljs-number">1</span><br>    gradient_tape.clear()<br></code></pre></td></tr></table></figure><p>现在来看看具体运算操作符是如何定义的，以乘法为例子啦，首先需要计算正向结果并创建一个新变量来表示，也就是 <code>x = Variable(self.value * other.value)</code>。然后定义了反向传播闭包 <code>propagate</code>，使用链规则来反向支撑梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ops_mul</span>(<span class="hljs-params">self, other</span>):<br>    <span class="hljs-comment"># forward</span><br>    x = Variable(self.value * other.value)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;x.name&#125;</span> = <span class="hljs-subst">&#123;self.name&#125;</span> * <span class="hljs-subst">&#123;other.name&#125;</span>&#x27;</span>)<br>    <br>    <span class="hljs-comment"># backward</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">propagate</span>(<span class="hljs-params">dl_doutputs</span>):<br>        dl_dx, = dl_doutputs<br>        dx_dself = other <span class="hljs-comment"># partial derivate of r = self*other</span><br>        dx_dother = self <span class="hljs-comment"># partial derivate of r = self*other</span><br>        dl_dself = dl_dx * dx_dself<br>        dl_dother = dl_dx * dx_dother<br>        dl_dinputs = [dl_dself, dl_dother]<br>        <span class="hljs-keyword">return</span> dl_dinputs<br>    <br>    <span class="hljs-comment"># record the input and output of the op</span><br>    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)<br>    gradient_tape.append(tape)<br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ops_add</span>(<span class="hljs-params">self, other</span>):<br>    x = Variable(self.value + other.value)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;x.name&#125;</span> = <span class="hljs-subst">&#123;self.name&#125;</span> + <span class="hljs-subst">&#123;other.name&#125;</span>&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">propagate</span>(<span class="hljs-params">dl_doutputs</span>):<br>        dl_dx, = dl_doutputs<br>        dx_dself = Variable(<span class="hljs-number">1.</span>)<br>        dx_dother = Variable(<span class="hljs-number">1.</span>)<br>        dl_dself = dl_dx * dx_dself<br>        dl_dother = dl_dx * dx_dother<br>        <span class="hljs-keyword">return</span> [dl_dself, dl_dother]<br>    <br>    <span class="hljs-comment"># record the input and output of the op</span><br>    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)<br>    gradient_tape.append(tape)<br>    <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ops_sub</span>(<span class="hljs-params">self, other</span>):<br>    x = Variable(self.value - other.value)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;x.name&#125;</span> = <span class="hljs-subst">&#123;self.name&#125;</span> - <span class="hljs-subst">&#123;other.name&#125;</span>&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">propagate</span>(<span class="hljs-params">dl_doutputs</span>):<br>        dl_dx, = dl_doutputs<br>        dx_dself = Variable(<span class="hljs-number">1.</span>)<br>        dx_dother = Variable(-<span class="hljs-number">1.</span>)<br>        dl_dself = dl_dx * dx_dself<br>        dl_dother = dl_dx * dx_dother<br>        <span class="hljs-keyword">return</span> [dl_dself, dl_dother]<br>    <br>    <span class="hljs-comment"># record the input and output of the op</span><br>    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)<br>    gradient_tape.append(tape)<br>    <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ops_sin</span>(<span class="hljs-params">self</span>):<br>    x = Variable(np.sin(self.value))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;x.name&#125;</span> = sin(<span class="hljs-subst">&#123;self.name&#125;</span>)&#x27;</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">propagate</span>(<span class="hljs-params">dl_doutputs</span>):<br>        dl_dx, = dl_doutputs<br>        dx_dself = Variable(np.cos(self.value))<br>        dl_dself = dl_dx * dx_dself<br>        <span class="hljs-keyword">return</span> [dl_dself]<br>    <br>    <span class="hljs-comment"># record the input and output of the op</span><br>    tape = Tape(inputs=[self.name], outputs=[x.name], propagate=propagate)<br>    gradient_tape.append(tape)<br>    <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ops_log</span>(<span class="hljs-params">self</span>):<br>    x = Variable(np.log(self.value))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;x.name&#125;</span> = log(<span class="hljs-subst">&#123;self.name&#125;</span>)&#x27;</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">propagate</span>(<span class="hljs-params">dl_doutputs</span>):<br>        dl_dx, = dl_doutputs<br>        dx_dself = Variable(<span class="hljs-number">1</span> / self.value)<br>        dl_dself = dl_dx * dx_dself<br>        <span class="hljs-keyword">return</span> [dl_dself]<br>    <br>    <span class="hljs-comment"># record the input and output of the op</span><br>    tape = Tape(inputs=[self.name], outputs=[x.name], propagate=propagate)<br>    gradient_tape.append(tape)<br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p><code>grad</code> 呢是将变量运算放在一起的梯度函数，函数的输入是 l 和对应的梯度结果 results。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">grad</span>(<span class="hljs-params">l, results</span>):<br>    dl_d = &#123;&#125; <span class="hljs-comment"># map dL/dX for all values X</span><br>    dl_d[l.name] = Variable(<span class="hljs-number">1.</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;dl_d&quot;</span>, dl_d)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">gather_grad</span>(<span class="hljs-params">entries</span>):<br>        <span class="hljs-keyword">return</span> [dl_d[entry] <span class="hljs-keyword">if</span> entry <span class="hljs-keyword">in</span> dl_d <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> entry <span class="hljs-keyword">in</span> entries]<br>    <br>    <span class="hljs-keyword">for</span> entry <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(gradient_tape):<br>        <span class="hljs-built_in">print</span>(entry)<br>        dl_doutputs = gather_grad(entry.outputs)<br>        dl_dinputs = entry.propagate(dl_doutputs)<br>        <br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, dl_dinput <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(entry.inputs, dl_dinputs):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">input</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> dl_d:<br>                dl_d[<span class="hljs-built_in">input</span>] = dl_dinput<br>            <span class="hljs-keyword">else</span>:<br>                dl_d[<span class="hljs-built_in">input</span>] += dl_dinput<br>            <br>    <span class="hljs-keyword">for</span> name, value <span class="hljs-keyword">in</span> dl_d.items():<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;d<span class="hljs-subst">&#123;l.name&#125;</span>_d<span class="hljs-subst">&#123;name&#125;</span> = <span class="hljs-subst">&#123;value.name&#125;</span>&#x27;</span>)<br>        <br>    <span class="hljs-keyword">return</span> gather_grad(result.name <span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results)<br></code></pre></td></tr></table></figure><p>以公式5为例：</p><p>$$<br>f(x1,x2)&#x3D;ln(x1)+x1x2−sin(x2) \tag{1}<br>$$</p><p>因为是基于操作符重载OO的方式进行计算，因此在初始化自变量 x 和 y 的值需要使用变量 <code>Variable</code> 来初始化，然后通过代码 <code>f = Variable.log(x) + x * y - Variable.sin(y)</code> 来实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">reset_tape()<br><br>x = Variable.constant(<span class="hljs-number">2.</span>, name=<span class="hljs-string">&#x27;v-1&#x27;</span>)<br>y = Variable.constant(<span class="hljs-number">5.</span>, name=<span class="hljs-string">&#x27;v0&#x27;</span>)<br><br>f = Variable.log(x) + x * y - Variable.sin(y)<br><span class="hljs-built_in">print</span>(f)<br></code></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Text">v-1 = 2.0<br>v0 = 5.0<br>v1 = log(v-1)<br>v2 = v-1 * v0<br>v3 = v1 + v2<br>v4 = sin(v0)<br>v5 = v3 - v4<br>11.652071455223084<br></code></pre></td></tr></table></figure><p>从 <code>print(f)</code> 可以看到是下面图中的左边正向运算，计算出前向的结果。下面的代码 <code>grad(f, [x, y])</code> 就是利用前向最终的结果，通过 Tape 一个个反向的求解。得到最后的结果啦。</p><p><img src="/images/ad/WX20220522-180618@2x.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">dx, dy = grad(f, [x, y])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;dx&quot;</span>, dx)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;dy&quot;</span>, dy)<br></code></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs Text">dl_d &#123;&#x27;v5&#x27;: 1.0&#125;<br>Tape(inputs=[&#x27;v3&#x27;, &#x27;v4&#x27;], outputs=[&#x27;v5&#x27;], propagate=&lt;function ops_sub.&lt;locals&gt;.propagate at 0x7fd7a2c8c0d0&gt;)<br>v9 = v6 * v7<br>v10 = v6 * v8<br>Tape(inputs=[&#x27;v0&#x27;], outputs=[&#x27;v4&#x27;], propagate=&lt;function ops_sin.&lt;locals&gt;.propagate at 0x7fd7a2c8c378&gt;)<br>v12 = v10 * v11<br>Tape(inputs=[&#x27;v1&#x27;, &#x27;v2&#x27;], outputs=[&#x27;v3&#x27;], propagate=&lt;function ops_add.&lt;locals&gt;.propagate at 0x7fd7a234e7b8&gt;)<br>v15 = v9 * v13<br>v16 = v9 * v14<br>Tape(inputs=[&#x27;v-1&#x27;, &#x27;v0&#x27;], outputs=[&#x27;v2&#x27;], propagate=&lt;function ops_mul.&lt;locals&gt;.propagate at 0x7fd7a3982ae8&gt;)<br>v17 = v16 * v0<br>v18 = v16 * v-1<br>v19 = v12 + v18<br>Tape(inputs=[&#x27;v-1&#x27;], outputs=[&#x27;v1&#x27;], propagate=&lt;function ops_log.&lt;locals&gt;.propagate at 0x7fd7a3982c80&gt;)<br>v21 = v15 * v20<br>v22 = v17 + v21<br>dv5_dv5 = v6<br>dv5_dv3 = v9<br>dv5_dv4 = v10<br>dv5_dv0 = v19<br>dv5_dv1 = v15<br>dv5_dv2 = v16<br>dv5_dv-1 = v22<br>dx 5.5<br>dy 1.7163378145367738<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>AutoGrad</tag>
      
      <tag>AI System</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VAE：学习高维数据分布</title>
    <link href="/2022/07/29/vae/"/>
    <url>/2022/07/29/vae/</url>
    
    <content type="html"><![CDATA[<p>当前的内容是梳理<a href="https://zhuanlan.zhihu.com/p/543227883">《Transformer视觉系列遨游》</a>系列过程中引申出来的。目前最近在AI作画这个领域 Transformer 火的一塌糊涂，AI画画效果从18年的 DeepDream[1] 噩梦中惊醒过来，开始从2022年 OpenAI 的 DALL·E 2[2] 引来插画效果和联想效果都达到惊人效果。虽然不懂，但是这个话题很吸引ZOMI，于是就着这个领域内容来看看有什么好玩的技术点。</p><p><img src="/images/vae/painter1.png"></p><p>但是要了解：<strong>Transformer 带来AI+艺术，从语言开始遇到多模态，碰撞艺术火花</strong> 这个主题，需要引申很多额外的知识点，可能跟 CV、NLP 等领域大力出奇迹的方式不同，AI+艺术会除了遇到 Transformer 结构以外，还会涉及到 VAE、ELBO、Diffusion Model等一系列跟数学相关的知识。</p><p>这里介绍的 AVE 是万里长征的第一步，<strong>VAE 希望训练一个生成模型 X&#x3D;g(Z)，这个模型能够将采样后的概率分布映射到训练集的概率分布</strong>。</p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><p>之前天天听公司去年入职的搞 NLP 的同事说 VAE 怎么样怎么样，一直有留意但是没有注意，以为跟 AE 一样比较简单几层网络模型加一个 Reconstruct Loss。没想到看到论文《Auto-Encoding Variational Bayes》就蒙圈了，于是乎照样翻了网上很多资料，无一例外发现都很含糊，主要的感觉是公式写了一大通，老瓜子嗡嗡的，最后感觉自己搞明白了，但是看看代码实现发现怎么这么简单，刚才理解的公式都去哪里了？</p><p>这里面参考了很多文章，特别感谢博主 苏剑林 的图文讲解和 李宏毅 老师的视频。加上自己这段时间每天挂着个问号在脑袋旁边。感觉有点想明白了，于是赶紧积累下来，希望通过下面的文字把 VAE 梳理清楚。</p><p>下面列了几个跟 VAE 强相关的简单知识点，首先是 1）<strong>混合高斯模型</strong>主要是对高维的数据进行概率表示，2）而 <strong>KL 散度</strong>则是用来对比两个概率分布之间的差异，3）当AI具备了解数据的概率表示和数据的分布差异，那么是不是可以通过一个<strong>自动编码器</strong>来去模拟数据生成呢？</p><p>于是就有了下面三个基础知识点。</p><h2 id="GMM-混合高斯模型"><a href="#GMM-混合高斯模型" class="headerlink" title="GMM 混合高斯模型"></a>GMM 混合高斯模型</h2><p><strong>混合高斯模型（Gaussian Mixture Model，GMM）</strong> 指的是多个高斯分布函数的线性组合，理论上 GMM 可以拟合出任意类型的分布，通常用于解决同一集合下的数据包含多个不同的分布情况。</p><p>简单可以理解 GMM 是一种聚类算法，一般使用期望最大算法（Expectation Maximization，EM）进行估计。而 EM 算法通常用来估计参数的隐变量 z 的一种方法，它是一种迭代的方法，大致可以分为 E 步和 M 步：</p><ol><li><strong>期望 E 步</strong>：若参数 Θ 已知，则可根据训练数据推断出最优隐变量 z 的值；</li><li><strong>最大化 M 步</strong>：若 z 的值已知，则可方便的对参数 Θ 做极大似然估计，求得参数 Θ 值；</li></ol><p><img src="/images/vae/gmm.png"></p><h2 id="KL-散度"><a href="#KL-散度" class="headerlink" title="KL 散度"></a>KL 散度</h2><p><strong>KL散度（Kullback-Leibler divergence，KLD）</strong> 是对两个概率分布 P 和 Q 差别的非对称性的度量。一般来说 P 表示数据的真实分布，Q 表示数据的理论分布。如果两个分布属于同一概率分布（即它们相似）, 那么KL散度越小；反之, 则越大。</p><p>$$<br>D_{\mathrm{KL}}(P | Q)&#x3D;-\sum_{i} P(i) \log \frac{Q(i)}{P(i)}<br>$$</p><p>即按概率分布 P 求得的 P 和 Q 的对数商的平均值。</p><p><img src="/images/vae/kl.jpg"></p><h2 id="AE-自动编码器"><a href="#AE-自动编码器" class="headerlink" title="AE 自动编码器"></a>AE 自动编码器</h2><p><strong>自编码器（auto encoder, AE）</strong> 是一类在半监督学习和非监督学习中使用的人工神经网络 ANN，其功能是通过将输入信息作为学习目标，对输入信息进行表征学习（representation learning）。</p><h3 id="AE-算法原理"><a href="#AE-算法原理" class="headerlink" title="AE 算法原理"></a>AE 算法原理</h3><p>自编码器比较好理解，直接看图就搞清楚了，主要是通常包括两部分：encoder（也称为识别网络）将输入转换成隐变量 z，decoder（也称为生成网络）将隐变量 z 表示转换成输出。</p><p><img src="/images/vae/ae1.png"></p><p>回到那一年，Auto-Encoder 自编码器是1986年由 Rumelhart 提出，可用于高维复杂数据的处理, 它促进了神经网络的发展。自编码神经网络是一种无监督学习算法（训练示例未标注），它使用了BP反向传播算法，致力于使输出与输入越接近越好，而输入和输出相同则是使用了 Reconstruct Error，即最小化输入和输出之间的重构误差。</p><p>根据图示，AE的算法描述为：</p><ol><li><p><strong>Encoder</strong>：负责将输入数据进行压缩，n 维输入数据通过 Hidden layer 压缩成 m 维的数据（m &lt;&lt; n），即通过编码器学习一组参数，得到一个 latent space；</p></li><li><p><strong>Decoder</strong>：负责还原数据，在需要用到的时候尽可能地以损失最小的方式恢复原始数据。</p></li></ol><p>AE应用范围一般，但扩展能力很强，可以应用于机器学习中的数据降维、特征抽取和数据可视化分析，在CV领域的应用有文本检索、以图搜图、还可用于预训练，也可扩展并应用于生成模型。结构上在 NLP 领域衍生出了 Seq2Seq 架构，而 CV 领域则衍生出类似于 U-Net 架构啦。（虽然很基础，但是很有用哦）</p><p><img src="/images/vae/ae2.png"></p><h3 id="AE-算法问题"><a href="#AE-算法问题" class="headerlink" title="AE 算法问题"></a>AE 算法问题</h3><p>AE 主要是表达一个自编码器模型，但还不是真正意义上的生成模型。对于一个特定的生成模型，它一般应该满足以下两点：</p><ol><li>编码器和解码器是可以独立拆分表达（类比GAN的Generator和Discriminator）。</li><li>固定维度下任意采样出来的编码向量，都应该能通过解码器产生一个对应的数据。</li></ol><p>下面参考李宏毅老师的解释，换成我最近比较喜欢的二哈。假设使用一张二哈和另外一张二哈去训练一个AE，经过训练，模型能够很好地还原出这两张图片。接下来，我们在 latent space 上中间一点，即两张图片编码空间中间处任取一点，将这点交给解码器进行解码，直觉上会得到一张二哈串串的图片。不过呢，实际 那这个点去 decode 的时候会发现 AE 还原出来的图片不仅模糊而且还是乱码的。</p><p>为什么会出现这种现象？一个直观上的解释是AE的 Encoder 和 Decoder 都使用了 DNN，DNN是一个非线性的变换过程，因此在latent space上点与点之间transform往往没有规律可循。</p><p><img src="/images/vae/ae3.png"></p><p>如何解决这个问题呢？一个思想就是引入噪声，扩大图片的编码区域，从而能够覆盖到失真的空白编码区。其实说白了就是通过增加输入的多样性从而增强输出的鲁棒性。当给输入图片进行编码之前引入一点噪声，那么就可以让每张图片的编码空间出现在绿色箭头范围内，这样一来所得到的 latent space 就能覆盖到更多的编码范围。此时再从中间点抽取去还原便可以得到一个比较理想的输出啦。</p><p><img src="/images/vae/ae4.png"></p><p>虽然在AE的基础上为输入数据增添了一些噪声使得 latent space 能够覆盖到比较多的区域，但是还是有不少地方没有被覆盖到，比如上图右边黄色的部分因为离得比较远所以就没编码到。</p><p>因此，是不是可以尝试利用更多的噪音，使得对于每一个输入样本，它的编码都能够覆盖到整个编码空间？也就是说，对每一个样本取得其编码空间的高斯分布，多个分布合起来变成一个混合高斯分布，latent space 的泛化能力就更强呢？</p><p>到这里已经不知不觉就引入了 VAE 变分自编码器的核心思想啦。</p><h1 id="VAE-变分自编码器"><a href="#VAE-变分自编码器" class="headerlink" title="VAE 变分自编码器"></a>VAE 变分自编码器</h1><p>通常意义上的编码器 Encoder 可以对数据进行压缩，降噪之类的一些处理，但是实际上不能来生成任意数据。上面不是已经简单用二哈举了个小例子， VAE 可以生成隐变量 z，并且 z 是及含有数据信息又含有噪声，除了还原输入的样本数据以外，还可以用于生成新的数据。</p><p>这么一说，感觉 VAE 其实也是个生成模型（功能和 GAN 相似），的确，GAN 和 VAE 两个的目标基本是一致的：希望构建一个从隐变量 z 生成目标数据 X 的模型，但是实现上有所不同。更准确地讲，它们是假设 z 服从某些常见的分布（比如正态分布），然后希望训练一个模型 X&#x3D;g(Z)，这个模型能够将原来的概率分布映射到训练集的概率分布，也就是说，<strong>目的都是进行分布之间的变换。</strong></p><p>不过 VAE 采用的是概率的思想用神经网络迭代求解数据分布概率，GAN 直接用神经网络训练一个判别器。实际效果上，VAE 的鲁棒性比 GAN 更好，但是 GAN 在调优之后效果比 VAE更好，这也是 GAN 大火的原因。</p><p>ZOMI 先来学习 VAE 的数学模型结构（之所以称为数学模型结构而不是网络模型结构呢，是因为 VAE 是基于数学模型原理推导出来的，为了去计算或者逼近拟合后验概率 p(z|x) 从而引入的MLP）。</p><p><img src="/images/vae/vae1.png"></p><p>从图中可以看到，VAE就是在原本的AE结构上，给编码添加合适的噪声。VAE 的输入是样本 X 通过编码器 Encoder 求得平均值 μ 和标准差 σ 的向量；然后通过采样得到隐向量 z，接着通过解码器 Decoder 得到输出 $\hat{x}$。</p><h2 id="Native-VAE"><a href="#Native-VAE" class="headerlink" title="Native VAE"></a>Native VAE</h2><p>VAE 看上去很简单，实际代码实心也很简单，Encoder 和 Decoder 都是用 MLP 来实现。难的在于为何要这么设计和计算。</p><p><img src="/images/vae/vae2.png"></p><p>假设现在我们手头上，有一批数据样本 ${x_1,…,x_n}$，其整体用 $x$ 来描述，本想根据 ${x_1,…,x_n}$ 得到  $x$ 的分布 $p(x)$。如果能直接得到的话，那根据 $p(x)$ 来采样，就可以得到所有可能的样本 $\hat{x}$ 。这是一个终极理想的生成模型，要是真的这么直接那就没 VAE 什么事情了。</p><p>由 VAE 的模型结构，可以看到编码空间 $z$ 是由一个标准正态分布所产生的向量。实际上概率分布可以表示为：</p><p>$$<br>p(x)&#x3D;\int_{z}p(z)p(x \mid z) dz<br>\tag{1}<br>$$</p><p>上面公式中的 $p(x\mid z)$ 英文是 p of x given z 描述一个由 z 来生成 x 的模型，假设 z 服从正态分布，也就是 $p(z)&#x3D;\mathcal{N}(0, I)$。那么现在就可以从标准正太分布中采样一个 z，然后根据编码空间 z 计算出 x ，生成的 x 和真实样本 x 进行对比迭代优化求解，这就是最简单的生成模型。根据博主 苏剑林的理解，从新画了张图。</p><p><img src="/images/vae/vae3.png" alt="vae的传统理解"></p><p>这里面有个问题，经过重新采样出来的 $z_k$，因为已经经过一个正态分布 $N(0, I)$ 的压缩处理，压缩后的 $z_k$ 就不再是对应着原来的 $x_k$ 了（已经经过编码压缩），如果直接最小化 $L(\hat{x}_k,x_k)^2$ 是很不合理的。</p><h2 id="Vanilla-VAE"><a href="#Vanilla-VAE" class="headerlink" title="Vanilla VAE"></a>Vanilla VAE</h2><p>上面的 Native VAE 中并没有使用 p(z) 是正态分布的假设，而是假设后验分布 $P(z \mid x)$ 符合正态分布，也就是经过编码 Encoder 得到的分布。</p><p>具体来说，给定一个真实样本 $x_k$，假设存在一个专属于样本 $x_k$ 的后验分布 $P(z\mid x_k)$，并进一步假设这个分布符合正态分布。之所以是后验分布，是因为 $P(z\mid x_k)$ 与 样本 $x_k$ 相关，从分布中采样出来的 z 可以还原回 $x_k$ 中。</p><p>现在对 latency space 随机采样 m 个点，其中 m 服从多项式分布 $p(x)$，每采样一个点 m，将其对应到一个高斯分布 $N(μ^m, σ^m)$，于是一个多项式分布利用高斯混合模型 GMM 可以表示为：</p><p>$$<br>p(x)&#x3D;\sum_{m} p(m) p(x \mid m)&#x3D;\int_{z}p(z)p(x \mid z) dz<br>\tag{2}<br>$$</p><p>我们知道正态分布有两组参数：分别是均值 $μ$ 和方差 $σ^2$，现在问题是怎么找到属于样本 $x_k$ 的正态分布 $p(z \mid x_k)$ 的均值和方差呢？这个时候神经网络就来了，可以用 MLP 来进行拟合。于是在 Encoder 阶段构建两个神经网络来计算均值 $μ$ 和方差 $σ^2$。</p><p>经过上面的推论，便可以将原先离散的、存在大量失真区域的编码方式，转换成连续有效的编码方式。根据 Z 来计算 X，即计算分布概率 $p(x \mid z)$。</p><p><img src="/images/vae/vae4.png" alt="vae的传统理解"></p><p>下面就是 VAE 的 Encoder 和 Decoder 阶段的内容：</p><ol><li><p><strong>Encoder</strong>：利用神经网络 MLP 来求解 $μ(z)$ 和 $σ(z)$， 等价于求解 $p(x \mid z)$。</p></li><li><p><strong>Decoder</strong>：利用神经网络 MLP 来求解 $q(z|x)$，q 为GMM的分布。</p></li></ol><p>VAE 需要求解的目标为表达式 (2)，原则上希望分布 $p(x)$ 越大越好。根据最大似然估计，等价于求解：</p><p>$$<br>\operatorname{Maximum} L&#x3D;\sum_{x} \log p(x)<br>\tag{3}<br>$$</p><p>给定任意一个分布q，因为 $\int_{z} q(z \mid x) d z&#x3D;1$，所以可以推导出：</p><p>$$<br>\begin{aligned}<br>\log p(x) &amp;&#x3D; \int_{z} q(z \mid x) \log p(x) dz \<br>&amp;&#x3D; \int_{z} q(z \mid x) \log \left(\frac{p(z, x)}{p(z \mid x)}\right) dz \<br>&amp;&#x3D; \int_{z} q(z \mid x) \log \left(\frac{p(z, x)}{q(z \mid x)} \frac{q(z \mid x)}{p(z \mid x)}\right) dz \<br>&amp;&#x3D; \int_{z} q(z \mid x) \log \left(\frac{p(z, x)}{q(z \mid x)}\right) dz+\int_{z} q(z \mid x) \log \left(\frac{q(z \mid x)}{p(z \mid x)}\right) dz \<br>&amp;&#x3D; \int_{z} q(z \mid x) \log \left(\frac{p(z, x)}{q(z \mid x)}\right) dz+KL(q(z \mid x) | p(z \mid x))<br>\end{aligned}<br>\tag{4}<br>$$</p><p>上式右边一项为 $q(z \mid x)$ 和 $p(z \mid x)$ 这两个分布的 KL 散度，根据KL 散度公式的性质，可以知道右边 KL 项恒大于等于0，于是可以找到 $log p(x)$ 的 对数似然的下界（Evidence Lower Bound，ELBO），此时可以把最大化对数似然转化为最大化 ELBO。即：</p><p>$$<br>\log p(x) \geq \int_{z} q(z \mid x) \log \left(\frac{p(x \mid z) p(z)}{q(z \mid x)}\right) dz<br>\tag{5}<br>$$</p><p>这里把 ELBO 记作：</p><p>$$<br>L_{b}&#x3D;\int_{z} q(z \mid x) \log \left(\frac{p(x \mid z) p(z))}{q(z \mid x)}\right) dz<br>\tag{6}<br>$$</p><p>代入式 (4)，可写成：</p><p>$$<br>\log p(x)&#x3D;L_{b}+KL(q(z \mid x) | p(z \mid x))<br>\tag{7}<br>$$</p><p>有趣的地方来了，原本我们是求使得 $log p(x)$ 最大化的 $p(x \mid z)$， 现在转为同时求解 $p(x \mid z)$ 和 $q(z \mid x)$。为什么把一个简单的事情复杂化求解多个变量？下面我们观察下 $log p(x)$  和 $L_b$ 之间的关系。</p><p><img src="/images/vae/vae7.png"></p><p>根据公式(2)，$Log p(x)$ 是固定的，如果调节 $q(z \mid x)$ 使得 $L_b$ 越大，那么 KL 散度就会越小。当两个分布 $q(z \mid x)$ 和 $p(x \mid z)$ 完全一致的时候，KL 散度为0，此时 ELBO $L_b$ 就等于 $Log p(x)$。</p><p>因为 $L_b$ 是 $Log p(x)$ 的下界，所以求解最大似然估计 $\operatorname{Maximum} L$ 等价于求解：</p><p>$$<br>\operatorname{Maximum} L_b<br>\tag{8}<br>$$</p><p>而调节 $p(x \mid z)$ 就是训练 Decoder 的过程；调节 $q(z \mid x)$ 变成训练 Encoder 的过程。下面继续打开 ELBO：</p><p>$$<br>\begin{aligned}<br>L_{b} &amp;&#x3D;\int_{z} q(z \mid x) \log \left(\frac{P(z, x)}{P(z \mid x)}\right) dz \<br>&amp;&#x3D;\int_{z} q(z \mid x) \log \left(\frac{P(x \mid z) P(z)}{q(z \mid x)}\right) dz \<br>&amp;&#x3D;\int_{z} q(z \mid x) \log \left(\frac{P(z)}{q(z \mid x)}\right) dz+\int_{z} q(z \mid x) \log P(x \mid z) dz \<br>&amp;&#x3D;-KL(q(z \mid x) | P(z))+\int_{z} q(z \mid x) \log P(x \mid z) dz<br>\end{aligned}<br>\tag{9}<br>$$</p><p>现在又把 $\operatorname{Maximum} L_b$ 转化为求解上式左边项 KL 散度的最小值，以及右半部分的最大值。对于右半部分实际上很好理解：</p><p>$$<br>\operatorname{Maximum} \int_{z} q(z \mid x) \log p(x \mid z) dz&#x3D;\operatorname{Maximum} E_{q(z \mid x)}[\log p(x \mid z)]<br>\tag{10}<br>$$</p><p>E 为期望，表示期望 Encoder 输出 $q(z \mid x)$ 的情况下 Decoder 输出 $p(x \mid z)$ 尽可能的大。即需要从编码器 Encoder 得到的隐变量空间中采样隐变量 z，对采样得到的隐变量 z 进行解码 Decoder，使得解码得到的 $\hat{x}$ 分布中，对应是输入 $x$ 的概率尽可能大。<br>对于左半部分，需要 $KL(q(z \mid x)|P(z))$ 尽可能小，即需要编码器 Encoder 得到的隐变量概率分布与隐变量的先验分布尽可能接近。</p><h2 id="重参数"><a href="#重参数" class="headerlink" title="重参数"></a>重参数</h2><p>最后是实现模型的一个技巧，英文名是 reparameterization trick，这里叫它做重参数吧。</p><p>其实很简单，就是要从 $p(z|x_k)$ 中采样一个 z_k 出来，尽管知道了 $p(z|x_k)$ 属于正态分布，但是均值方差都是靠 MLP 模型计算出来的，要靠这个过程反过来优化均值和方差的模型，但是对于“采样”这个操作是不可导的，而采样的结果却是可导。我们利用：</p><p>$$<br>\begin{aligned}<br>&amp; \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(z-\mu)^{2}}{2 \sigma^{2}}\right) d z \<br>&#x3D;&amp; \frac{1}{\sqrt{2 \pi}} \exp \left[-\frac{1}{2}\left(\frac{z-\mu}{\sigma}\right)^{2}\right] d\left(\frac{z-\mu}{\sigma}\right)<br>\end{aligned}<br>\tag{11}<br>$$</p><p>这说明 $(z−μ)&#x2F;σ&#x3D;ε$ 是服从均值为0、方差为1的标准正态分布，要同时把 dz 考虑进去，是因为乘上 dz 才算是概率，去掉 dz 是概率密度而不是概率。这时候我们得到：</p><blockquote><p>从 $N(μ,σ^2)$ 中采样一个 z，相当于从 $N(0,I)$ 中采样一个 ε，然后让 $Z&#x3D;μ+ε×σ$。</p></blockquote><p>于是，从 $N(μ,σ^2)$ 采样变成了从 $N(0,I)$ 中采样，然后通过参数变换得到从 $N(μ,σ^2)$ 中采样的结果。这样一来，“采样”这个操作就不用参与梯度下降了，改为采样的结果参与，使得整个模型变得可训练。</p><p><img src="/images/vae/vae5.png" alt="为了使模型具有生成能力，vae要求每个p(Z_X)都向正态分布看齐"></p><p>用更加通俗的话来说就是，当编码器和解码器输出的分布都是高斯分布，且要求隐变量先验分布为 $\mathcal{N}(0,I)$ 时，VAE的训练过程如下图上半部分所示。然而上半部分中的采样过程会导致训练过程中计算重构误差 $|X-f(z)|^2$。得到的梯度无法反向传播到编码器。因此将采样过程修改为下图下半部分的形式，从直接采样目标变量，变成采样目标变量到分布均值的差，这样使得反向传播过程中梯度可以传递到编码器。</p><p><img src="/images/vae/vae6.png"></p><h1 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h1><p>数学原理和公式都写了很多，如果跟我一样还没有被劝退，那么我们估计到这里已经满足不了求知的欲望，希望来点代码刺激刺激。毕竟原理推导很复杂，代码实现一行调包，就感觉自己懂了。</p><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>数据集对于模型训练非常重要，好的数据集可以有效提高训练精度和效率。数据处理使用非常经典的 MNIST 手写字体识别，示例中用到的MNIST数据集是由10类28∗28的灰度图片组成，训练数据集包含60000张图片，测试数据集包含10000张图片。</p><p><img src="/images/vae/code1.png"></p><p>具体可以通过 pytroch 的 vision 套件来下载，比较简单，使用 torchvision.datasets 接口就可以啦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><br>bs = <span class="hljs-number">100</span><br><span class="hljs-comment"># MNIST 手写字体下载</span><br>train_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;./mnist_data/&#x27;</span>, train=<span class="hljs-literal">True</span>, transform=transforms.ToTensor(), download=<span class="hljs-literal">True</span>)<br>test_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;./mnist_data/&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=transforms.ToTensor(), download=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 数据加载</span><br>train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=<span class="hljs-literal">True</span>)<br>test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>下载的数据集文件的目录结构如下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Text">./mnist/<br>├── test<br>│   ├── t10k-images-idx3-ubyte<br>│   └── t10k-labels-idx1-ubyte<br>└── train<br>    ├── train-images-idx3-ubyte<br>    └── train-labels-idx1-ubyte<br></code></pre></td></tr></table></figure><h2 id="AVE-模型结构"><a href="#AVE-模型结构" class="headerlink" title="AVE 模型结构"></a>AVE 模型结构</h2><p>上面数据集虽然使用的是 MNIST 手写字体，但是 VAE 的网络模型结构不是使用 LeNet 网络模型，而是很简单的 Encoder 与 Decoder。</p><p>Encoder 阶段并没有什么秘密，正如上面原理介绍一样，2个 fc 层后接一个 fc 层用于计算均值 mu，另外一个 fc 层用于计算方差 var。</p><p>Decoder 阶段更加粗暴，直接使用3个 fc 层最后端到端的还原回输入 x 所相对应的向量大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">VAE</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x_dim, h_dim1, h_dim2, z_dim</span>):<br>        <span class="hljs-built_in">super</span>(VAE, self).__init__()<br><br>        <span class="hljs-comment"># encoder part</span><br>        self.fc1 = nn.Linear(x_dim, h_dim1)<br>        self.fc2 = nn.Linear(h_dim1, h_dim2)<br>        self.fc31 = nn.Linear(h_dim2, z_dim)<br>        self.fc32 = nn.Linear(h_dim2, z_dim)<br><br>        <span class="hljs-comment"># decoder part</span><br>        self.fc4 = nn.Linear(z_dim, h_dim2)<br>        self.fc5 = nn.Linear(h_dim2, h_dim1)<br>        self.fc6 = nn.Linear(h_dim1, x_dim)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encoder</span>(<span class="hljs-params">self, x</span>):<br>        h = F.relu(self.fc1(x))<br>        h = F.relu(self.fc2(h))<br>        <span class="hljs-keyword">return</span> self.fc31(h), self.fc32(h) <span class="hljs-comment"># mu, log_var</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decoder</span>(<span class="hljs-params">self, z</span>):<br>        h = F.relu(self.fc4(z))<br>        h = F.relu(self.fc5(h))<br>        <span class="hljs-keyword">return</span> F.sigmoid(self.fc6(h)) <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sampling</span>(<span class="hljs-params">self, mu, log_var</span>):<br>        std = torch.exp(<span class="hljs-number">0.5</span>*log_var)<br>        eps = torch.randn_like(std)<br>        <span class="hljs-keyword">return</span> eps.mul(std).add_(mu) <span class="hljs-comment"># return z sample</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>          <span class="hljs-comment"># where z = mu + sigma * torch.randn_like(mu)</span><br>        mu, log_var = self.encoder(x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">784</span>))<br>        z = self.sampling(mu, log_var)<br>        <span class="hljs-keyword">return</span> self.decoder(z), mu, log_var<br></code></pre></td></tr></table></figure><p>上面的代码其实很简单的啦，不过其中有一个 <code>sampling</code> 函数比较有意思，它的输入是 encoder 之后的均值mu和方差var，然后对var求解得到标准差 std。然后使用标准差求 eps，最后通过高斯采样获得隐变量 z。即对应公式：</p><p>$$z&#x3D;μ+ε×σ$$</p><p>引用英语原文的意思，就是从均值为0、方差为1的正态分布中随机数的二维隐变量生成样本。</p><blockquote><p>Generated samples from 2-D latent variable with random numbers from a normal distribution with mean 0 and variance 1.</p></blockquote><p>最后就是构建 VAE 神经网络模型啦，输入是一个 1x784 的 tensor。为什么是 784？因为一张 MNIST 手写字体的小图片就是 28x28 &#x3D; 784。z 设置得比较小，也就一个 1x2 的 tensor，所以看看最后生成什么样的效果图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># build model</span><br>vae = VAE(x_dim=<span class="hljs-number">784</span>, h_dim1= <span class="hljs-number">512</span>, h_dim2=<span class="hljs-number">256</span>, z_dim=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="优化器和损失函数"><a href="#优化器和损失函数" class="headerlink" title="优化器和损失函数"></a>优化器和损失函数</h2><p>优化器没有特殊的地方，直接采用 Adam 优化器就好啦，这个比较通用。不过损失函数就有点不一样啦，VAE的损失函数有两个部分：一个重构因子和一个正则化因子。</p><p>那么现在打开看看 损失函数，损失函数输入有点多，首先第一个 <code>recon_x</code> 是 VAE 网络模型生成输出（注意啦，不是预测输出），后面的三个入参就不用介绍啦。损失函数总体用下面这公式来表示：</p><p>$$<br>\mathcal{L}(\mathbf{x})\approx \frac{1}{n}\sum^{n}<em>{i&#x3D;1}\Bigg[\frac{1}{2}\sum^{k}</em>{j&#x3D;1}\Big[\mu^{2}(\mathbf{x}_i)+\sigma^{2}(\mathbf{x}_i)-\log\sigma^{2}(\mathbf{x}<em>i)-1\Big]-\sum^{L}</em>{l&#x3D;1}\Big[\log p(\mathbf{x}<em>i\mid\mathbf{z}</em>{i,l})\Big]\Bigg]<br>$$</p><p>实际上根据公式()，目标是最小化KL散度：</p><p>$$<br>Min \ KL(q(z \mid x) | p(z \mid x))&#x3D;Min \ \sum(\mu^{2}+\sigma^{2}-\log\sigma^{2})<br>$$</p><p>而上面的式子右半部分可以理解为：</p><p>$$<br>-\log p(\mathbf{x}_i\mid \mathbf{z}<em>i)&#x3D;-\Big[\sum^{784}</em>{j&#x3D;1}x_j\log \hat{x}_j + (1-x_j)\log(1-\hat{x}_j)\Big]<br>$$</p><p>上面这个公式其实认真看一看，发现其实就是Binary Cross Entropy loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Python">optimizer = optim.Adam(vae.parameters())<br><span class="hljs-comment"># return reconstruction error + KL divergence losses</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss_function</span>(<span class="hljs-params">recon_x, x, mu, log_var</span>):<br>    BCE = F.binary_cross_entropy(recon_x, x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">784</span>))<br>    KLD = <span class="hljs-number">0.5</span> * torch.<span class="hljs-built_in">sum</span>(mu.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>) + log_var.exp() - log_var - <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> BCE + KLD<br></code></pre></td></tr></table></figure><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>训练代码就没啥好讲的啦，Pytorch 都一样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">epoch</span>):<br>    vae.train()<br>    train_loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> batch_idx, (data, _) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        optimizer.zero_grad()<br>        recon_data, mu, log_var = vae(data)<br>        loss = loss_function(recon_data, data, mu, log_var)<br><br>        loss.backward()<br>        train_loss += loss.item()<br>        optimizer.step()<br><br>        <span class="hljs-keyword">if</span> batch_idx % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                epoch, batch_idx * <span class="hljs-built_in">len</span>(data), <span class="hljs-built_in">len</span>(train_loader.dataset),<br>                <span class="hljs-number">100.</span> * batch_idx / <span class="hljs-built_in">len</span>(train_loader), loss.item() / <span class="hljs-built_in">len</span>(data)))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;====&gt; Epoch: &#123;&#125; Average loss: &#123;:.4f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(epoch, train_loss / <span class="hljs-built_in">len</span>(train_loader.dataset)))<br></code></pre></td></tr></table></figure><p>输出结果示例：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Text">rain Epoch: 1 [0/60000 (0%)]    Loss: 544.540078<br>Train Epoch: 1 [10000/60000 (17%)]    Loss: 184.232109<br>Train Epoch: 1 [20000/60000 (33%)]    Loss: 162.313955<br>Train Epoch: 1 [30000/60000 (50%)]    Loss: 165.958750<br>Train Epoch: 1 [40000/60000 (67%)]    Loss: 159.636836<br>Train Epoch: 1 [50000/60000 (83%)]    Loss: 157.480146<br>====&gt; Epoch: 1 Average loss: 178.0764<br></code></pre></td></tr></table></figure><h2 id="VAE-结果"><a href="#VAE-结果" class="headerlink" title="VAE 结果"></a>VAE 结果</h2><p>下面图中左边第一个是原始的MNIST 手写字体，那么后面的X-D 代表的是隐向量 z 的维度，可以看到啦维度越高能生成的数据越精确和清晰。</p><p><img src="/images/vae/code2.png"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>VAE虽然也称是AE（AutoEncoder）的一种，但它的做法是别具一格的。在VAE中，它的Encoder有两个，一个用来计算均值，一个用来计算方差。很意外的是 Encoder 不是用来做编码压缩的，是用来算样本的均值和方差，这真是大新闻了，均值和方差不都是统计量吗，怎么是用神经网络来算的？</p><p>事实上，我觉得 VAE 从让普通人望而生畏的变分和贝叶斯理论出发，最后落地到一个具体的模型中，虽然走了比较长的一段路，但最终的模型其实是很接地气的：它本质上就是在常规的自编码器的基础上，对 Encoder 的结果加上了 “高斯噪声”，使得结果 Decoder 能够对噪声有鲁棒性；而额外的KL loss（目的是让均值为0，方差为1），事实上就是相当于对 Encoder 的一个正则项，希望 Encoder 出来的向量均有零均值。</p><p>那另外一个Encoder（对应着计算方差的网络）的作用呢？它是用来动态调节噪声的强度的。直觉上来想，当 Encoder 还没有训练好时（重构误差远大于KL loss），就会适当降低噪声（KL loss增加），使得拟合起来容易一些（重构误差开始下降）；反之，如果 Decoder 训练得还不错时（重构误差小于KL loss），这时候噪声就会增加（KL loss减少），使得拟合更加困难了（重构误差又开始增加），这时候 Decoder 就要想办法提高它的生成能力了。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] <a href="https://deepdreamgenerator.com/">https://deepdreamgenerator.com/</a> Gkotzos, Konstantinos. “Google’s DeepDream: Algorithms on LSD.”</p><p>[2] Marcus, Gary, Ernest Davis, and Scott Aaronson. “A very preliminary analysis of DALL-E 2.” arXiv preprint arXiv:2204.13807 (2022).</p><p>[3] <a href="https://spaces.ac.cn/archives/5253">变分自编码器|Scientific Spaces</a></p><p>[4] <a href="https://zhuanlan.zhihu.com/p/112513743">无监督学习之VAE——变分自编码器详解</a></p><p>[5] <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">From Autoencoder to Beta-VAE</a></p><p>[6] <a href="https://www.bilibili.com/video/BV1JK4y1D7Wb?p=44&vd_source=26de035c60e6c7f810371fdfd13d14b6">李宏毅2022机器学习深度学习课程</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EfficientFormer：轻量化ViT Backbone</title>
    <link href="/2022/07/22/efficientformer/"/>
    <url>/2022/07/22/efficientformer/</url>
    
    <content type="html"><![CDATA[<p><strong>EfficientFormer：轻量化ViT Backbone</strong></p><p>论文：<a href="https://arxiv.org/abs/2206.01191">《EfficientFormer: Vision Transformers at MobileNet Speed<br>》</a></p><p>Vision Transformers (ViT) 在计算机视觉任务中取得了快速进展，开启了 Vision + Transformer 的先河，之后大量的论文和研究都基于 ViT 之上的。不过呢，Transformer 由于 Attention 的结构设计需要大量的参数，执行的性能也比经过特殊优化的 CNN 要慢一点。</p><p>像是之前介绍的 DeiT 利用 ViT + 蒸馏让训练得更快更方便，但是没有解决 ViT 在端侧实时运行的问题。于是后来有了各种 MateFormer、PoolFormer 等各种 XXXFormer 的变种。应该在不久之前呢，Facebook 就提出了 mobilevit，借鉴了端侧 YYDS 永远的神 mobileNet 的优势结构和 Block（CNN） + ViT 结合，让 ViT 开启了端侧可运行的先河。不管是 XXXFormer 还是 mobileNet，主要是试图通过网络架构搜索（AutoML）或与 MobileNet 块的混合设计来降低 ViT 的计算复杂度，但推理速度嘛，还是没办法跟 mobileNet 媲美。</p><blockquote><p>这就引出了一个重要的问题：Transformer 能否在获得高性能的同时跑的跟 MobileNet 一样快？</p></blockquote><p>作者重新审视基于 ViT 的模型中使用的网络架构和具体的算子，找到端侧低效的原因。然后引入了维度一致的 Transformer Block 作为设计范式。最后，通过网络模型搜索获得不同系列的模型 —— EfficientFormer。</p><p><img src="/images/efficientformer/former1.png"></p><h1 id="ViT-实时运行分析"><a href="#ViT-实时运行分析" class="headerlink" title="ViT 实时运行分析"></a>ViT 实时运行分析</h1><p>图中作者作者对不同模型在端侧运行进行了一些分析，主要是分为 ViT 对图像进行分块的 Patch Embedding、Transformer 中的 Attention 和 MLP，另外还有 LeViT 提出的 Reshape 和一些激活等。基于下面这个表，提出了几个猜想，然后设计出了 EfficientFormer 结构。</p><p><img src="/images/efficientformer/former2.png"></p><ol><li><strong>猜想分析1：大 kernel 和 stride 的Patch embedding是速度瓶颈</strong></li></ol><p>Patch embedding 通常使用具有较大kernel-size和stride的非重叠卷积层来实现。大部分AI编译器都不能很好地支持大内核卷积，并且无法通过 Winograd 等现有算法进行加速。</p><ol start="2"><li><strong>猜想分析2：特征维度一致对于 token mixer 的选择很重要</strong></li></ol><p>很多翻译这里写得很玄乎，ZOMI酱的理解是特征维度一致比多头注意力机制对延迟的影响更重要啦，也就是 MLP 实际上并没有那么耗时，但是如果 tensor 的shape一会大一会小，就会影响计算时延。所以 EfficientFormer 提出了具有 4D 特征实现和 3D 多头注意力的维度一致网络，并且消除了低效且频繁 Reshape 操作（主要指 LeViT 中的 Reshape 操作）。</p><ol start="3"><li><strong>猜想分析3：CONV + BN 比 MLP + LN 效率更高</strong></li></ol><p>在 CNN 结构中最经典的就是使用3x3卷积 Conv + Batch Normalization（BN）的组合方式（获取局部特征），而在 Transformer 中最典型的方式是使用 linear projection（MLP）+ layer normalization（LN）（获取全局特征）的组合。不过作者对比测试中发现呀，CONV + BN 比 MLP + LN 效率更高。</p><ol start="4"><li><strong>猜想分析4：激活函数取决于编译器</strong></li></ol><p>最后这个就比较简单，激活函数包括 GeLU、ReLU 和 HardSwish 的性能在 TensorRT 或者 CoreML 中都不一样，所以激活的优化主要是看用什么端侧编译器。</p><h1 id="EfficientFormer-架构"><a href="#EfficientFormer-架构" class="headerlink" title="EfficientFormer 架构"></a>EfficientFormer 架构</h1><p>第4个点不太重要，主要是关注1,2,3点。于是引出了 EfficientFormer 的总结架构啦。<br>EfficientFormer 由 patch embedding (PatchEmbed) 和 meta transformer blocks 组成，表示为 MB：</p><p>$$<br>y&#x3D;\prod_{i}^{m} MB_{i}\left(PatchEmbed \left(X_{0}^{B, 3, H, W}\right)\right)<br>$$</p><p>X_0 是输入图像，Batch Size 为 B，featur map 大小为 [H,W]，Y 是输出，m 是Block的数量。MB 由未指定的 TokenMixer 和 MLP Block 组成，可以表示如下：</p><p>$$<br>X_{i+1}&#x3D; MB_{i}(X_{i})&#x3D; MLP\left(\text { TokenMixer }\left(X_{i}\right)\right)<br>$$</p><p>X_i 是第 i 个 MB 的featur map。进一步将 Stage 定义为处理具有相同空间大小的特征的几个 MetaBlock 的堆栈，图 N_1x 表示 S1 具有 N_1 个 MetaBlock。</p><p><img src="/images/efficientformer/former3.png"></p><p>可以看到 EfficientFormer 一共有4个阶段。每个阶段都有一个 Embeding（两个3x3的Conv组成一个Embeding） 来投影 Token 长度（可以理解为CNN中的feature map）。可以看到啦，EfficientFormer 是一个完全基于Transformer设计的模型，没有集成 MobileNet 相关内容啦。</p><p>最后通过 AUTOML 来搜索 MB_3D 和 MB_4D block 相关参数。</p><p><img src="/images/efficientformer/former4.png"></p><h2 id="维度一致性-dimension-consistent"><a href="#维度一致性-dimension-consistent" class="headerlink" title="维度一致性 dimension-consistent"></a>维度一致性 dimension-consistent</h2><p>根据 <strong>猜想分析2：特征维度一致对于 token mixer 的选择很重要</strong> EfficientFormer 提出了一种维度一致的设计，将网络分成一个 MB_4D，以 CNN 结构为主 (MB4D) 实现；以及一个 MB_3D ，MLP 线性投影和 Attention 注意力在 3D tensor 上运行。网络从 patch embedding 开始，然后就到了 4D 分区，3D 分区在最后阶段应用。最后 4D 和 3D 分区的实际长度是稍后通过架构搜索指定的。</p><p>这里面的 4D 主要是指 CNN 结构中 tensor 的维度 [B, C, W, H]，而 3D 主要是指 Tran 结构中 tensor 的维度 [B, W, H]。</p><p><img src="/images/efficientformer/former5.png"></p><p>网络从使用由具有2个 kernel-size为 3×3， Stride&#x3D;2 的卷积组成的 Conv stem 处理后的图像作为 patch embedding：</p><p>$$<br>X_{1} &#x3D; \text { PatchEmbed }(X_{0}^{B, 3, H, W})<br>$$</p><p>其中 C_j 是第 j 个阶段的通道数（宽度）。然后网络从 MB_4D 开始，使用简单的 Pool ing 来提取 low level特征：</p><p>$$<br>\mathcal{I}<em>{i} &#x3D; \operatorname{Pool}(\mathcal{X}</em>{i})+\mathcal{X}_{i}<br>$$</p><p>$$<br>\mathcal{X}<em>{i+1} &#x3D; \operatorname{Conv}</em>{B}\left(\operatorname{Conv}<em>{B, G}\left(\mathcal{I}</em>{i}\right)\right)+\mathcal{I}_{i}<br>$$</p><p>其中 Conv_B,G 是指卷积后是否分加上BN和GeLU。</p><p><img src="/images/efficientformer/former6.png"></p><p>在处理完所有 MB4D 块后，执行一次Reshape以变换 freature map 并进入 3D 分区。MB3D 遵循传统的 ViT，不过作者把 ReLU 换成了 GeLU 哦。</p><p><img src="/images/efficientformer/former7.png"></p><h2 id="实时运行模型瘦身-Latency-Driven-Slimming"><a href="#实时运行模型瘦身-Latency-Driven-Slimming" class="headerlink" title="实时运行模型瘦身 Latency Driven Slimming"></a>实时运行模型瘦身 Latency Driven Slimming</h2><p>基于 dimension-consistent，EfficientFormer 构建了一个 Supernet，用于搜索 EfficientFormer 架构的高效模型。下面定义一个 MetaPath (MP)：</p><p>$$<br>\mathrm{MP}<em>{i, j&#x3D;1,2} \in\left{\mathrm{MB}</em>{i}^{4 D}, I_{i}\right}<br>$$</p><p>$$<br>\mathrm{MP}<em>{i, j&#x3D;3,4} \in\left{\mathrm{MB}</em>{i}^{4 D}, \mathrm{MB}<em>{i}^{3 D}, I</em>{i}\right}<br>$$</p><p>I 呢表示 identity path，j 表示第 j 个阶段，i 表示第 i 个块。搜索网络Supernet 中通过用 MP 代替 EfficientFormer 的 MB。</p><p>在 Supernet 的第1阶段和第2阶段中，每个 Block 可以选择 MB4D 或 I，而在第3阶段和第4阶段中，Block可以是 MB3D、MB4D 或 I。</p><p>EfficientFormer 只在最后两个阶段启用 MB3D，原因有2个：1）多头注意力的计算相对于Token长度呈二次增长，因此在模型早期集成会大大增加计算成本。2）将全局多头注意力应用于最后阶段符合直觉，即网络的早期阶段捕获低级特征，而后期层则学习长期依赖关系。</p><ol><li><strong>搜索空间</strong></li></ol><p>搜索空间包括 C_j（每个 Stage 的宽度）、N_j（每个 Stage 中的块数，即深度）和最后 N 个 MB3D 的块。</p><ol start="2"><li><strong>搜索算法</strong></li></ol><p>传统的硬件感知网络搜索方法，通常依赖于每个候选模型在搜索空间中的硬件部署来获得延迟，这是非常耗时的。EfficientFormer提出了基于梯度的搜索算法，以获得只需要训练一次Supernet的候选网络。</p><p><img src="/images/efficientformer/former8.png"></p><p>（后续可以针对 NASA 搜索进行详细补充这个内容。）</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>总的来说呢，EfficientFormer-L1 在 ImageNet-1K 分类任务上实现了 79.2% 的 top-1 准确率，推理时间仅为 1.6 ms，与 MobileNetV2 相比，延迟降低了 6%，top-1 准确率提高了 7.4%。延迟不是 ViT 在端侧部署的障碍。</p><p>另外，EfficientFormer-L7 实现了 83.3% 的准确率，延迟仅为 7.0 ms，大大优于 ViT×MobileNet 混合设计（MobileViT-XS，74.8%，7.2ms）。</p><p><img src="/images/efficientformer/former9.png"></p><p>最后通过使用 EfficientFormer 作为图像检测和分割基准的 Backbone，性能也是非常赞的。ViTs 确实可以实现超快的推理速度和强大的性能。</p><p><img src="/images/efficientformer/former10.png"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>MobileViT 结构上基本基于 MobileNet V2 而改进增加了 MobileViT block，但是同样能够实现一个不错的精度表现，文章实验部分大量的对比了 MobileViT 跟 CNN 和 ViT 模型的参数量和模型大小，不过值得一提的是在端侧除了模型大小以外，更加重视模型的性能，只能说这篇文章经典之处是开创了 CNN 融合 ViT 在端侧的研究。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] Li, Yanyu, et al. “EfficientFormer: Vision Transformers at MobileNet Speed.” arXiv preprint arXiv:2206.01191 (2022).</p><p>[2] <a href="https://mp.weixin.qq.com/s/w9mrhOv5aG_QY7QQL9_8gw">https://mp.weixin.qq.com/s/w9mrhOv5aG_QY7QQL9_8gw</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/524295602">https://zhuanlan.zhihu.com/p/524295602</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>MobileViT：挑战MobileNet端侧霸主</title>
    <link href="/2022/07/20/mobilevit/"/>
    <url>/2022/07/20/mobilevit/</url>
    
    <content type="html"><![CDATA[<p><strong>MobileViT：挑战MobileNet端侧霸主</strong></p><p>论文：<a href="https://arxiv.org/abs/2110.02178">《MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer》</a></p><p>轻量级的 CNN 在移动视觉任务中非常有用，通过卷积层的局部感知功能，可以让 CNN 网络模型在不同的视觉任务中以较少的参数学习表征。不过值得注意的是，CNN 类型结构的网络模型在空间上是局部建模的，也就是主要对局部特征进行感知提取。如果想要学习全局表征，可以采用基于自注意的视觉Transformer(ViT)。但是与 CNN 不同，ViT 的参数量一般来说都比较大。</p><p>那现在！是否有可能结合 CNN 和 ViT 的优势，为移动视觉任务构建一个轻量级、低延迟的网络？ 为此，作者提出了 MobileViT，一种用于移动设备的轻量级通用视觉Transformer。</p><p>实验结果表明，MobileViT 在不同的任务和数据集上显著优于基于 CNN 和 ViT 的网络。在 ImageNet-1k 数据集上，MobileViT 实现了 78.4% 的 Top-1 精度，拥有约 600 万个参数，对于类似数量的参数，其精度分别比 MobileNetv3（基于CNN）和 DeIT（基于ViT）高3.2%和6.2%。在 MS-COCO 目标检测任务中，对于相同数量的参数，MobileViT 比 MobileNetv3 的准确度高 5.7%！</p><p><img src="/images/mobilevit/mobilevit1.png"></p><h1 id="MobileNet-V2"><a href="#MobileNet-V2" class="headerlink" title="MobileNet V2"></a>MobileNet V2</h1><p>因为 MobileViT 借鉴了 MobileNet V2 的结构体，所以在这里简单的讲讲 MobileNet V2 的经典内容。</p><p>MobileNet V2 是在 V1 基础之上的改进，V1主要思想就是<strong>深度可分离卷积（depthwise separable convolution）</strong>，V2的新想法包括 <strong>Linear Bottleneck</strong> 和 <strong>Inverted Residuals</strong>。</p><h2 id="Linear-Bottleneck"><a href="#Linear-Bottleneck" class="headerlink" title="Linear Bottleneck"></a>Linear Bottleneck</h2><p>深度神经网络是由 n 个 L_i 层构成，每层经过激活输出的张量为 [d_i,w_i,h_i]。作者认为一连串的卷积和激活层形成一个 manifold of interest，在神经网络中 manifold 可以嵌入到低维子空间，通俗点说，我们查看的卷积层中所有单个 d 通道像素时，这些值中存在多种编码信息，manifold 位于其中的。通过变换，进一步嵌入到下一个低维子空间中，例如通过 1x1 卷积变换维数，转换 manifold 所在空间维度。</p><p>这样的想法比较容易验证，可通过减少层维度从而降低激活空间的维度。MobileNet v1 通过宽度因子（width factor）在计算量和精度之间取折中。用上面的理论来说，宽度因子控制激活空间的维度，直到 manifold 横跨整个空间。</p><p>然而，由于深度卷积神经网络的层是具有非线性激活函数的。以ReLU变换为例，会存在以下特点：</p><ul><li>如果当前激活空间内 manifold 完整度较高，经过ReLU层可能会让激活空间坍塌，不可避免的会丢失信息。</li><li>如果输入对应的输出是非零的，那么输入和输出之间其实对应的就是一个线性映射关系。</li></ul><p><img src="/images/mobilevit/mobilenetv21.png"></p><p>在设计网络结构的时候，想要减少运算量，就需要尽可能将网络维度设计的低一些，但是维度如果低的话，激活变换 ReLU 函数可能会滤除很多有用信息。然后作者就想到了，反正 ReLU 0以上的计算其实就是一个线性映射。那么如果全用线性分类器，会不会就不会丢失一些维度信息，同时可以设计出维度较低的层呢？</p><p>作者针对这个问题使用 linear bottleneck（即不使用ReLU激活，做了线性变换）的来代替原本的非线性激活变换。到此，优化网络架构的思路也出来了：</p><blockquote><p>通过在卷积模块中插入linear bottleneck来捕获 manifold。实验证明，使用linear bottleneck 可以防止非线性数据被破坏太多。</p></blockquote><p>从linear bottleneck到深度卷积之间的的维度比，称为<strong>Expansion factor（扩展系数）,该系数控制了整个block的通道数。</strong>下一部分介绍就要用到这个Expansion了。</p><p>在 Depthwise Separable Convolution 引入 bottlenck layer 后，结构如图所示，其中浅色代表的是下一个 block 的开始，并且注意方块代表的是特征图，而红色映射才是卷积或者 ReLU 操作。</p><p>图（a）是常规的卷积，而图（b）是 Depthwise Separable Convolution，在MobileNetv1 中大量使用。当在 Depthwise Separable Convolution 后面加入 bottlenck layer 就变成了图（c），但是考虑到block是互相堆积的，调整一下视角，如果将 bottlenck layer 看成 block 的输入，那么这种结构也等价于图（d）。</p><p>对于图（d），block 中开始的 1x1 卷积层称为 expansion layer，它的通道大小（inner size）和输入 bottleneck 层的通道大小之比，称为扩展比（expansion ratio）。扩展层之后是 depthwise 卷积，然后采用 1x1 卷积得到 block 的输出特征，这个卷积后面没有非线性激活。对于图（d）这种结构，block 的输入和输出特征是bottleneck 特征，所以这个 block 称为 bottleneck block。</p><p><img src="/images/mobilevit/mobilenetv22.png"></p><h2 id="Inverted-Residuals"><a href="#Inverted-Residuals" class="headerlink" title="Inverted Residuals"></a>Inverted Residuals</h2><p>MobileNetV1 网络主要思路就是深度可分离卷积的堆叠。在 V2 的网络设计中，除了继续使用深度可分离结构之外，还使用了 Expansion layer 和 Projection layer。这个<strong>projection layer</strong> 使用 1x1 的卷积，目的是希望把高维特征映射到低维空间去。<strong>Expansion layer</strong> 的功能正相反，使用 1x1 的卷积，目的是将低维空间映射到高维空间。</p><p><strong>bottleneck residual block（ResNet）</strong>是中间窄两头胖，在 MobileNetV2 中正好反了过来，所以，在 MobileNetV2 的中称这样的网络结构为 <strong>Inverted residuals</strong>。</p><p><img src="/images/mobilevit/mobilenetv23.png"></p><p>需要注意的是 residual connection 是在输入和输出的部分进行连接。刚才我们知道，Linear Bottleneck 因为从高维向低维转换，使用 ReLU 激活函数可能会造成信息丢失或破坏。所以在 projection convolution 这一部分，不再使用 ReLU 激活函数而是使用线性激活函数。</p><p>下面谈谈为什么要构造一个这样的网络结构：如果 tensor 维度越低，卷积层的乘法计算量就越小。如果整个网络都是低维的 tensor，整体计算速度就会很快。</p><p>然而，如果只是使用低维的 tensor 效果并不会好。如果卷积层的过滤器都是使用低维的 tensor 来提取特征的话，那么就没有办法提取到整体的足够多的信息。所以，如果提取特征数据的话，我们可能更希望有高维的 tensor 来做这个事情。V2就设计这样一个结构来达到平衡。</p><p>先通过 Expansion layer 来扩展维度，之后在用深度可分离卷积来提取特征，之后使用Projection layer来压缩数据，让网络从新变小。因为 Expansion layer 和 Projection layer 都是有可以学习的参数，所以整个网络结构可以学习到如何更好的扩展数据和从新压缩数据。</p><p><img src="/images/mobilevit/mobilenetv24.png"></p><h1 id="MobileViT"><a href="#MobileViT" class="headerlink" title="MobileViT"></a>MobileViT</h1><p>MobileViT 个人认为是基于 Mobilenet V2 模型架构之上，加入了新的 ViT 模块，那这个模块作者称为 MobileViT block。并带来了一些新的结果：</p><ol><li><strong>更好的性能</strong>：在相同的参数情况下，与现有的轻量级CNN相比，MobileViT 模型在不同的移动视觉任务中实现了更好的精度。</li><li><strong>更好的泛化能力</strong>：泛化能力是指训练和评价指标之间的差距，对于具有相似的训练指标的两个模型，具有更好评价指标的模型更具有通用性，因为它可以更好地预测未见的数据集。</li><li><strong>更好的鲁棒性</strong>：一个好的模型应该对超参数具有鲁棒性，因为调优这些超参数会消耗时间和资源。与大多数ViT的模型不同，MobileViT 模型使用基于增强训练，对L2正则化不太敏感。</li></ol><p>MobileViT 的初始层是一个 stride&#x3D;3×3 的标准卷积，其次是 MobileNetv2（MV2） Block 和 MobileViT Block，使用 Swish 作为激活函数。跟随 CNN 模型，在MobileViT 块中使用 n&#x3D;3。特征图的空间维数通常是2和h的倍数。因此，在所有空间层面设 h&#x3D;w&#x3D;2，MobileViT 网络中的 MV2 块主要负责降采样。</p><p><img src="/images/mobilevit/mobilevit2.png"></p><h2 id="动机和方法"><a href="#动机和方法" class="headerlink" title="动机和方法"></a>动机和方法</h2><p>一个标准的 ViT 模型将输入 reshape 为 patches，将其投影到固定的 D 维空间，然后使用 L 个 transformer block 学习 patches 之间的表征。</p><p>vision transformer 由于这些模型忽略了 CNN 模型固定的局部空间特征，所以它们需要更多的参数来学习图像中的表征。例如，与 CNN 网络 deeplabv3 相比，基于 vit 的网络多学习了6倍的参数才可以提供相似的分割性能（DPT vs DeepLabv3:345 M vs. 59 M）。此外与 CNN 相比，这些模型的优化性能不佳。ViT 对 L2 正则化很敏感，需要大量的数据增强以防止过拟合。</p><p><img src="/images/mobilevit/transformer1.png"></p><p>MobileViT 引入了 MobileViT block，它可以有效地将局部和全局信息进行编码。与ViT 及其变体不同，MobileViT 从不同的角度学习全局表示。标准卷积涉及三个操作：展开（unfloading） 、局部处理（local processing） 和展开（folding）。MobileViT 块使用 Transformer 将卷积中的局部建模替换为全局建模。这使得MobileViT 块具有 CNN 和 ViT 的性质，有助于它用更少的参数和简单的训练方式学习更好的表示。一句话来说，就是：</p><blockquote><p>作者提出了一种轻量级ViT模型 MobileViT，其核心思想是用 Transformer 作为卷积来学习全局表示。</p></blockquote><h2 id="MobileViT-block"><a href="#MobileViT-block" class="headerlink" title="MobileViT block"></a>MobileViT block</h2><p>MobileViT 块的结构如图所示，可以用较少的参数在输入张量中建模局部和全局信息。对于输入的张量 X，MobileViT 块首先用 n×n 和 1×1 卷积对输入进行操作，得到 X_L。其中 n×n 卷积用于学习局部的空间信息，1×1 卷积用于将输入特征投影到高维空间。</p><p>为了获取更长距离的关系，有一种方法是利用空洞卷积（dilated convolution）进行建模。然而，这种方法需要仔细选择扩张率（dilation rate）。另一个解决方案是自注意力，具有多头自注意的视觉 Transformer（ViT）被证明对视觉识别任务是有效的。然而，ViT 的参数量很大，并且优化能力较弱。</p><p>为了使 MobileViT 能够学习具有空间归纳偏置的全局表示，作者首先将 X_L 展开为 N 个不重叠的patch X_U。其中，P&#x3D;wh，N&#x3D;WH&#x2F;P 为patch的数量，w,h为 patch 的高和宽。跨 patch 中的每个像素 p 通过 Transformer 来进行建模，得到X_G：</p><p>$$<br>X_{G}(p)&#x3D;Transformer \left(X_{U}(p)\right), 1 \leq p \leq P<br>$$</p><p>与丢失像素空间顺序的ViT不同，MobileViT 既不会丢失 patch 顺序，也不会丢失每个 patch 内像素的空间顺序。作者折叠了 X_G 来获得 X_F，然后 X_F 用 1x1 的卷积得到 C 维的特征。再使用 nxn 的卷积用于融合局部和全局特征。由于 X_U 使用卷积对 n×n 区域的局部信息进行编码，而 X_G 对 P 个 patch 中的第 p 个位置的全局信息进行编码，因此 X_G 可以对 X 中的全局信息进行感知。最终，MobileViT块的整体有效感受野为 H×W，也就是一个全局感知的操作。</p><p><img src="/images/mobilevit/mobilevit3.png"></p><p>在MobileViT Block中，每个像素都可以感知到其他像素。图中，黑色和灰色网格中的每个单元分别表示一个patch和一个像素。红色像素通过transformer处理蓝色像素（其他patch中相应位置的像素）。因为蓝色像素已经使用卷积对邻近像素的信息进行了编码，这就允许红色像素对图像中所有像素的信息进行编码。</p><p><img src="/images/mobilevit/mobilevit4.png"></p><h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><ol><li>与 CNN 结构模型进行对比</li></ol><p>下图（a）示了本文方法和其他轻量级CNN网络的对比，可以看出 MobileViT 模型精度的优越性。（b）对比了本文方法和其他轻量级网络在相似参数量下的性能对比，MobileViT 具有更高的性能。（c）展示了本文方法和 heavy-weight CNN的对比，MobileViT 可以用更少的参数，达到更高的准确率。</p><p>MobileViT 在不同网络规模（MobileNetv1、MobileNetv2、ShuffleNetv2、ESPNetv2和MobileNetv3）上的性能优于轻量级CNN。例如，对于一个大约有250万个参数的模型，在ImageNet-1k验证集上，MobileViT 比 MobileNetv2、ShuffleNetv2和MobileNetv3 的性能分别高出 5.0%、5.4%和7.4%。</p><p>MobileViT 提供了比 Heavy-weight CNN（ResNet, DenseNet, ResNet-se和EfficientNet）更好的性能。例如，对于相同数量的参数，MobileVi T比 effentnet 的准确率高出 2.1%。</p><p><img src="/images/mobilevit/experience1.png"></p><ol start="2"><li>与 ViTs 结构模型进行对比</li></ol><p>下面还对比了 MobileViT 和 ViT 结构的的参数量和性能，可以看出，MobileViT 可以用更少的参数、更简单的数据增强，达到更高的性能。图中比较了 MobileViT 和在ImageNet-1k 数据集上从头开始训练的ViT变体（DeIT、T2T、PVT、CAIT、DeepViT、CeiT、CrossViT、LocalViT、PiT、ConViT、ViL、BoTNet和Mobile-former）。</p><p>不像 ViT 变体，显著受益于高级的数据增强(PiT w&#x2F; basic vs. advanced: 72.4%(R4) vs. 78.1%(R17))， MobileViT 通过更少的参数和基本的增强实现了更好的性能。例如，MobileViT 比 DeiT 小2.5，好2.6%。</p><p>总的来说，这些结果表明，与CNN相似，MobileViTs易于优化和鲁棒性强。因此，MobileViT 可以很容易地应用于新的任务和数据集。</p><p><img src="/images/mobilevit/experience2.png"></p><ol start="3"><li>下游任务进行对比测试</li></ol><p>下面展示了 MobileViT 在语义分割任务&#x2F;分类任务&#x2F;检测任务，基于不同主干网络的DeepLabv3 性能和参数对比，可以看出，本文的方法在各种轻量级网络中，能够达到更高的性能。并且推理性能还能够做到实时 33ms 以内啦。</p><p><img src="/images/mobilevit/experience3.png"></p><p>带有 MobileViT 的 DeepLabv3 更小更好。使用 MobileViT 代替 MobileNetv2 作为 Backbone 时，DeepLabv3 的性能提高了 1.4%，体积减少了1.6×。此外，MobileViT 提供了具有竞争力的性能与模型 renet-101 相比，所需参数减少了 9 倍。这一段数字就很有意思了，因为实验部分就看你用谁跟谁比较，毕竟田忌赛马嘛。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>MobileViT 是 Apple 公司发表的一种用于移动设备的轻量级通用视觉 Transformer。作者称，这是首个能比肩轻量级 CNN 网络性能的轻量级 ViT 工作。</p><p>MobileViT 结构上基本基于 MobileNet V2 而改进增加了 MobileViT block，但是同样能够实现一个不错的精度表现，文章实验部分大量的对比了 MobileViT 跟 CNN 和 ViT 模型的参数量和模型大小，不过值得一提的是在端侧除了模型大小以外，更加重视模型的性能，只能说这篇文章经典之处是开创了 CNN 融合 ViT 在端侧的研究。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] <a href="https://blog.csdn.net/qq_42178122/article/details/121028215">https://blog.csdn.net/qq_42178122&#x2F;article&#x2F;details&#x2F;121028215</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/424365052">https://zhuanlan.zhihu.com/p/424365052</a></p><p>[3] Sandler, Mark, et al. “Mobilenetv2: Inverted residuals and linear bottlenecks.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.</p><p>[4] Mehta, Sachin, and Mohammad Rastegari. “Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer.” arXiv preprint arXiv:2110.02178 (2021).</p>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MoCo V3：视觉自监督迎来Transformer</title>
    <link href="/2022/07/20/mocov3/"/>
    <url>/2022/07/20/mocov3/</url>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/2104.02057">An Empirical Study of Training Self-Supervised Visual Transformers</a></p><p>何凯明从 CVPR 2020 上发表的 MoCo V1（Momentum Contrast for Unsupervised Visual Representation Learning），到前几天挂在arxiv上面的 MoCo V3（An Empirical Study of Training Self-Supervised Visual Transformers），MoCo一共走过了三个版本。</p><p>今天介绍 MoCo 系列第三版，MoCo v1 和 v2 是针对 CNN 设计的，而 MoCo v3 是针对 Transformer 结构设计的，反映了 MoCo 系列对视觉模型的普适性。</p><h1 id="MoCo-V3-原理分析"><a href="#MoCo-V3-原理分析" class="headerlink" title="MoCo V3 原理分析"></a>MoCo V3 原理分析</h1><p>MoCo v3 的算法原理不应该是这篇论文的重点，这篇论文的重点应该是将目前无监督学习最常用的对比学习应用在 ViT 上。MoCo v3 相比 v2 去掉了 memory queue，转而像SimCLR 那样采用large batch来取得稍好一点的结果，从结构上 encoder f_q 借鉴 BYOL（Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning） 那样增加了一个 prediction head（两层FC），在ResNet上效果有稍许提升。最后通过大量的实验，去证明如何去克服自监督中引入 ViT 训练不稳定的问题。</p><p><img src="/images/mocov3/BYOL.png"></p><h2 id="MoCo-V3-算法原理"><a href="#MoCo-V3-算法原理" class="headerlink" title="MoCo V3 算法原理"></a>MoCo V3 算法原理</h2><p>作者给出的结论是：影响自监督 ViT 模型训练的关键是：instability，即训练的不稳定性。这种训练的不稳定性所造成的结果并不是训练过程无法收敛 (convergence)，而是性能的轻微下降 (下降1%-3%的精度)。</p><p>MoCo v3 损失函数和 v1 和 v2 相同都是使用 InfoNCE，表达式如下：</p><p>$$<br>\mathcal{L}<em>{q}&#x3D;-\log \frac{\exp \left(q \cdot k</em>{+} &#x2F; \tau\right)}{\sum_{i&#x3D;0}^{K} \exp \left(q \cdot k_{i} &#x2F; \tau\right)}<br>\tag{1}<br>$$</p><p>不一样的是 MoCo V3 在网络结构的组成 Framework 有所差异，具体如图所示。因为引入了 ViT 视觉Transformer结构，所以对数据的输入不再是一张完成的图片，而是 image Patch。另外 Transformer对于长序列具有 Attention 的 Q、K、V 结构能够存储和记忆大量的信息，因此取消了 Memory Queue，直接利用大 Batch Size 来进行学习训练。</p><p><img src="/images/mocov3/moco1.png"></p><p>MoCo v3 的训练方法和 MoCo v1&#x2F;2 的训练方法的差异是：</p><ol><li><strong>取消 Memory Queue，用大 Batch Size</strong>：MoCo V3 的 Framework 里面没有 Memory Queue，这就意味着 MoCo v3 所观察的负样本都来自一个 Batch 的图片。换句话讲，只有当 Batch size 足够大时，模型才能看到足够的负样本，所以 MoCo v3 中 Batch size &#x3D; 4096 这样一个巨大的 Batch size。</li></ol><p><img src="/images/mocov3/experience1.png"></p><ol start="2"><li><p><strong>学习 BYOL 添加 Prediction head</strong>：Encoder f_q 除了 Backbone 和预测头 Projection head 以外，还添加了个 Prediction head，是遵循了 BYOL 这篇论文的方法。即 Encoder f_q（ViT(BP) + Projection head + Prediction head），Encoder f_k（ViT(Momentum) + Projection head）。</p></li><li><p><strong>Contrastive loss的更新方式修改</strong>：对于同一张图片的2个增强版本 x1​,x2​ ，分别通过 Encoder f_q​ 和 Momentum Encoder f_k​ 得到 q1​,q2​ 和 k1​,k2​。让 q1​,k2​ 通过 Contrastive loss (式 1) 进行优化 Encoder f_q​的参数，让 q2​,k1​ 通过 Contrastive loss (式 1) 进行优化 Encoder f_q​的参数。Momentum Encoder f_k ​则跟 MoCo V1 版本相同通过动量更新。即：</p></li></ol><p>$$<br>loss &#x3D; Contrastive(q_1, k_2) + Contrastive(q_2, k_1)<br>\tag{2}<br>$$</p><h2 id="MoCo-V3-算法分析"><a href="#MoCo-V3-算法分析" class="headerlink" title="MoCo V3 算法分析"></a>MoCo V3 算法分析</h2><p><img src="/images/mocov3/moco2.png"></p><ol><li>数据增强：</li></ol><p>在ImageNet中 有一堆无标签的数据，拿出一个 MiniBatch，代码表示为 x，也就是 N 张图片，分别进行两种不同的数据增强，得到 x_1 和 x_2，此时 x_1 是 N 张图片，x_2 也是 N 张图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> loader: <span class="hljs-comment"># load a minibatch x with N samples</span><br>    x1, x2 = aug(x), aug(x) <span class="hljs-comment"># augmentation</span><br></code></pre></td></tr></table></figure><ol start="2"><li>分别通过 Encoder 和 Momentum Encoder：</li></ol><p>x_1 分别通过 Encoder 和 Momentum Encoder 得到特征 q_1 和 k_1，维度是 [N,C]，这里特征空间由一个长度为 C&#x3D;128 的向量表示。</p><p>x_2 分别通过 Encoder 和 Momentum Encoder 得到特征 q_2 和 k_2，维度是 [N,C] ，这里特征空间由一个长度为 C&#x3D;128 的向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">q1, q2 = f_q(x1), f_q(x2) <span class="hljs-comment"># queries: [N, C] each</span><br>k1, k2 = f_k(x1), f_k(x2) <span class="hljs-comment"># keys: [N, C] each</span><br></code></pre></td></tr></table></figure><ol start="3"><li>Contrastive loss 的定义：</li></ol><p>对两个维度是 [N,C] 的矩阵（比如是q_1和k_2）做矩阵相乘，得到维度是 [N,N] 的矩阵，其对角线元素代表的就是 positive sample 的相似度，就是让对角线元素越大越好，所以目标是整个 [N,N] 的矩阵越接近单位阵越好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">ctr</span>(<span class="hljs-params">q, k</span>):<br>    logits = mm(q, k.t()) <span class="hljs-comment"># [N, N] pairs</span><br>    labels = <span class="hljs-built_in">range</span>(N) <span class="hljs-comment"># positives are in diagonal</span><br>    loss = CrossEntropyLoss(logits/tau, labels)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * tau * loss<br></code></pre></td></tr></table></figure><ol start="4"><li>Contrastive loss 优化：</li></ol><p>通过一个 Contrastive loss 优化 q_1 和 k_2，通过另一个 Contrastive loss 优化 q_2 和 k_1，并反向传播更新 f_q 的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Python">loss = ctr(q1, k2) + ctr(q2, k1) <span class="hljs-comment"># symmetrized</span><br>loss.backward()<br><br>update(f_q) <span class="hljs-comment"># optimizer update: f_q</span><br></code></pre></td></tr></table></figure><ol start="5"><li>Momentum Encoder的参数使用动量更新：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">f_k = m*f_k + (<span class="hljs-number">1</span>-m)*f_q <span class="hljs-comment"># momentum update: f_k</span><br></code></pre></td></tr></table></figure><h1 id="MoCo-V3-提升ViT训练稳定性"><a href="#MoCo-V3-提升ViT训练稳定性" class="headerlink" title="MoCo V3 提升ViT训练稳定性"></a>MoCo V3 提升ViT训练稳定性</h1><p>重头戏主要在MoCo v3在ViT上的实验，下面主要是对实验部分进行介绍。由于作者给出的结论是：影响自监督ViT模型训练的关键是：instability，即训练的不稳定性。 而这种训练的不稳定性所造成的结果并不是训练过程无法收敛 (convergence)，而是性能的轻微下降 (下降1%-3%的精度)。下面具体来看看每个实验部分的内容。</p><h2 id="不稳定性测试"><a href="#不稳定性测试" class="headerlink" title="不稳定性测试"></a>不稳定性测试</h2><ol><li>Batch size 过大使得训练不稳定</li></ol><p>如下图，Encoder 架构换成 ViT-B&#x2F;16 ，Learning rate&#x3D;1e-4，在 ImageNet 数据集上训练 100 epochs 的结果。作者使用了4种不同的 Batch size：1024, 2048, 4096, 6144 的结果。可以看到当 bs&#x3D;4096 时，曲线出现了 dip 现象 (稍稍落下又急速升起)。这种不稳定现象导致了精度出现下降。当 bs&#x3D;6144 时，曲线的 dip 现象更大了，可能是因为跳出了当前的 local minimum。这种不稳定现象导致了精度出现了更多的下降。</p><p><img src="/images/mocov3/experience1.png"></p><p>正常在CNN架构上，Batch size越大，那么自监督学习中负样本数量越大，能够学习更多的负样本特征，但是使用 ViT结构取消 Memory Queue，用大 Batch Size后却出现了 dip 现象，继续实验。</p><ol start="2"><li>Learning rate 过大使得训练不稳定</li></ol><p>如下图，Encoder 架构换成 ViT-B&#x2F;16 ，Batch size&#x3D;4096，在 ImageNet 数据集上训练 100 epochs 的结果。作者使用了4种不同的 Learning rate：0.5e-4, 1.0e-4, 1.5e-4 的结果。可以看到当Learning rate 较大时，曲线出现了 dip 现象 (稍稍落下又急速升起)。这种不稳定现象导致了精度出现下降。</p><ol start="3"><li>LARS optimizer 的不稳定性</li></ol><p>如下图，使用了 LARS 优化器，分别使用了4种不同的 Learning rate：3e-4, 5e-4, 6e-4, 8e-4 的结果。结果发现当给定合适的学习率时，LARS的性能可以超过AdamW，但是当学习率稍微变大时，性能就会显著下降。而且曲线自始至终都是平滑的，没有 dip 现象。所以最终为了使得训练对学习率更鲁棒，作者还是采用 AdamW 作为优化器。因为若采用 LARS，则每换一个网络架构就要重新搜索最合适的 Learning rate。</p><p><img src="/images/mocov3/experience2.png"></p><h2 id="提升稳定性的方法"><a href="#提升稳定性的方法" class="headerlink" title="提升稳定性的方法"></a>提升稳定性的方法</h2><p>既然上面重点分析了 Batch Size、Learning rate 和 LARS optimizer对不稳定性的分析，论文实验中就给出了提升稳定性的方法：<strong>random patch projection</strong>。</p><p>导致训练出现不稳定的这些 dip 跟梯度暴涨 (spike) 有关，第1层会先出现梯度暴涨的现象，结果几十次迭代后，会传到到最后1层。作者解决的办法是冻结第1层的参数，也就是patch embedding那层，随机初始化后，不再更新这一层的参数。</p><p>实验结果通过下面的图可以看到，使用 MoCo v3 or SimCLR, BYOL 方法，Encoder 架构换成 ViT-B&#x2F;16 ，Batch size&#x3D;4096，在 ImageNet 数据集上训练 100 epochs 的结果，不同的是冻结了patch embedding那层的参数，使用了随机参数初始化。</p><p>不论是 MoCo v3 还是 SimCLR, BYOL 方法，冻结 patch embedding 那层的参数都能够提升自监督 ViT 的训练稳定性。除此之外， gradient-clip 也能够帮助提升训练稳定性，其极限情况就是冻结参数，真的可以避免出现 dip 现象。</p><p><img src="/images/mocov3/experience3.png"></p><h1 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h1><p>有意思的是这篇论文的写作方式，首先通过实验发现问题，然后通过实验去解决问题，然后再做一些对比实验，深度学习发文章真的是实验对比非常重要。</p><p>作者对 1) position embedding 的具体形式，2) class token 的必要性，3) Prediction head 的必要性和 4) momentum 超参数的影响分别做了不同的对照实验。其中 position embedding 和 class token 的影响分析比较重要，另外两个对精度影响在千分之一之间。</p><ol><li>位置编码的具体形式</li></ol><p>在 Transformer 结构里面position embedding很重要，在无监督训练过程去除位置编码，效果下降了1个多点，说明 ViT 的学习能力很强，在没有位置信息的情况下就可以学习的很好；从另外一个角度来看，也说明 ViT 并没有充分利用好位置信息。</p><p><img src="/images/mocov3/experience4.png"></p><ol start="2"><li>class token 的必要性</li></ol><p>使用 class token 的性能是76.5，而简单地取消 class token，并换成 Global Average Pooling 会下降到69.7，这时候最后一层后面有个LN层。如果把它也去掉，性能会提升到76.3。说明 class token 并不是必要的，LN的选择也很重要。个人猜测 负样本中的 Layer Norm 数据归一化很重要，否则会引起负样本数据不均衡。</p><p><img src="/images/mocov3/experience5.png"></p><p>最后作者跟采用了 Big ResNet 的方法进行对比，以 VIT-L 为backbone的 MoCo v3 完胜。</p><p><img src="/images/mocov3/experience6.png"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>MoCo v3 的改进主要1）取消了 Memory Queue 的机制，2）添加了 Prediction head，3）更新Contrastive loss 优化 Encoder 参数方式。在 自监督学习引入 ViT 的过程中发现了训练不稳定性的问题，通过 random patch embedding 的方式暂时解决了目前问题，但是更大的 Batch Size仍然会引起 dip 问题还需要进一步研究啦。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] Hadsell, Raia, Sumit Chopra, and Yann LeCun. “Dimensionality reduction by learning an invariant mapping.” 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). Vol. 2. IEEE, 2006.</p><p>[2] Chen, Ting, et al. “A simple framework for contrastive learning of visual representations.” International conference on machine learning. PMLR, 2020.</p><p>[3] He, Kaiming, et al. “Momentum contrast for unsupervised visual representation learning.” Proceedings of the IEEE&#x2F;CVF conference on computer vision and pattern recognition. 2020.</p><p>[4] Chen, Xinlei, et al. “Improved baselines with momentum contrastive learning.” arXiv preprint arXiv:2003.04297 (2020).</p><p>[5] Grill, Jean-Bastien, et al. “Bootstrap your own latent-a new approach to self-supervised learning.” Advances in neural information processing systems 33 (2020): 21271-21284.</p><p>[6] <a href="https://zhuanlan.zhihu.com/p/364446773">https://zhuanlan.zhihu.com/p/364446773</a></p><p>[7] <a href="https://www.zhihu.com/question/453203448/answer/1843542119">https://www.zhihu.com/question/453203448/answer/1843542119</a></p><p>[8] <a href="https://zhuanlan.zhihu.com/p/382763210">https://zhuanlan.zhihu.com/p/382763210</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MoCo V2：MoCo系列再升级</title>
    <link href="/2022/07/20/mocov2/"/>
    <url>/2022/07/20/mocov2/</url>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/2003.04297">Improved Baselines with Momentum Contrastive Learning</a></p><p>何凯明从 CVPR 2020 上发表的 MoCo V1（Momentum Contrast for Unsupervised Visual Representation Learning），到前几天挂在arxiv上面的 MoCo V3（An Empirical Study of Training Self-Supervised Visual Transformers），MoCo一共走过了三个版本。</p><p>今天介绍 MoCo 系列第二版 MoCo v2 就是在 SimCLR 发表后结合了 SimCLR 优点的图像自监督学习方法，MoCo v1 和 v2 是针对 CNN 设计的，而 MoCo v3 是针对 Transformer 结构设计的，反映了 MoCo 系列对视觉模型的普适性。</p><h1 id="MoCo-V2-的改进"><a href="#MoCo-V2-的改进" class="headerlink" title="MoCo V2 的改进"></a>MoCo V2 的改进</h1><p>在 SimCLR v1 发布以后，MoCo的作者团队就迅速地将 SimCLR 的两个提点的方法移植到了 MoCo 上面，想看下性能的变化，也就是 MoCo v2。结果显示，MoCo v2 的结果取得了进一步的提升并超过了 SimCLR v1，证明 MoCo 系列方法的地位。因为 MoCo v2 文章只是移植了 SimCLR v1 的技巧而没有大的创新，所以作者就写成了一个只有2页的技术报告，还有长长的一页引用了大量文章。</p><p>有兴趣的读者可以参考MoCo V2的文章，Improved Baselines with Momentum Contrastive Learning。</p><h2 id="MoCo-V2-相关工作"><a href="#MoCo-V2-相关工作" class="headerlink" title="MoCo V2 相关工作"></a>MoCo V2 相关工作</h2><p>动量对比（MoCo V1）表明，无监督预训练可以在多个检测和分割任务中超过其图像监督的预训练，而 SimCLR 进一步减少了无监督和监督预训练表示之间的线性分类器性能差距。</p><p><img src="/images/mocov2/moco1.png"></p><p>SimSLR仍然采用端到端的方法，如图（a）的方式，不过在三个方面改进了实例识别的端到端变体：(i)一个更大的批(4k或8k)，可以提供更多的负样本；(ii)用MLP头替换输出fc投影头；(iii)更强的数据增强。</p><p>在 SimCLR 中具体的来说，就是使用强大的数据增强策略，额外使用了 Gaussian Deblur 的策略和使用巨大的 Batch size，让自监督学习模型在训练时的每一步见到足够多的负样本 (negative samples)，这样有助于自监督学习模型学到更好的 visual representations。</p><p>使用预测头 Projection head。在 SimCLR 中，Encoder 得到的2个 visual representation再通过Prediction head 进一步提特征，预测头是一个 2 层的 MLP，将 visual representation 这个 2048 维的向量 h_i, h_j 进一步映射到 128 维隐空间中，得到新的representation z_i, z_j。利用表征向量 z_i, z_j 去求 Contrastive loss 完成训练，训练完毕后扔掉预测头，保留 Encoder 用于获取 visual representation。</p><p><img src="/images/mocov2/moco2.png"></p><p>我们继续根据 end-to-end 的方法来继续展开。图中 End-to-end 的方法：一个Batch的数据假设有 N 张 image，这里面有一个样本 query q 和它所对应的正样本 k+， q 和 k+ 来自同一张图片的不同的 Data Augmentation，这个Batch剩下的数据就是负样本 (negative samples)。接着将这个 Batch 的数据同时输入给2个架构相同但参数不同的 Encoder f_q 和 Encoder f_k。然后对两个 Encoder的输出使用 Contrastive loss 损失函数使得 query q 和正样本 k+ 的相似程度尽量地高，使得 query q 和负样本 k- 的相似程度尽量地低，通过这样来训练Encoder f_q 和 Encoder f_k，这个过程就称之为自监督预训练。训练完毕后得到的 Encoder 的输出就是图片的 visual representation。</p><p>End-to-end 方法的缺点是：因为 Encoder f_q 和 Encoder f_k 的参数都是通过反向传播来更新的，所以 Batch size 的大小不能太大，否则 NPU 显存就不够了。Batch size 的大小限制了负样本的数量，也限制了自监督模型的性能。SimCLR 是 Google 提出的，有庞大的TPU集群加持，肯定不愁吃不愁穿，但是普通老百姓肯定不能这样。</p><h2 id="MoCo-V2-直接上实验"><a href="#MoCo-V2-直接上实验" class="headerlink" title="MoCo V2 直接上实验"></a>MoCo V2 直接上实验</h2><p>回到今天的主角身边，MoCo v2 的亮点是不需要强大的 Google TPU 加持，仅仅使用 8-GPU 就能超越 SimCLR v1 的性能。v2 将 SimCLR的两个提点的方法 (a 使用预测头 b 使用强大的数据增强策略) 移植到了 MoCo v1上面，实验如下。</p><p><strong>训练集</strong>：ImageNet 数据集。</p><p><strong>评价手段</strong>：</p><ol><li><p>Linear Evaluation：Encoder (ResNet-50) 的参数固定不动，在Encoder后面加分类器（具体就是一个FC层+softmax激活函数），使用全部的 ImageNet label 只训练分类器的参数，而不训练 Encoder 的参数)。看最后 Encoder+分类器的性能。</p></li><li><p>VOC 目标检测 使用 Faster R-CNN 检测器 (C4 backbone)，在 VOC 07+12 trainval set 数据集进行 End-to-end 的 Fine-tune。在 VOC 07 test 数据集进行 Evaluation。</p></li></ol><h3 id="使用预测头"><a href="#使用预测头" class="headerlink" title="使用预测头"></a><strong>使用预测头</strong></h3><p>预测器 Projection head 分类任务的性能只存在于自监督的预训练过程，在 Linear Evaluation 和下游任务中都是被去掉的。MoCo V1 的 Encoder 简单使用了 ResNet50，然后输出通过 L2-norm 处理得到最后的输出。在 MoCo V2 中把 ResNet 中输出与1000分类相关的FC层换成了两层的 FC + Relu，隐藏层为2048维。</p><p>Linear Evaluation 结果如下图：</p><p><img src="/images/mocov2/experience1.png"></p><p>图中的 τ 就是损失函数对应的 τ 。在使用预测器且 τ&#x3D;0.07 时精度从 from 60.6% to 62.9%。</p><h3 id="数据增强策略"><a href="#数据增强策略" class="headerlink" title="数据增强策略"></a><strong>数据增强策略</strong></h3><p>对数据增强策略，作者在 MoCo v1 的基础上又添加了 blur augmentation，发现更强的色彩干扰作用有限。只添加 blur augmentation 就可以使得 ImageNet 分类任务的性能从 60.6% 增长到 63.4%，再加上预测头 Projection head 就可以使性能进一步涨到67.3%。</p><p><img src="/images/mocov2/experience2.png"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>MoCo v2 把 SimCLR 中的两个主要提升方法：1）使用强大的数据增强策略，具体就是额外使用了 Gaussian Deblur 的策略；2）使用预测头 Projection head 到 MoCo 中，并且验证了 SimCLR 算法的有效性。最后 MoCo v2 的结果更优于 SimCLR v1，证明 MoCo 系列自监督预训练方法的高效性。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] Hadsell, Raia, Sumit Chopra, and Yann LeCun. “Dimensionality reduction by learning an invariant mapping.” 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). Vol. 2. IEEE, 2006.</p><p>[2] Chen, Ting, et al. “A simple framework for contrastive learning of visual representations.” International conference on machine learning. PMLR, 2020.</p><p>[3] He, Kaiming, et al. “Momentum contrast for unsupervised visual representation learning.” Proceedings of the IEEE&#x2F;CVF conference on computer vision and pattern recognition. 2020.</p><p>[4] Chen, Xinlei, et al. “Improved baselines with momentum contrastive learning.” arXiv preprint arXiv:2003.04297 (2020).</p><p>[5] <a href="https://zhuanlan.zhihu.com/p/364446773">https://zhuanlan.zhihu.com/p/364446773</a></p><p>[6] <a href="https://zhuanlan.zhihu.com/p/469100381">https://zhuanlan.zhihu.com/p/469100381</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MoCo V1：视觉领域也能自监督啦</title>
    <link href="/2022/07/20/mocov1/"/>
    <url>/2022/07/20/mocov1/</url>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1911.05722">Momentum Contrast for Unsupervised Visual Representation Learning</a></p><p>何凯明从 CVPR 2020 上发表的 MoCo V1（Momentum Contrast for Unsupervised Visual Representation Learning），到前几天挂在arxiv上面的 MoCo V3（An Empirical Study of Training Self-Supervised Visual Transformers），MoCo一共走过了三个版本。</p><p>今天介绍 MoCo 系列第一版 MoCo v1 就是在 SimCLR 发表前经典的图像自监督学习方法，MoCo v1 和 v2 是针对 CNN 设计的，而 MoCo v3 是针对 Transformer 结构设计的，反映了 MoCo 系列对视觉模型的普适性。</p><h1 id="自监督学习-Self-Supervised-Learning"><a href="#自监督学习-Self-Supervised-Learning" class="headerlink" title="自监督学习 Self-Supervised Learning"></a>自监督学习 Self-Supervised Learning</h1><p>一般机器学习分为有无监督学习，无监督学习和强化学习。而自监督学习（Self-Supervised Learning）是无监督学习里面的一种，主要是希望能够学习到一种通用的特征表达用于下游任务 (Downstream Tasks)。而在视觉模型中，MoCo 之所以经典是创造出了一个固定的视觉自监督的模式：</p><blockquote><p><strong>Unsupervised Pre-train, Supervised Fine-tune.</strong></p><p><strong>预训练模型使用自监督方法，下游任务使用监督方法微调</strong></p></blockquote><p><img src="/images/mocov1/self_supervised1.png"></p><p>对应图中，预训练阶段使用无标签的数据集 (unlabeled data)，因为带标签的（labeled data）数据收集非常昂贵，需要大量的新一代农民工去标注，成本是相当高。<br>相反，无标签的数据集收集很方便，不需要大量的新一代农民工。</p><p><img src="/images/mocov1/farmer.jpg"></p><p>在无监督CV领域，第一阶段叫做in a task-agnostic way，在训练模型参数的时候，Self-Supervised Learning 就想不用带标签的数据，先把初始化网络模型的权重参数训练到基本可用，得到一个中间权重参数结果，我们把它叫做 Visual Representation。</p><p>第二阶段叫做in a task-specific way，根据下游任务 (Downstream Tasks) 使用带标签的数据集把参数训练到精度达标，这时使用的数据集量就不用太多了，因为参数经过了阶段一的预训练啦。</p><p>MoCo 遵循这个思想，预训练的 MoCo 模型也会得到 Visual Representation，然后通过 Fine-tune 以适应各种各样的下游任务（比如目标检测、语义分割等）。下面图中的实验结果表明，MoCo在 7 个检测&#x2F;语义分割任务（PASCAL VOC, COCO, 其他的数据集）上可以超过了监督学习训练版本。</p><p><img src="/images/mocov1/experience1.png"></p><p>自监督学习的关键可以概括为两点：Pretext Task，Loss Function，在下面分别介绍。</p><h2 id="Contrastive-loss"><a href="#Contrastive-loss" class="headerlink" title="Contrastive loss"></a>Contrastive loss</h2><p>Contrastive loss 来自于 2006年 Yann LeCun 组的工作（Dimension- ality reduction by learning an invariant mapping）。</p><p>Contrastive loss 的思想是想让：1）相近的样本之间的距离越小越好。2）不似样本之间的距离如果小于m，则通过互斥使其距离接近m。文章对第二个点有个形象的解释，就像长度为m的弹簧，如果它被压缩，则会因为斥力恢复到长度m。</p><p><img src="/images/mocov1/loss1.png"></p><p>$$<br>L(W,Y,\vec{X_1},\vec{X_2})&#x3D;(1-Y)\frac{1}{2}(D_w)^2+(Y) \frac{1}{2} { max(0, m-D_w) }^2<br>\tag{1}<br>$$</p><p>其中 W 是网络权重；Y 是成对标签，如果 X1，X2 这对样本属于同一个类，Y&#x3D;0，属于不同类则 Y&#x3D;1。Dw 是 X1 与 X2 在潜变量空间的欧几里德距离。当 Y&#x3D;0，调整参数最小化X1与X2 之间的距离。当 Y&#x3D;1，如果 X1与X2 之间距离大于 m，则不做优化；如果 X1 与 X2 之间的距离小于 m, 则增大两者距离到 m。</p><p>最后的实际效果就像论文给出的实验结果，训练完后在Mnist手写字体数据集上4和9明确的分开出来了。</p><p><img src="/images/mocov1/loss2.png"></p><h2 id="Pretext-Task"><a href="#Pretext-Task" class="headerlink" title="Pretext Task"></a>Pretext Task</h2><p>Pretext Task（译作：借口、托辞）是无监督学习领域的一个常见的术语，专指通过完成暂时的任务A，能够对后续的任务B、C、D有帮助。下面针对NLP和CV有两种主要的Pretext模式。</p><ol><li>NLP领域的 Pretext Task：在训练 BERT 的时候，预训练过程进行作填空的任务。</li></ol><p>如下图所示，把输入文字里面的一部分随机盖住，就是直接用一个掩码 Mask 把要盖住的token（字符或者一个字）给遮盖住，换成一个特殊的字符。接下来把这个盖住的 token 对应位置输出的向量执行线性变换 Linear Transformation，对输出执行softmax计算输出关于每一个字的概率分布。因为这时候 BERT 并不知道被掩盖住的字是 “湾” ，但是输入的原始数据是知道这个信息的，所以损失就是让这个输出和被盖住的 “湾” 越接近越好。这个任务和下游任务毫不相干，但是 BERT 就是通过 Pretext Task 学习到了很好的 Language Representation 作为预训练模型，很好地适应了下游任务。</p><p><img src="/images/mocov1/self_supervised2.png"></p><p>(2) CV领域的 Pretext Task：在训练 SimCLR 的时候，预训练过程让模型区分相似和不相似的图像。</p><p>如下图所示，假设现在有1张图片 x ，先对 x 进行数据增强，得到2张增强以后的图片 x_i, x_j 。接下来把增强后的图片 x_i, x_j 输入到Encoder里面，注意这2个Encoder是共享参数的，得到representation h_i 和 h_j ，再把 h_i 和 h_j 通过 Projection head 得到 representation z_i 和 z_j。下面的目标就是最大化同一张图片得到的 z_i 和 z_j ，最小化不同张图片得到的 z_i 和 z_j。其具体的结构表达式是：</p><p>$$<br>z_i &#x3D; g(h+i) &#x3D; W_2 \sigma(W_1 h_i) \tag{2}<br>$$</p><p><img src="/images/mocov1/self_supervised3.png"></p><p>通过上图方式训练视觉模型，学习到了很好的视觉预训练模型的表达 Image Representation，在下游任务只要稍微进行 Fine-tune，效果就会比有很大的提升。</p><h1 id="MoCo-V1对自监督的改进"><a href="#MoCo-V1对自监督的改进" class="headerlink" title="MoCo V1对自监督的改进"></a>MoCo V1对自监督的改进</h1><p>整篇文章其实主要是在介绍如何用对比学习去无监督地学习视觉的表征。</p><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>先考虑一个任务，现在有两个图片，图片1和图片2。先在图片1中通过数据增益产生两张图片，记作A，B，在图片2中截出一个patch记作C，现在把B和C放到样本库里面，样本库图片的位置随机打乱，然后以A作为查找的对象，让你从样本库中找到与A对应的图片。</p><p><img src="/images/mocov1/moco4.png"></p><p>假设随机裁剪了A，B， C三个图，然后将A设为被预测的对象，然后A通过encoder1编码为向量q，接着B、C经过encoder2编码为k1和k2。q和k1算相似度得到S1，q和k2算相似度得到S2。我们的目的是想要让机器学出来A和B是一类(关联性强)，而A和C其它不是(关联性弱)。</p><p><img src="/images/mocov1/moco5.png"></p><p>由于提前知道A和B是同一张图截出来的，而C不是，因此希望S1（A和B的相似度）尽可能高而S2（A和C的相似度）尽可能低。把B打上是正类的标签，把C打上是负类的标签，即同一张图片截出来的patch彼此为正类，不同的图片截出来的记为负类，由于这种方式只需要设定一个规则，然后让机器自动去打上标签，基于这些自动打上的标签去学习，所以也叫做自监督学习，MoCo就是通过不需要借助手工标注去学习视觉表征。</p><p>MoCo通过构建一个动态的负类队列来进行对比学习，依旧通过上面的例子来说，一般要学到好的表征需要比较多的负类样本，但是由于计算资源限制又不能加入太多的负类样本，并且我们也不希望负类样本是一成不变的，因此提出了就有了 dynamic dictionary with a queue。</p><p><img src="/images/mocov1/moco1.png"></p><p>x^query可以类比上面的图A，x^key类比是图B和图C，图中的encoder可以是CNN，queue就是样本队列，剩下momentum encoder和contrastive loss。</p><h2 id="contrastive-loss"><a href="#contrastive-loss" class="headerlink" title="contrastive loss"></a>contrastive loss</h2><p>对比学习关注的是能不能区别出同类和非同类的样本，Contrastive loss有很多不同的形式，MoCo使用的是InfoNCE，表达式如下：</p><p>$$<br>\mathcal{L}<em>{q}&#x3D;-\log \frac{\exp \left(q \cdot k</em>{+} &#x2F; \tau\right)}{\sum_{i&#x3D;0}^{K} \exp \left(q \cdot k_{i} &#x2F; \tau\right)}<br>\tag{3}<br>$$</p><p>这里通过点积来计算 q 和 k 的相似度，k+ 是指正样本经过momentum encoder编码成的向量，注意的是里面对照样本里面只有一个正样本，其余都是负样本，至于分母 τ 就是softmax的温度参数，用来控制概率分布的尖锐和平滑。</p><h2 id="momentum-encoder"><a href="#momentum-encoder" class="headerlink" title="momentum encoder"></a>momentum encoder</h2><p>原始的自监督学习方法里面的这一批负样本就相当于是有个字典 （Dictionary），字典的key就是负样本，字典的value就是负样本通过 Encoder 之后得到的特征向量。</p><p>那么现在问题来了：这一批负样本，即字典的大小是多大呢？</p><p>负样本的规模就是 batch size，即字典的大小就是 batch size。</p><p>举个例子，假设 batch size &#x3D; 256，那么对于给定的一个样本 ，选择一个正样本 （经过data augmentation的图像）。然后选择256个负样本，然后使用 loss function 来将与正样本之间的距离拉近，负样本之间的距离推开到系数m。</p><p>毫无疑问是 batch size 越大效果越好的，这一点在 SimCLR 中也得到了证明。但是，由于硬件的影响 batch size 不能设置过大，因此很难应用大量的负样本。因此效率较低，如图（a）。</p><p>于是图（b）采用一个较大的memory bank存储较大的字典：对于给定的一个样本 ，选择一个正样本 （经过data augmentation的图像）。采用一个较大的 memory bank 存储较大的字典，这个 memory bank 具体存储的是所有样本的表征 representation（涵盖所有的样本，比如样本一共有60000个，那么memory bank大小就是60000，字典大小也是60000）。采样其中的一部分负样本 ，然后使用Contrastive loss将 q 与正样本之间的距离拉近，负样本之间的距离推开。这次只更新 Encoder 的参数，和采样的key值 。因为这时候没有了 Encoder 的反向传播，所以支持memory bank容量很大。</p><p>但是，这一个step更新的是 Encoder 的参数，和几个采样的key值 ，下个step更新的是 Encoder 的参数，和几个采样的key值 ，Encoder 的参数每个step都更新，但是某一个 key 可能很多个step才被采样到更新一次，而且一个epoch只会更新一次。这就出现了一个问题：每个step编码器都会进行更新，这样最新的 query 采样得到的 key 可能是好多个step之前的编码器编码得到的 key，因此丧失了一致性。</p><p>从这一点来看，（a）端到端自监督学习方法的一致性最好，但是受限于batchsize的影响。而（b）采用一个memory bank存储较大的字典，一致性却较差。</p><p><img src="/images/mocov1/moco2.png"></p><p>实现对比学习可以有以上三种形式。在(a)中，encoder q和encoder k都是端对端一起训练，encoder q和encoder k可以是两个不同的网络。(b)的话是把对比的样本全部存到一个memory bank中，训练的时候之间从memory bank中采样。</p><p>（c）就是MoCo的做法，与（a）不同的是，右边的 Encoder 是不直接通过反向传播来训练的，而是优化器产生的动量更新，更新的表达式如下。</p><p>$$<br>\theta_{\mathrm{k}} \leftarrow m \theta_{\mathrm{k}}+(1-m) \theta_{\mathrm{q}}<br>\tag{4}<br>$$</p><p>θ_k 是右边 Encoder 的参数，m默认设为0.999，θ_q 是左边编码 query 的 Encoder，θ_q 通过反向传播来更新，θ_k 则是通过 θ_q 动量更新。为什么采用这样的方式来更新？论文给出的解释是 θ_k 直接通过反向传播来更新的效果并不好，因为 θ_k 快速的变化会导致 key 的表征不稳定，但是动量更新很好地解决了这个问题。</p><p>现在的 Momentum Encoder 的更新是通过4式，以动量的方法更新的，不涉及反向传播，所以 输入的负样本 (negative samples) 的数量可以很多，具体就是 Queue 的大小可以比较大，那当然是负样本的数量越多越好了。这就是 Dictionary as a queue 的含义，即通过动量更新的形式，使得可以包含更多的负样本。而且 Momentum Encoder 的更新极其缓慢，所以Momentum Encoder 的更新相当于是看了很多的 Batch，也就是很多负样本。</p><p>MoCo的每个step都会更新Momentum Encoder，虽然更新缓慢，但是每个step都会通过式（4）更新 Momentum Encoder，这样 Encoder 和 Momentum Encoder 每个step 都有更新，就解决了一致性的问题。</p><h1 id="MoCo-V1算法理解"><a href="#MoCo-V1算法理解" class="headerlink" title="MoCo V1算法理解"></a>MoCo V1算法理解</h1><p>如果还没有了解清楚的话，可以来看下算法训练的伪代码，也许会更清晰一点。</p><p><img src="/images/mocov1/moco3.png"></p><ol><li>数据增强：</li></ol><p>现在我们有一堆无标签的数据，拿出一个 Batch，代码表示为 x，也就是 张图片，分别进行两种不同的数据增强，得到 x_q 和 x_k，则 x_q 是 张图片，x_k 也是 张图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> loader: <span class="hljs-comment"># 输入一个图像序列x，包含N张图，没有标签</span><br>    x_q = aug(x) <span class="hljs-comment"># 查询queue的图 (数据增强得到)    </span><br>    x_k = aug(x) <span class="hljs-comment"># 模板图 (数据增强得到)</span><br></code></pre></td></tr></table></figure><ol start="2"><li>分别通过 Encoder 和 Momentum Encoder：</li></ol><p>x_q 通过 Encoder 得到特征 q，维度是 NxC，这里特征空间由一个长度为 C&#x3D;128 的向量表示。</p><p>x_k 通过 Momentum Encoder 得到特征 k，维度是 NxC。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">q = f_q.forward(x_q) <span class="hljs-comment"># 提取查询特征，输出NxC    </span><br>k = f_k.forward(x_k) <span class="hljs-comment"># 提取模板特征，输出NxC</span><br></code></pre></td></tr></table></figure><ol start="3"><li>Momentum Encoder的参数不更新：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 不使用梯度更新f_k的参数，假设用于提取模板的表示应该是稳定的，不应立即更新    </span><br>k = k.detach()<br></code></pre></td></tr></table></figure><ol start="4"><li>计算 N 张图片的自己与自己的增强图的特征的匹配度：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 这里bmm是分批矩阵乘法，输出Nx1，也就是自己与自己的增强图的特征的匹配度</span><br>l_pos = bmm(q.view(N,<span class="hljs-number">1</span>,C), k.view(N,C,<span class="hljs-number">1</span>)) <br></code></pre></td></tr></table></figure><p>这里得到的 l_pos 的维度是 (N, 1, 1)，N 代表 N 张图片的自己与自己的增强图的特征的匹配度。</p><ol start="5"><li>计算 N 张图片与队列中的 K 张图的特征的匹配度：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 输出Nxk，自己与上一批次所有图的匹配度（全不匹配）</span><br>l_neg = mm(q.view(N,C), queue.view(C,K)) <br></code></pre></td></tr></table></figure><p>这里得到的 l_neg 的维度是 (N, K)，代表 N 张图片与队列 Queue 中的 K 张图的特征的匹配度。</p><ol start="6"><li>把 4, 5 两步得到的结果concat起来：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">logits = cat([l_pos, l_neg], dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># 输出 Nx(1+k)</span><br></code></pre></td></tr></table></figure><p>这里得到的 logits 的维度是 (N, K+1)，把它看成是一个矩阵的话呢，有 N 行，代表一个 Batch Size 里面的 N 张图片。每一行的第1个元素是某张图片自己与自己的匹配度。</p><ol start="7"><li>NCE损失函数，就是为了保证自己与自己衍生的匹配度输出越大越好，否则越小越好：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python">labels = zeros(N)<br><br><span class="hljs-comment"># NCE损失函数，就是为了保证自己与自己衍生的匹配度输出越大越好，否则越小越好</span><br>loss = CrossEntropyLoss(logits/t, labels)<br>loss.backward()<br></code></pre></td></tr></table></figure><ol start="8"><li>更新 Encoder 的参数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">update(f_q.params) <span class="hljs-comment"># f_q 使用梯度立即更新</span><br></code></pre></td></tr></table></figure><ol start="9"><li>Momentum Encoder 的参数使用动量更新：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 这里使用动量法更新</span><br>f_k.params = m * f_k.params + (<span class="hljs-number">1</span> - m) * f_q.params<br></code></pre></td></tr></table></figure><ol start="10"><li>更新队列，删除最老的一个 Batch，加入一个新的 Batch：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">enqueue(queue, k) <span class="hljs-comment"># 为了生成反例，所以引入了队列</span><br>dequeue(queue)<br></code></pre></td></tr></table></figure><h1 id="MoCo-V1-对比实验"><a href="#MoCo-V1-对比实验" class="headerlink" title="MoCo V1 对比实验"></a>MoCo V1 对比实验</h1><ol><li><strong>实验一：Linear Classification Protocol</strong></li></ol><p>评价一个自监督模型的性能，最关键和最重要的实验莫过于 Linear Classification Protocol 了，它也叫做 Linear Evaluation，具体做法就是先使用自监督的方法预训练 Encoder，这一过程不使用任何 label。预训练完以后 Encoder 部分的权重也就确定了，这时候把它的权重冻结住，同时在 Encoder 的末尾添加 Global Average Pooling 和一个线性分类器 (FC+softmax)，并在固定数据集上做 Fine-tune，这一过程使用全部的 label。</p><p>上述方法在（a）原始的端到端自监督学习方法，（b）采用一个较大的memory bank存储较大的字典，（c）MoCo方法的结果对比如下图。</p><p><img src="/images/mocov1/experience2.png"></p><p>看到图中的3条曲线都是随着 K 的增加而上升的，证明对于每一个样本来讲，正样本的数量都是一个，随着负样本数量的上升，自监督训练的性能会相应提升。我们看图中的黑色线（a）最大取到了1024，因为这种方法同时使用反向传播更新 Encoder 和 Encoder 的参数，所以 Batch size 的大小受到了显存容量的限制。同时橙色曲线是最优的，证明了MoCo方法的有效性。</p><ol start="2"><li><strong>实验四：下游任务 Fine-tune 结果</strong></li></ol><p>有了预训练好的模型，就相当于是已经把参数训练到了初步成型，这时候再根据下游任务 (Downstream Tasks) 的不同去用带标签的数据集把参数训练到完全成型，那这时用的数据集量就不用太多了，因为参数经过了第1阶段就已经训练得差不多了。</p><p>本文的下游任务是：PASCAL VOC Object Detection 以及 COCO Object Detection and Segmentation，主要对比的对象是 ImageNet 预训练模型 (ImageNet supervised pre-training)，注意这个模型是使用100%的 ImageNet 标签训练的。</p><p>如下图是在 trainval07+12 (约16.5k images) 数据集上 Fine-tune 之后的结果，当Backbone 使用 R50-dilated-C5 时，在 ImageNet-1M 上预训练的 MoCo 模型的性能与有监督学习的性能是相似的。在 Instagram-1B 上预训练的 MoCo 模型的性能超过了有监督学习的性能。当Backbone 使用 R50-dilated-C5 时，在 ImageNet-1M 或者 Instagram-1B 上预训练的 MoCo 模型的性能都超过了有监督学习的性能。</p><p><img src="/images/mocov1/experience4.png"></p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] Hadsell, Raia, Sumit Chopra, and Yann LeCun. “Dimensionality reduction by learning an invariant mapping.” 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). Vol. 2. IEEE, 2006.</p><p>[2] Chen, Ting, et al. “A simple framework for contrastive learning of visual representations.” International conference on machine learning. PMLR, 2020.</p><p>[3] He, Kaiming, et al. “Momentum contrast for unsupervised visual representation learning.” Proceedings of the IEEE&#x2F;CVF conference on computer vision and pattern recognition. 2020.</p><p>[4] <a href="https://zhuanlan.zhihu.com/p/364446773">https://zhuanlan.zhihu.com/p/364446773</a></p><p>[5] <a href="https://zhuanlan.zhihu.com/p/469100381">https://zhuanlan.zhihu.com/p/469100381</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeiT：注意力Attention也能蒸馏</title>
    <link href="/2022/07/20/deit/"/>
    <url>/2022/07/20/deit/</url>
    
    <content type="html"><![CDATA[<p>《Training data-efﬁcient image transformers &amp; distillation through attention》</p><p>ViT 在大数据集 ImageNet-21k（14million）或者 JFT-300M（300million） 上进行训练，Batch Size 128 下 NVIDIA A100 32G GPU 的计算资源加持下预训练 ViT-Base&#x2F;32 需要3天时间。</p><p>Facebook 与索邦大学 Matthieu Cord 教授合作发表 Training data-efficient image transformers（DeiT） &amp; distillation through attention，DeiT 模型（8600万参数）仅用一台 GPU 服务器在 53 hours train，20 hours finetune，仅使用 ImageNet 就达到了 84.2 top-1 准确性，而无需使用任何外部数据进行训练。性能与最先进的卷积神经网络（CNN）可以抗衡。所以呢，很有必要讲讲这个 DeiT 网络模型的相关内容。</p><p>下面来简单总结 DeiT：</p><blockquote><p>DeiT 是一个全 Transformer 的架构。其核心是提出了针对 ViT 的教师-学生蒸馏训练策略，并提出了 token-based distillation 方法，使得 Transformer 在视觉领域训练得又快又好。</p></blockquote><p><img src="/images/deit/experience1.png"></p><h1 id="DeiT-相关背景"><a href="#DeiT-相关背景" class="headerlink" title="DeiT 相关背景"></a>DeiT 相关背景</h1><p>ViT 文中表示数据量不足会导致 ViT 效果变差。针对以上问题，DeiT 核心共享是使用了蒸馏策略，能够仅使用 ImageNet-1K 数据集就就可以达到 83.1% 的 Top1。</p><p>那么文章主要贡献可以总结为三点：</p><ol><li>仅使用 Transformer，不引入 Conv 的情况下也能达到 SOTA 效果。</li><li>提出了基于 token 蒸馏的策略，针对 Transformer 蒸馏方法超越传统蒸馏方法。</li><li>DeiT 发现使用 Convnet 作为教师网络能够比使用 Transformer 架构效果更好。</li></ol><p>正式了解 DeiT 算法之前呢，有几个问题需要去了解的：ViT的缺点和局限性，为什么训练ViT要准备这么多数据，就不能简单快速训练一个模型出来吗？另外 Transformer 视觉模型又怎么玩蒸馏呢？</p><h2 id="ViT-的缺点和局限性"><a href="#ViT-的缺点和局限性" class="headerlink" title="ViT 的缺点和局限性"></a>ViT 的缺点和局限性</h2><p>Transformer的输入是一个序列（Sequence），ViT 所采用的思路是把图像分块（patches），然后把每一块视为一个向量（vector），所有的向量并在一起就成为了一个序列（Sequence），ViT 使用的数据集包括了一个巨大的包含了 300 million images的 JFT-300，这个数据集是私有的，即外部研究者无法复现实验。而且在ViT的实验中作者明确地提到：</p><blockquote><p>“That transformers do not generalize well when trained on insufficient amounts of data.”</p></blockquote><p><img src="/images/deit/transformer1.png" alt="ViT"></p><p>意思是当不使用 JFT-300 大数据集时，效果不如CNN模型。也就反映出Transformer结构若想取得理想的性能和泛化能力就需要这样大的数据集。DeiT 作者通过所提出的蒸馏的训练方案，只在 Imagenet 上进行训练，就产生了一个有竞争力的无卷积 Transformer。</p><h3 id="Visual-transformer"><a href="#Visual-transformer" class="headerlink" title="Visual transformer"></a>Visual transformer</h3><p><strong>Multi-head Self Attention layers (MSA)：</strong></p><p>首先有一个 Query 矩阵 Q 和一个 Key 矩阵 K，把二者矩阵乘在一起并进行归一化以后得到 attention 矩阵，它再与Value矩阵 V 相乘得到最终的输出得到 Z。最后经过 linear transformation 得到 NxD 的输出结果。</p><p><img src="/images/deit/transformer2.png" alt="Multi-head Self Attention layers (MSA)"></p><p><strong>Feed-Forward Network (FFN)：</strong></p><p>Multi-head Self Attention layers 之后往往会跟上一个 Feed-Forward Network (FFN) ，它一般是由2个linear layer构成，第1个linear layer把维度从 D 维变换到 ND 维，第2个linear layer把维度从 ND 维再变换到 D 维。</p><p>此时 Transformer block 是不考虑位置信息的，基于此 ViT 加入了位置编码 (Positional Encoding)，这些编码在第一个 block 之前被添加到 input token 中代表位置信息，作为额外可学习的embedding（Exgra learnable class embedding）。</p><p><strong>Class token：</strong></p><p>Class token 与 input token 并在一起输入 Transformer block 中，最后的输出结果用来预测类别。这样一来，Transformer 相当于一共处理了 N+1 个维度为 D 的token，并且只有第一个 token 的输出用来预测类别。</p><h2 id="知识蒸馏介绍"><a href="#知识蒸馏介绍" class="headerlink" title="知识蒸馏介绍"></a>知识蒸馏介绍</h2><p>Knowledge Distillation（KD）最初被 Hinton 提出 “Distilling the Knowledge in a Neural Network”，与 Label smoothing 动机类似，但是 KD 生成 soft label 的方式是通过教师网络得到的。</p><p>KD 可以视为将教师网络学到的信息压缩到学生网络中。还有一些工作 “Circumventing outlier of autoaugment with knowledge distillation” 则将 KD 视为数据增强方法的一种。</p><h3 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h3><p>虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性。在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下:</p><ul><li>推理速度和性能慢</li><li>对部署资源要求高(内存，显存等)</li></ul><p>在部署时，对延迟以及计算资源都有着严格的限制。因此，模型压缩（在保证性能的前提下减少模型的参数量）成为了一个重要的问题，而“模型蒸馏”属于模型压缩的一种方法。</p><h3 id="理论原理"><a href="#理论原理" class="headerlink" title="理论原理"></a>理论原理</h3><p>知识蒸馏使用的是 Teacher—Student 模型，其中 Teacher 是“知识”的输出者，Student 是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p><ol><li><strong>原始模型训练</strong>: 训练 “Teacher模型”, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li><li><strong>精简模型训练</strong>: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li></ol><p>论文中，Hinton 将问题限定在分类问题下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。知识蒸馏时，由于已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力。</p><p>其中KD的训练过程和传统的训练过程的对比：</p><ol><li>传统training过程 <strong>Hard Targets</strong>: 对 ground truth 求极大似然 Softmax 值。</li><li>KD的training过程 <strong>Soft Targets</strong>: 用 Teacher 模型的 class probabilities作为soft targets。</li></ol><p><img src="/images/deit/kd1.png"></p><p>这就解释了为什么通过蒸馏的方法训练出的 Net-S 相比使用完全相同的模型结构和训练数据只使用Hard Targets的训练方法得到的模型，拥有更好的泛化能力。</p><h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p>第一步是训练Net-T；第二步是在高温 T 下，蒸馏 Net-T 的知识到 Net-S。</p><p><img src="/images/deit/kd2.png"></p><p>训练 Net-T 的过程很简单，而高温蒸馏过程的目标函数由distill loss（对应soft target）和student loss（对应hard target）加权得到：</p><p>$$<br>L&#x3D;\alpha L_{soft}+\beta L_{hard}<br>$$</p><p>Deit 中使用 Conv-Based 架构作为教师网络，以 soft 的方式将归纳偏置传递给学生模型，将局部性的假设通过蒸馏方式引入 Transformer 中，取得了不错的效果。</p><h1 id="DeiT-具体方法"><a href="#DeiT-具体方法" class="headerlink" title="DeiT 具体方法"></a>DeiT 具体方法</h1><p>为什么DeiT能在大幅减少 <strong>1. 训练所需的数据集</strong> 和 <strong>2. 训练时长</strong> 的情况下依旧能够取得很不错的性能呢？我们可以把这个原因归结为DeiT的训练策略。ViT 在小数据集上的性能不如使用CNN网络 EfficientNet，但是跟ViT结构相同，仅仅是使用更好的训练策略的DeiT比ViT的性能已经有了很大的提升，在此基础上，再加上蒸馏 (distillation) 操作，性能超过了 EfficientNet。</p><p>假设有一个性能很好的分类器作为teacher model，通过引入了一个 Distillation Token，然后在 self-attention layers 中跟 class token，patch token 在 Transformer 结构中不断学习。</p><p>Class token的目标是跟真实的label一致，而Distillation Token是要跟teacher model预测的label一致。</p><p><img src="/images/deit/deit1.png" alt="DeiT结构"></p><p>对比 ViT 的输出是一个 softmax，它代表着预测结果属于各个类别的概率的分布。ViT的做法是直接将 softmax 与 GT label取 CE Loss。</p><p>$$<br>CELoss(x, y) &#x3D; - \sum y_i * log(x_i)<br>$$</p><p>而在 DeiT 中，除了 CE Loss 以外，还要 1）定义蒸馏损失；2）加上 Distillation Token。</p><ol><li><strong>定义蒸馏损失</strong></li></ol><p>蒸馏分两种，一种是软蒸馏（soft distillation），另一种是硬蒸馏（hard distillation）。软蒸馏如下式所示，Z_s 和 Z_t 分别是 student model 和 teacher model 的输出，KL 表示 KL 散度，psi 表示softmax函数，lambda 和 tau 是超参数：</p><p>$$<br>\mathcal{L}<em>{\text {global }}&#x3D;(1-\lambda) \mathcal{L}</em>{\mathrm{CE}}\left(\psi\left(Z_{\mathrm{s}}\right), y\right)+\lambda \tau^{2} \mathrm{KL}\left(\psi\left(Z_{\mathrm{s}} &#x2F; \tau\right), \psi\left(Z_{\mathrm{t}} &#x2F; \tau\right)\right)<br>$$</p><p>硬蒸馏如下式所示，其中 CE 表示交叉熵：</p><p>$$<br>\mathcal{L}<em>{\text {global }}^{\text {hardDistill }}&#x3D;\frac{1}{2} \mathcal{L}</em>{\mathrm{CE}}\left(\psi\left(Z_{s}\right), y\right)+\frac{1}{2} \mathcal{L}<em>{\mathrm{CE}}\left(\psi\left(Z</em>{s}\right), y_{\mathrm{t}}\right)<br>$$</p><p>学生网络的输出 Z_s 与真实标签之间计算 CE Loss 。如果是硬蒸馏，就再与教师网络的标签取 CE Loss。如果是软蒸馏，就再与教师网络的 softmax 输出结果取 KL Loss 。</p><p>值得注意的是，Hard Label 也可以通过标签平滑技术 （Label smoothing） 转换成Soft Labe，其中真值对应的标签被认为具有 1- esilon 的概率，剩余的 esilon 由剩余的类别共享。</p><ol start="2"><li><strong>加入 Distillation Token</strong></li></ol><p>Distillation Token 和 ViT 中的 class token 一起加入 Transformer 中，和class token 一样通过 self-attention 与其它的 embedding 一起计算，并且在最后一层之后由网络输出。</p><p>而 Distillation Token 对应的这个输出的目标函数就是蒸馏损失。Distillation Token 允许模型从教师网络的输出中学习，就像在常规的蒸馏中一样，同时也作为一种对class token的补充。</p><p><img src="/images/deit/deit2.png" alt="DeiT训练流程"></p><h1 id="DeiT-具体实验"><a href="#DeiT-具体实验" class="headerlink" title="DeiT 具体实验"></a>DeiT 具体实验</h1><p>实验参数的设置：图中表示不同大小的 DeiT 结构的超参数设置，最大的结构是 DeiT-B，与 ViT-B 结构是相同，唯一不同的是 embedding 的 hidden dimension 和 head 数量。作者保持了每个head的隐变量维度为64，throughput是一个衡量DeiT模型处理图片速度的变量，代表每秒能够处理图片的数目。</p><p><img src="/images/deit/experience2.png" alt="不同大小的DeiT结构的超参数设置"></p><ol><li><strong>Teacher model对比</strong></li></ol><p>作者首先观察到使用 CNN 作为 teacher 比 transformer 作为 teacher 的性能更优。下图中对比了 teacher 网络使用 DeiT-B 和几个 CNN 模型 RegNetY 时，得到的 student 网络的预训练性能以及 finetune 之后的性能。</p><p>其中，DeiT-B 384 代表使用分辨率为 384×384 的图像 finetune 得到的模型，最后的那个小蒸馏符号 alembic sign 代表蒸馏以后得到的模型。</p><p><img src="/images/deit/experience3.png" alt="不同teacher模型的性能指标对比"></p><ol start="2"><li><strong>蒸馏方法对比</strong></li></ol><p>下图是不同蒸馏策略的性能对比，label 代表有监督学习，前3行分别是不使用蒸馏，使用soft蒸馏和使用hard蒸馏的性能对比。前3行不使用 Distillation Token 进行训练，只是相当于在原来 ViT 的基础上给损失函数加上了蒸馏部分。</p><p>对于Transformer来讲，硬蒸馏的性能明显优于软蒸馏，即使只使用 class token，不使用 distill token，硬蒸馏达到 83.0%，而软蒸馏的精度为 81.8%。  </p><p><img src="/images/deit/experience4.png" alt="不同蒸馏策略的性能对比"></p><p>从最后两列 B224 和 B384 看出，以更高的分辨率进行微调有助于减少方法之间的差异。这可能是因为在微调时，作者不使用教师信息。随着微调，class token 和 Distillation Token 之间的相关性略有增加。</p><p>除此之外，蒸馏模型在 accuracy 和 throughput 之间的 trade-off 甚至优于 teacher 模型，这也反映了蒸馏的有趣之处。</p><ol start="3"><li><strong>性能对比</strong></li></ol><p>下面是不同模型性能的数值比较。可以发现在参数量相当的情况下，卷积网络的速度更慢，这是因为大的矩阵乘法比小卷积提供了更多的优化机会。EffcientNet-B4和DeiT-B alembic sign的速度相似，在3个数据集的性能也比较接近。</p><p><img src="/images/deit/experience5.png" alt="不同模型性能的数值比较"></p><ol start="4"><li><strong>对比实验</strong></li></ol><p>作者还做了一些关于数据增强方法和优化器的对比实验。Transformer的训练需要大量的数据，想要在不太大的数据集上取得好性能，就需要大量的数据增强，以实现data-efficient training。几乎所有评测过的数据增强的方法都能提升性能。对于优化器来说，AdamW比SGD性能更好。</p><p>此外，发现Transformer对优化器的超参数很敏感，试了多组 lr 和 weight+decay。stochastic depth有利于收敛。Mixup 和 CutMix 都能提高性能。Exp.+Moving+Avg. 表示参数平滑后的模型，对性能提升只是略有帮助。最后就是 Repeated augmentation 的数据增强方式对于性能提升帮助很大。</p><p><img src="/images/deit/experience6.png" alt="对比实验"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>DeiT 模型（8600万参数）仅用一台 GPU 服务器在 53 hours train，20 hours finetune，仅使用 ImageNet 就达到了 84.2 top-1 准确性，而无需使用任何外部数据进行训练，性能与最先进的卷积神经网络（CNN）可以抗衡。其核心是提出了针对 ViT 的教师-学生蒸馏训练策略，并提出了 token-based distillation 方法，使得 Transformer 在视觉领域训练得又快又好。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] <a href="https://zhuanlan.zhihu.com/p/349315675">https://zhuanlan.zhihu.com/p/349315675</a></p><p>[2] <a href="http://giantpandacv.com/academic/%E7%AE%97%E6%B3%95%E7%A7%91%E6%99%AE/Transformer/DeiT%EF%BC%9A%E4%BD%BF%E7%94%A8Attention%E8%92%B8%E9%A6%8FTransformer/">DeiT：使用Attention蒸馏Transformer</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/102038521">https://zhuanlan.zhihu.com/p/102038521</a></p><p>[4] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 2.7 (2015).</p><p>[5] Touvron, Hugo, et al. “Training data-efficient image transformers &amp; distillation through attention.” International Conference on Machine Learning. PMLR, 2021.</p><p>[6] Dosovitskiy, Alexey, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).</p><p>[7] Wei, Longhui, et al. “Circumventing outliers of autoaugment with knowledge distillation.” European Conference on Computer Vision. Springer, Cham, 2020.</p>]]></content>
    
    
    
    <tags>
      
      <tag>DeiT</tag>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
