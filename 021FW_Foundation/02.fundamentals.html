<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>1.1.2. AI框架作用 &#8212; 人工智能系统（AISys） 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.1.3. AI框架之争" href="03.history.html" />
    <link rel="prev" title="1.1.1. 本章内容" href="01.introduction.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="README.html"><span class="section-number">1. </span>AI框架基础(DONE)</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">1.1.2. </span>AI框架作用</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/021FW_Foundation/02.fundamentals.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/chenzomi12/DeepLearningSystem">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="人工智能系统（AISys）"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../020Framework/readme.html">== 二、AI框架核心模块 ==</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="README.html">1. AI框架基础(DONE)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01.introduction.html">1.1.1. 本章内容</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">1.1.2. AI框架作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.history.html">1.1.3. AI框架之争</a></li>
<li class="toctree-l2"><a class="reference internal" href="04.programing.html">1.1.4. 框架编程范式</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../022FW_AutoDiff/README.html">2. 自动微分(DONE)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/01.introduction.html">2.1.1. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/02.base_concept.html">2.1.2. 什么是微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/03.grad_mode.html">2.1.3. 微分计算模式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/04.implement.html">2.1.4. 微分实现方式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/05.forward_mode.html">2.1.5. 动手实现自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/06.reversed_mode.html">2.1.6. 动手实现PyTorch微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/07.challenge.html">2.1.7. 自动微分的挑战&amp;未来</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../023FW_DataFlow/README.html">3. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/01.introduction.html">3.1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/02.computation_graph.html">3.1.2. 计算图原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/03.atuodiff.html">3.1.3. 与自动微分关系</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/04.dispatch.html">3.1.4. 图优化与图执行调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/05.control_flow.html">3.1.5. 计算图的控制流实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/06.future.html">3.1.6. 计算图的挑战&amp;未来</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../024FW_AICluster/README.html">4. 分布式集群</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../024FW_AICluster/01.introduction.html">4.1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024FW_AICluster/02.architecture.html">4.1.2. AI集群服务器架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024FW_AICluster/03.communication.html">4.1.3. AI集群软硬件通信</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024FW_AICluster/04.primitive.html">4.1.4. 集合通信原语</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024FW_AICluster/05.system.html">4.1.5. 分布式功能</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../025FW_AIAlgo/README.html">5. 分布式算法</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../025FW_AIAlgo/01.challenge.html">5.1.1. 大模型训练挑战</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025FW_AIAlgo/02.algorithm_arch.html">5.1.2. 大模型算法结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025FW_AIAlgo/03.algorithm_sota.html">5.1.3. 亿级规模大模型</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../026FW_Parallel/README.html">6. 分布式并行</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/01.introduction.html">6.1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/02.data_parallel.html">6.1.2. 数据并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/03.tensor_parallel.html">6.1.3. 张量并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/04.mindspore_parallel.html">6.1.4. MindSpore张量并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/05.pipeline_parallel.html">6.1.5. 流水并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/06.hybrid_parallel.html">6.1.6. 混合并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/07.summary.html">6.1.7. 分布式训练总结</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../030Compiler/README.html">== 三、AI编译器原理 ==</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../031CM_Tradition/README.html">1. 传统编译器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/01.introduction.html">1.1.1. 课程概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/02.history.html">1.1.2. 编译器发展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/03.gcc.html">1.1.3. GCC编译过程和原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/04.llvm.html">1.1.4. LLVM架构设计和原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/05.llvm_detail01.html">1.1.5. LLVM IR详解</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/06.llvm_detail02.html">1.1.6. LLVM前端和优化层</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/07.llvm_detail03.html">1.1.7. LLVM后端代码生成</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../032CM_AICompiler/README.html">2. AI 编译器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/01.appear.html">2.1.1. 为什么需要AI编译器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/02.stage.html">2.1.2. AI编译器的发展阶段</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/03.architecture.html">2.1.3. AI编译器的通用架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/04.future.html">2.1.4. AI编译器挑战与思考</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../033CM_Frontend/README.html">3. 前端优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/01.introduction.html">3.1.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/02.graph_ir.html">3.1.2. 图算 IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/03.op_fusion.html">3.1.3. 算子融合</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/04.layout_trans01.html">3.1.4. 布局转换原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/04.layout_trans02.html">3.1.5. 布局转换算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/05.memory.html">3.1.6. 内存分配算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/06.constant_fold.html">3.1.7. 常量折叠原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/07.cse.html">3.1.8. 公共表达式消除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/08.dce.html">3.1.9. 死代码消除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/09.algebraic.html">3.1.10. 代数简化原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/10.summary.html">3.1.11. 优化Pass总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../034CM_Backend/README.html">4. 后端优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/01.introduction.html">4.1.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/02.ops_compute.html">4.1.2. 算子的计算与调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/03.optimization.html">4.1.3. 算子手工优化方式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/04.loop_opt.html">4.1.4. 算子循环优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/05.other_opt.html">4.1.5. 指令和内存优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/06.auto_tuning.html">4.1.6. Auto-Tuning原理</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../036CM_PyTorch/README.html">5. PyTorch2.0 图模式</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/01.introduction.html">5.1.1. PyTorch2.0 特性</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/02.torchscript.html">5.1.2. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/03.torchfx_lazy.html">5.1.3. FX 与 LazyTensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/04.torchdynamo.html">5.1.4. TorchDynamo 获取图</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/05.aotatuograd.html">5.1.5. AOTAutograd 原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/06.dispatch.html">5.1.6. Dispatch 机制</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../040Inference/README.html">== 四、推理系统&amp;引擎 ==</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../041INF_Inference/README.html">1. 推理系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/01.introduction.html">1.1.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/02.constraints.html">1.1.2. 推理系统介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/03.workflow.html">1.1.3. 推理流程全景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/04.system.html">1.1.4. 推理系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/05.inference.html">1.1.5. 推理引擎架构（上）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/06.architecture.html">1.1.6. 推理引擎架构（下）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../042INF_Mobilenet/README.html">2. 模型轻量化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/01.introduction.html">2.1.1. 推理参数了解</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/02.cnn.html">2.1.2. CNN模型小型化（上）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/03.cnn.html">2.1.3. CNN模型小型化（下）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/04.transformer.html">2.1.4. Transformer小型化</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../043INF_Slim/README.html">3. 模型压缩</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/01.introduction.html">3.1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/02.quant.html">3.1.2. 低比特量化原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/03.qat.html">3.1.3. 感知量化训练QAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/04.ptq.html">3.1.4. 训练后量化PTQ与部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/05.pruning.html">3.1.5. 模型剪枝原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/06.distillation.html">3.1.6. 知识蒸馏原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/07.distillation.html">3.1.7. 知识蒸馏算法</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../044INF_Converter/README.html">4. 模型转换&amp;优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/01.introduction.html">4.1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/02.converter_princ.html">4.1.2. 架构与文件格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/03.converter_ir.html">4.1.3. 自定义计算图IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/04.converter_detail.html">4.1.4. 模型转换流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/05.optimizer.html">4.1.5. 计算图优化策略</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/06.basic.html">4.1.6. 常量折叠&amp;冗余节点消除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/07.extend.html">4.1.7. 算子融合/替换/前移</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../045INF_Kernel/README.html">5. Kernel优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/01.introduction.html">5.1.1. Kernel优化架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/02.conv.html">5.1.2. 卷积操作原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/03.im2col.html">5.1.3. Im2Col算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/04.winograd.html">5.1.4. Winograd算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/05.qnnpack.html">5.1.5. QNNPack算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/06.memory.html">5.1.6. 推理内存布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/07.nc4hw4.html">5.1.7. nc4hw4内存排布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/08.others.html">5.1.8. 汇编与循环优化</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../050Hardware/README.html">== 五、AI芯片核心原理 ==</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../051HW_Foundation/README.html">1. AI 计算体系</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/01.introduction.html">1.1.1. 课程内容</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/02.arch_slim.html">1.1.2. AI计算模式(上)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/03.mobile_parallel.html">1.1.3. AI计算模式(下)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/04.metrics.html">1.1.4. 关键设计指标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/05.matrix.html">1.1.5. 核心计算：矩阵乘</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/06.bit_width.html">1.1.6. 数据单位：bits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/07.summary.html">1.1.7. AI计算体系总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../052HW_ChipBase/README.html">2. AI 芯片基础</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../053HW_GPUBase/README.html">3. GPU 原理详解</a></li>
<li class="toctree-l1"><a class="reference internal" href="../054HW_GPUDetail/README.html">4. NVIDIA GPU原理</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../000Others/README.html">1. == 附录 ==</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../000Others/instruments.html">1.1. 书写工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/criterion.html">1.2. 书写规范</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/glossary.html">1.3. 术语表</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/inference.html">1.4. 参考链接</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="人工智能系统（AISys）"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../020Framework/readme.html">== 二、AI框架核心模块 ==</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="README.html">1. AI框架基础(DONE)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01.introduction.html">1.1.1. 本章内容</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">1.1.2. AI框架作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.history.html">1.1.3. AI框架之争</a></li>
<li class="toctree-l2"><a class="reference internal" href="04.programing.html">1.1.4. 框架编程范式</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../022FW_AutoDiff/README.html">2. 自动微分(DONE)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/01.introduction.html">2.1.1. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/02.base_concept.html">2.1.2. 什么是微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/03.grad_mode.html">2.1.3. 微分计算模式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/04.implement.html">2.1.4. 微分实现方式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/05.forward_mode.html">2.1.5. 动手实现自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/06.reversed_mode.html">2.1.6. 动手实现PyTorch微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../022FW_AutoDiff/07.challenge.html">2.1.7. 自动微分的挑战&amp;未来</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../023FW_DataFlow/README.html">3. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/01.introduction.html">3.1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/02.computation_graph.html">3.1.2. 计算图原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/03.atuodiff.html">3.1.3. 与自动微分关系</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/04.dispatch.html">3.1.4. 图优化与图执行调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/05.control_flow.html">3.1.5. 计算图的控制流实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../023FW_DataFlow/06.future.html">3.1.6. 计算图的挑战&amp;未来</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../024FW_AICluster/README.html">4. 分布式集群</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../024FW_AICluster/01.introduction.html">4.1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024FW_AICluster/02.architecture.html">4.1.2. AI集群服务器架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024FW_AICluster/03.communication.html">4.1.3. AI集群软硬件通信</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024FW_AICluster/04.primitive.html">4.1.4. 集合通信原语</a></li>
<li class="toctree-l2"><a class="reference internal" href="../024FW_AICluster/05.system.html">4.1.5. 分布式功能</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../025FW_AIAlgo/README.html">5. 分布式算法</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../025FW_AIAlgo/01.challenge.html">5.1.1. 大模型训练挑战</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025FW_AIAlgo/02.algorithm_arch.html">5.1.2. 大模型算法结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../025FW_AIAlgo/03.algorithm_sota.html">5.1.3. 亿级规模大模型</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../026FW_Parallel/README.html">6. 分布式并行</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/01.introduction.html">6.1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/02.data_parallel.html">6.1.2. 数据并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/03.tensor_parallel.html">6.1.3. 张量并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/04.mindspore_parallel.html">6.1.4. MindSpore张量并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/05.pipeline_parallel.html">6.1.5. 流水并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/06.hybrid_parallel.html">6.1.6. 混合并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../026FW_Parallel/07.summary.html">6.1.7. 分布式训练总结</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../030Compiler/README.html">== 三、AI编译器原理 ==</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../031CM_Tradition/README.html">1. 传统编译器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/01.introduction.html">1.1.1. 课程概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/02.history.html">1.1.2. 编译器发展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/03.gcc.html">1.1.3. GCC编译过程和原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/04.llvm.html">1.1.4. LLVM架构设计和原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/05.llvm_detail01.html">1.1.5. LLVM IR详解</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/06.llvm_detail02.html">1.1.6. LLVM前端和优化层</a></li>
<li class="toctree-l2"><a class="reference internal" href="../031CM_Tradition/07.llvm_detail03.html">1.1.7. LLVM后端代码生成</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../032CM_AICompiler/README.html">2. AI 编译器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/01.appear.html">2.1.1. 为什么需要AI编译器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/02.stage.html">2.1.2. AI编译器的发展阶段</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/03.architecture.html">2.1.3. AI编译器的通用架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../032CM_AICompiler/04.future.html">2.1.4. AI编译器挑战与思考</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../033CM_Frontend/README.html">3. 前端优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/01.introduction.html">3.1.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/02.graph_ir.html">3.1.2. 图算 IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/03.op_fusion.html">3.1.3. 算子融合</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/04.layout_trans01.html">3.1.4. 布局转换原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/04.layout_trans02.html">3.1.5. 布局转换算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/05.memory.html">3.1.6. 内存分配算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/06.constant_fold.html">3.1.7. 常量折叠原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/07.cse.html">3.1.8. 公共表达式消除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/08.dce.html">3.1.9. 死代码消除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/09.algebraic.html">3.1.10. 代数简化原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../033CM_Frontend/10.summary.html">3.1.11. 优化Pass总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../034CM_Backend/README.html">4. 后端优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/01.introduction.html">4.1.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/02.ops_compute.html">4.1.2. 算子的计算与调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/03.optimization.html">4.1.3. 算子手工优化方式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/04.loop_opt.html">4.1.4. 算子循环优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/05.other_opt.html">4.1.5. 指令和内存优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../034CM_Backend/06.auto_tuning.html">4.1.6. Auto-Tuning原理</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../036CM_PyTorch/README.html">5. PyTorch2.0 图模式</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/01.introduction.html">5.1.1. PyTorch2.0 特性</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/02.torchscript.html">5.1.2. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/03.torchfx_lazy.html">5.1.3. FX 与 LazyTensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/04.torchdynamo.html">5.1.4. TorchDynamo 获取图</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/05.aotatuograd.html">5.1.5. AOTAutograd 原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../036CM_PyTorch/06.dispatch.html">5.1.6. Dispatch 机制</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../040Inference/README.html">== 四、推理系统&amp;引擎 ==</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../041INF_Inference/README.html">1. 推理系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/01.introduction.html">1.1.1. 内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/02.constraints.html">1.1.2. 推理系统介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/03.workflow.html">1.1.3. 推理流程全景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/04.system.html">1.1.4. 推理系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/05.inference.html">1.1.5. 推理引擎架构（上）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../041INF_Inference/06.architecture.html">1.1.6. 推理引擎架构（下）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../042INF_Mobilenet/README.html">2. 模型轻量化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/01.introduction.html">2.1.1. 推理参数了解</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/02.cnn.html">2.1.2. CNN模型小型化（上）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/03.cnn.html">2.1.3. CNN模型小型化（下）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../042INF_Mobilenet/04.transformer.html">2.1.4. Transformer小型化</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../043INF_Slim/README.html">3. 模型压缩</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/01.introduction.html">3.1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/02.quant.html">3.1.2. 低比特量化原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/03.qat.html">3.1.3. 感知量化训练QAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/04.ptq.html">3.1.4. 训练后量化PTQ与部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/05.pruning.html">3.1.5. 模型剪枝原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/06.distillation.html">3.1.6. 知识蒸馏原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../043INF_Slim/07.distillation.html">3.1.7. 知识蒸馏算法</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../044INF_Converter/README.html">4. 模型转换&amp;优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/01.introduction.html">4.1.1. 基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/02.converter_princ.html">4.1.2. 架构与文件格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/03.converter_ir.html">4.1.3. 自定义计算图IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/04.converter_detail.html">4.1.4. 模型转换流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/05.optimizer.html">4.1.5. 计算图优化策略</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/06.basic.html">4.1.6. 常量折叠&amp;冗余节点消除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../044INF_Converter/07.extend.html">4.1.7. 算子融合/替换/前移</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../045INF_Kernel/README.html">5. Kernel优化</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/01.introduction.html">5.1.1. Kernel优化架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/02.conv.html">5.1.2. 卷积操作原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/03.im2col.html">5.1.3. Im2Col算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/04.winograd.html">5.1.4. Winograd算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/05.qnnpack.html">5.1.5. QNNPack算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/06.memory.html">5.1.6. 推理内存布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/07.nc4hw4.html">5.1.7. nc4hw4内存排布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../045INF_Kernel/08.others.html">5.1.8. 汇编与循环优化</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../050Hardware/README.html">== 五、AI芯片核心原理 ==</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../051HW_Foundation/README.html">1. AI 计算体系</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/01.introduction.html">1.1.1. 课程内容</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/02.arch_slim.html">1.1.2. AI计算模式(上)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/03.mobile_parallel.html">1.1.3. AI计算模式(下)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/04.metrics.html">1.1.4. 关键设计指标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/05.matrix.html">1.1.5. 核心计算：矩阵乘</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/06.bit_width.html">1.1.6. 数据单位：bits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../051HW_Foundation/07.summary.html">1.1.7. AI计算体系总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../052HW_ChipBase/README.html">2. AI 芯片基础</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../053HW_GPUBase/README.html">3. GPU 原理详解</a></li>
<li class="toctree-l1"><a class="reference internal" href="../054HW_GPUDetail/README.html">4. NVIDIA GPU原理</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../000Others/README.html">1. == 附录 ==</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../000Others/instruments.html">1.1. 书写工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/criterion.html">1.2. 书写规范</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/glossary.html">1.3. 术语表</a></li>
<li class="toctree-l2"><a class="reference internal" href="../000Others/inference.html">1.4. 参考链接</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="ai">
<h1><span class="section-number">1.1.2. </span>AI框架作用<a class="headerlink" href="#ai" title="Permalink to this heading">¶</a></h1>
<p>深度学习范式主要是通过发现经验数据中，错综复杂的结构进行学习。通过构建包含多个处理层的计算模型（网络模型），深度学习可以创建多个级别的抽象层来表示数据。例如，卷积神经网络CNN可以使用大量图像进行训练，例如对猫狗分类去学习猫和狗图片的特征。这种类型的神经网络通常从所采集图像中，包含的像素进行学习。</p>
<p>本章将从深度学习的原理开始，进而深入地讨论在实现深度学习的计算过程中使用到的AI框架，看看AI框架具体的作用和目的。</p>
<div class="section" id="id1">
<h2><span class="section-number">1.1.2.1. </span>深度学习原理<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<p>深度学习的概念源于人工神经网络的研究，但是并不完全等于传统神经网络。在叫法上，很多深度学习算法中都会包含”神经网络”这个词，比如：卷积神经网络、循环神经网络。所以，深度学习可以说是在传统神经网络基础上的升级，约等于神经网络。</p>
<p>虽然深度学习理论最初创立于上世纪八十年代，但有两个主要原因导致其直到近年来才得以发挥巨大作用：</p>
<ul class="simple">
<li><p>深度学习需要大量的标签化数据。例如，无人驾驶汽车模型训练需要数万亿张图片和数千万小时的视频进行学习。</p></li>
<li><p>深度学习需要巨大的计算能力。例如，需要局别并行架构和集群组网能力的高性能
GPU/NPU 对于深度学习计算进行加速。</p></li>
</ul>
<div class="section" id="id2">
<h3><span class="section-number">1.1.2.1.1. </span>神经网络<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<p>现在业界比较通用对神经网络概念的解释是：</p>
<ol class="arabic simple">
<li><p>从通用概念的角度上来看的话，神经网络是在模拟人脑的工作机制，神经元与神经突触之间的连接产生不同的信号传递，每个神经元都记录着信号的特征。</p></li>
<li><p>从统计学的角度来说，就是在预测数据的分布，从数据中学得一个模型，然后再通过这个模型去预测新的数据（这一点就要求测试数据和训练数据必须是同分布）。</p></li>
</ol>
<p>实际上，一个神经网络由多个神经元结构组成，每一层的神经元都拥有多个输入和输出，一层可以由多个神经元组成。例如，第2层神经网络的神经元输出是第3层神经元的输入，输入的数据通过神经元上的激活函数（非线性函数如tanh、sigmod等），来控制输出的数值。</p>
<p>数学上简单地理解，单个神经元其实就是一个 <span class="math notranslate nohighlight">\(X·W\)</span>
的矩阵乘，然后加一个激活函数
<span class="math notranslate nohighlight">\(fun(X·W)\)</span>，通过复合函数组合神经元，就变成一个神经网络的层。这种模拟生物神经元的数学计算，能够很好地对大规模独立同分布的数据进行非线性映射和处理，使得其能够应对到人工智能的不同任务。</p>
<div class="figure align-default" id="id9">
<img alt="../_images/deeplearning01.svg" src="../_images/deeplearning01.svg" /><p class="caption"><span class="caption-number">图1.1.2 </span><span class="caption-text">神经网络与神经元关系</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="id3">
<h3><span class="section-number">1.1.2.1.2. </span>函数逼近<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<p>现在，如果把神经网络看做一个复杂函数，那么这个函数可以逼近任何函数。上面只是定义了什么是神经网络，其实神经网络内部的参数（神经元链接间的权重）需要通过求解函数逼进来确定的。</p>
<p>直观地看下一个简单的例子：假设1个圆圈代表一个神经元，那么一个神经元可模拟“与或非”3种运算，3个神经元组成包含1个隐层的神经网络即可以模拟异或运算。因此，理论上，如此组合的神经网络可模拟任意组合的逻辑函数。</p>
<div class="figure align-default" id="id10">
<img alt="../_images/function_apprcimate.png" src="../_images/function_apprcimate.png" />
<p class="caption"><span class="caption-number">图1.1.3 </span><span class="caption-text">神经元表示与或非</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>很多人会说神经网络只要网络模型足够深和足够宽，就可以拟合（fit）任意函数，这样的说法数学理论上靠谱吗？严格地说，神经网络并不是拟合任意函数，其数学理论建立在通用逼近定理（Universal
approximation theorem）的基础之上：</p>
<blockquote>
<div><p>神经网络则是传统的逼近论中的逼近函数的一种推广。逼近理论证明，只要神经网络规模经过巧妙的设计，使用非线性函数进行组合，它可以以任意精度逼近任意一个在闭集里的连续函数。</p>
</div></blockquote>
<p>既然神经网络模型理论上能够逼近任何连续函数，那么有意思的事情就来了。我们可以利用神经网络处理数学上分类、回归、拟合、逼近等问题啦。例如在CV领域对人脸图像进行分类、通过回归检测图像中的车辆和行人，在NLP中对离散的语料数据进行拟合。</p>
<p>可是，神经网络介绍现在还只能逼近任何函数，逼近函数需要求解，怎么去求解神经网络呢？</p>
<p>函数逼近求解：在数学的理论研究和实际应用中经常遇到逼近求解问题，在选定的一类函数中寻找某个函数
<span class="math notranslate nohighlight">\(f\)</span>，使它与已知函数
<span class="math notranslate nohighlight">\(g\)</span>（或观测数据）在一定意义下为最佳近似表示，并求出用 <span class="math notranslate nohighlight">\(f\)</span>
近似表示 <span class="math notranslate nohighlight">\(g\)</span> 而产生的最小误差（即损失函数）：</p>
<div class="math notranslate nohighlight" id="equation-fund-02-eq1">
<span class="eqno">(1.1.1)<a class="headerlink" href="#equation-fund-02-eq1" title="Permalink to this equation">¶</a></span>\[loss(w)=f(w)-g\]</div>
<p>所以，神经网络可以通过求解损失函数的最小值，来确定这个神经网络中的参数
<span class="math notranslate nohighlight">\(w\)</span>，从而固化这个逼近函数。</p>
</div>
<div class="section" id="id4">
<h3><span class="section-number">1.1.2.1.3. </span>反向求导<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<p>深度学习一般流程是：1）构建神经网络模型，2）定义损失函数和优化器（优化目标），3）开始训练神经网络模型（计算梯度并更新网络模型中的权重参数），4）最后验证精度，其流程如下图所示，前三步最为重要。</p>
<div class="figure align-default" id="id11">
<img alt="../_images/deeplearning02.png" src="../_images/deeplearning02.png" />
<p class="caption"><span class="caption-number">图1.1.4 </span><span class="caption-text">深度学习构建流程</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
<p>因为AI框架已经帮我们封装好了许多功能，所以遇到神经网络模型的精度不达标，算法工程师可以调整网络模型结构、调节损失函数、优化器等参数重新训练，不断地测试验证精度，因此很多人戏称算法工程师又是“调参工程师”。</p>
<p>但是在这一过程中，这种机械的调参是无法触碰到深度学习的本质的，为了了解实际的工作原理，进行总结：<strong>训练的过程本质是进行反向求导（反向传播算法实现）的过程，然后通过迭代计算求得神经网络中的参数，调整参数是控制这一过程的前进速度和方向。</strong></p>
<p>上面这段话我们仍然听不懂，没关系。我们需要了解的是，什么是训练？训练的作用是什么？为什么要求导？为什么在训练的过程中用到求导？求导的数学依据和意义在哪里？</p>
<blockquote>
<div><p>导数是函数的局部性质。一个函数在某一点的导数，描述该函数在这一点附近的变化率。如果函数的自变量和取值都是实数的话，函数在某一点的导数就是该函数所代表的曲线在这一点上的切线斜率。</p>
</div></blockquote>
<p>那么，针对导数的几何意义，其可以表示为函数在某点处的切线斜率；在代数上，其意味着可以求得函数的瞬时变化率。如果把神经网络看做一个高维复杂的函数，那么训练的过程就是对损失函数进行求导，利用导数的性质找到损失函数的变化趋势，每次一点点地改变神经网络仲的参数
<span class="math notranslate nohighlight">\(w\)</span>，最后逼近得到这个高维函数。</p>
</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">1.1.2.2. </span>AI框架的作用<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h2>
<div class="section" id="id6">
<h3><span class="section-number">1.1.2.2.1. </span>AI框架与微分关系<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h3>
<p>根据深度学习的原理，AI框架最核心和基础的功能是自动求导（后续统一称为自动微分，AutoGrad）。</p>
<p>接下来有个更加重要的问题，深度学习中的神经网络为什么需要反向求导？</p>
<p>按照高中数学的基本概念，假设神经网络是一个复合函数（高维函数），那么对这个复合函数求导，用的是链式法则。举个简单的例子，考虑函数
<span class="math notranslate nohighlight">\(z=f(x,y)\)</span>，其中 <span class="math notranslate nohighlight">\(x=g(t),t=h(t)\)</span> ，其中 <span class="math notranslate nohighlight">\(g(t), h(t)\)</span>
是可微函数，那么对函数 <span class="math notranslate nohighlight">\(z\)</span> 关于 <span class="math notranslate nohighlight">\(t\)</span>
求导，函数会顺着链式向外逐层进行求导。</p>
<div class="math notranslate nohighlight" id="equation-fund-02-eq2">
<span class="eqno">(1.1.2)<a class="headerlink" href="#equation-fund-02-eq2" title="Permalink to this equation">¶</a></span>\[\frac{\mathrm{d} x}{\mathrm{d} t} = \frac{\partial z}{\partial x}  \frac{\mathrm{d} x}{\mathrm{d} t} + \frac{\partial z}{\partial y}  \frac{\mathrm{d} y}{\mathrm{d} t}\]</div>
<p>既然有了链式求导法则，而神经网络其实就是个庞大的复合函数，直接求导不就解决问题了吗？反向到底起了什么作用？下面来看几组公式。</p>
<p>假设用3组复合函数来表示一个简单的神经网络：</p>
<div class="math notranslate nohighlight" id="equation-fund-02-eq3">
<span class="eqno">(1.1.3)<a class="headerlink" href="#equation-fund-02-eq3" title="Permalink to this equation">¶</a></span>\[L_1 = sigmoid(w_1\cdot x)\]</div>
<div class="math notranslate nohighlight" id="equation-fund-02-eq4">
<span class="eqno">(1.1.4)<a class="headerlink" href="#equation-fund-02-eq4" title="Permalink to this equation">¶</a></span>\[L_2 = sigmoid(w_2\cdot L_1)\]</div>
<div class="math notranslate nohighlight" id="equation-fund-02-eq5">
<span class="eqno">(1.1.5)<a class="headerlink" href="#equation-fund-02-eq5" title="Permalink to this equation">¶</a></span>\[L_3 = sigmoid(w_3 \cdot L_2)\]</div>
<p>现在定义深度学习中网络模型的损失函数，即优化目标：</p>
<div class="math notranslate nohighlight" id="equation-fund-02-eq6">
<span class="eqno">(1.1.6)<a class="headerlink" href="#equation-fund-02-eq6" title="Permalink to this equation">¶</a></span>\[loss = Loss(L_3,y)\]</div>
<p>根据链式求导法则可以得到：</p>
<div class="math notranslate nohighlight" id="equation-fund-02-eq7">
<span class="eqno">(1.1.7)<a class="headerlink" href="#equation-fund-02-eq7" title="Permalink to this equation">¶</a></span>\[\frac{\partial loss}{\partial w_3} = {Loss}'(L_3, y) {sigmoid}'(w_3,L_2)L_2\]</div>
<div class="math notranslate nohighlight" id="equation-fund-02-eq8">
<span class="eqno">(1.1.8)<a class="headerlink" href="#equation-fund-02-eq8" title="Permalink to this equation">¶</a></span>\[\frac{\partial loss}{\partial w_2} = {Loss}'(L_3, y) {sigmoid}'(w_3,L_2) {sigmoid}'(w_2,L_1) L_1\]</div>
<div class="math notranslate nohighlight" id="equation-fund-02-eq9">
<span class="eqno">(1.1.9)<a class="headerlink" href="#equation-fund-02-eq9" title="Permalink to this equation">¶</a></span>\[\frac{\partial loss}{\partial w_1} = {Loss}'(L_3, y) {sigmoid}'(w_3,L_2) {sigmoid}'(w_2,L_1) {sigmoid}'(w_1,x) x\]</div>
<p>假设神经网络为上述公式
<span class="math notranslate nohighlight">\(L_1,L_2, L_3\)</span>，对损失函数求神经网络中各参数求偏导，可以看到在接下来的求导公式中，每一次导数的计算都可以重用前一次的的计算结果，于是Paul
Werbos在1975年发明了反向传播算法（并在1990重新使用神经网络对反向求导进行表示）。</p>
<p>这里的反向，指的是图中的反向箭头，每一次对损失函数中的参数进行求导，都会复用前一次的计算结果和与其对称的原公式中的变量，更方便地对复合函数进行求导。</p>
<div class="figure align-default" id="id12">
<img alt="../_images/deeplearning03.png" src="../_images/deeplearning03.png" />
<p class="caption"><span class="caption-number">图1.1.5 </span><span class="caption-text">神经网络计算流程</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="id7">
<h3><span class="section-number">1.1.2.2.2. </span>AI框架与程序结合<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h3>
<p>下面左图的公式是神经网络表示的复合函数表示，蓝色框框表示的是AI框架，AI框架给开发者提供构建神经网络模型的数学操作，AI框架把复杂的数学表达，转换成计算机可识别的计算图。</p>
<div class="figure align-default" id="id13">
<img alt="../_images/deeplearning05.png" src="../_images/deeplearning05.png" />
<p class="caption"><span class="caption-number">图1.1.6 </span><span class="caption-text">神经网络表示到AI框架</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<p>定义整个神经网络最终的损失函数为 <span class="math notranslate nohighlight">\(Loss\)</span>
之后，AI框架会自动对损失函数求导（即对神经网络模型中各个参数求其偏导数）。</p>
<p>上面提到过，每一次求导都会复用前一次的计算结果和与其对称的原公式中的变量。那么干脆直接基于表示神经网络的计算图计的基础之上，构建一个与之对称的计算图（反向计算图）。通过反向计算图表示神经网络模型中的偏导数，反向传播则是对链式求导法则的展开。</p>
<div class="figure align-default" id="id14">
<img alt="../_images/deeplearning07.png" src="../_images/deeplearning07.png" />
<p class="caption"><span class="caption-number">图1.1.7 </span><span class="caption-text">AI框架自动微分</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<p>通过损失函数对神经网络模型进行求导，训练过程中更新网络模型中的参数（函数逼近的过程），使得损失函数的值越来越小（表示网络模型的表现越好）。这一过程，只要你定义好网络AI框架都会主动地帮我们完成。</p>
<p>很有意思的是，AI框架对整体开发流程进行了封装，好处是让算法研究人员专注于神经网络模型结构的设计（更好地设计出逼近复合函数），针对数据集提供更好的解决方案，研究让训练加速的优化器或者算法等。</p>
<p>综上所述，<strong>AI框架最核心的作用是提供开发者构建神经网络的接口（数学操作），自动对神经网络训练（进行反向求导，逼近地求解最优值），得到一个神经网络模型（逼近函数）用于解决分类、回归、拟合的问题，实现目标分类、语音识别等应用场景。</strong></p>
</div>
</div>
<div class="section" id="id8">
<h2><span class="section-number">1.1.2.3. </span>本节视频<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h2>
<html><iframe src="https://player.bilibili.com/player.html?aid=388680558&amp;bvid=BV1fd4y1q7qk&amp;cid=911393542&amp;page=1&amp;as_wide=1&amp;high_quality=1&amp;danmaku=0&amp;t=30&amp;autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe></html></div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">1.1.2. AI框架作用</a><ul>
<li><a class="reference internal" href="#id1">1.1.2.1. 深度学习原理</a><ul>
<li><a class="reference internal" href="#id2">1.1.2.1.1. 神经网络</a></li>
<li><a class="reference internal" href="#id3">1.1.2.1.2. 函数逼近</a></li>
<li><a class="reference internal" href="#id4">1.1.2.1.3. 反向求导</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id5">1.1.2.2. AI框架的作用</a><ul>
<li><a class="reference internal" href="#id6">1.1.2.2.1. AI框架与微分关系</a></li>
<li><a class="reference internal" href="#id7">1.1.2.2.2. AI框架与程序结合</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id8">1.1.2.3. 本节视频</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="01.introduction.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>1.1.1. 本章内容</div>
         </div>
     </a>
     <a id="button-next" href="03.history.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>1.1.3. AI框架之争</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>